{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "from util import *\n",
    "from sklearn.preprocessing import LabelEncoder,LabelBinarizer\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\n",
    "from sklearn.svm import SVC\n",
    "from mlxtend.classifier import StackingCVClassifier,StackingClassifier\n",
    "from sklearn.multiclass import  OneVsOneClassifier,OneVsRestClassifier\n",
    "from sklearn.model_selection import train_test_split, KFold, StratifiedKFold\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import GradientBoostingClassifier,AdaBoostClassifier\n",
    "from sklearn.preprocessing import StandardScaler,Normalizer\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "pd.set_option('display.max_colwidth',1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_all = load_train()\n",
    "test_all = load_testA()\n",
    "preprocess_basic_time(train_all)\n",
    "preprocess_basic_wifi(train_all)\n",
    "shop_info = load_shop_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mall_id = \"m_1175\" # 6587 ,9 21 36 72 77 86 \n",
    "train = train_all[train_all.mall_id == mall_id]\n",
    "# label\n",
    "y = train.shop_id.values\n",
    "le = LabelEncoder().fit(y)\n",
    "y = le.transform(y)\n",
    "#split\n",
    "_train_index, _valid_index = get_last_one_week_index(train)\n",
    "valid = train.iloc[_valid_index]\n",
    "train = train.iloc[_train_index]\n",
    "#wifi info\n",
    "df, (train_index, train_use_wifi, train_matrix), (test_index, test_use_wifi, test_matrix) = get_wifi_cache2(mall_id)\n",
    "train_wifi_all_x = train_matrix[_train_index]\n",
    "valid_wifi_all_x = train_matrix[_valid_index]\n",
    "valid_y = y[_valid_index]\n",
    "train_y = y[_train_index]\n",
    "train_lonlats = train[[\"longitude\",\"latitude\"]].values\n",
    "valid_lonlats = valid[[\"longitude\",\"latitude\"]].values\n",
    "train_wh = train[[\"weekday\",\"hour\"]].values\n",
    "valid_wh = valid[[\"weekday\",\"hour\"]].values\n",
    "train_w = train[[\"weekday\"]].values\n",
    "valid_w = valid[[\"weekday\"]].values\n",
    "train_h = train[[\"hour\"]].values\n",
    "valid_h = valid[[\"hour\"]].values\n",
    "\n",
    "train[\"dayofyear\"] = train.dt.dt.dayofyear\n",
    "valid[\"dayofyear\"] = valid.dt.dt.dayofyear\n",
    "\n",
    "indexs = choose_strong_wifi_index(-90,6,train_wifi_all_x)\n",
    "train_x = np.concatenate([train_wifi_all_x[:,indexs],train_lonlats,train_wh],axis=1)\n",
    "valid_x = np.concatenate([valid_wifi_all_x[:,indexs],valid_lonlats,valid_wh],axis=1)\n",
    "lb = LabelBinarizer().fit(y)\n",
    "train_b_y = lb.transform(train_y)\n",
    "valid_b_y = lb.transform(valid_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [],
   "source": [
    "tud = train.groupby([\"user_id\", \"dayofyear\"])[\"shop_id\"].count().reset_index()\n",
    "tud = tud[tud.shop_id >=2]\n",
    "vud = valid.groupby([\"user_id\", \"dayofyear\"])[\"longitude\"].count().reset_index()\n",
    "vud = vud[vud.longitude >= 2]\n",
    "train[\"minute\"] = train.dt.dt.minute\n",
    "train[\"hour_minute\"] = train.hour * 60 + train.minute\n",
    "valid[\"minute\"] = valid.dt.dt.minute\n",
    "valid[\"hour_minute\"] = valid.hour * 60 + valid.minute\n",
    "train[\"dayofyear\"] = train.dt.dt.dayofyear\n",
    "valid[\"dayofyear\"] = valid.dt.dt.dayofyear\n",
    "valid[\"sample_index\"] = range(valid.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9468840389166447"
      ]
     },
     "execution_count": 288,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#  多分类\n",
    "rf_all = RandomForestClassifier(n_estimators=500,n_jobs=-1,class_weight=\"balanced\")\n",
    "train_all_x = np.concatenate([train_wifi_all_x[:, indexs], train_lonlats, train_wh],axis=1)\n",
    "valid_all_x = np.concatenate([valid_wifi_all_x[:, indexs], valid_lonlats, valid_wh],axis=1)\n",
    "rf_all.fit(train_all_x,train_y)\n",
    "rf_all_pba = rf_all.predict_proba(valid_all_x)\n",
    "acc(rf_all.predict(valid_all_x),valid_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_k(pba,k = 3, min_proba = 0):\n",
    "    _pba = pba.copy()\n",
    "    def top_k(x,k):\n",
    "        rs = []\n",
    "        for _ in range(k):\n",
    "            ind = np.argmax(x)\n",
    "            if x[ind] < min_proba:\n",
    "                continue\n",
    "            rs.append(ind)\n",
    "            x[ind] = 0\n",
    "        return rs\n",
    "    r = map(lambda x: top_k(x,k), _pba)\n",
    "    return r\n",
    "\n",
    "def acc_top_k(candidate, y):\n",
    "    all_size = len(candidate)\n",
    "    cor = 0\n",
    "    for _can,_true in zip(candidate,y):\n",
    "        if _true in _can:\n",
    "            cor += 1\n",
    "    return float(cor) / all_size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [],
   "source": [
    "def corr_analyse_in_time(x, train, shops_cor_one_day,time=30): # 一天交易多次， 去某个商店在time分钟内可能去哪些商店\n",
    "    shopids = train[(train.user_id == x[0]) & (train.dayofyear == x[1])][[\"shop_id\",\"hour_minute\"]].values\n",
    "    for _i,_s in enumerate(shopids):\n",
    "        for _j,_s2 in enumerate(shopids):\n",
    "            if _i == _j:\n",
    "                continue\n",
    "            else:\n",
    "                if abs(_s[1] - _s2[1]) > time:\n",
    "                    continue\n",
    "                if _s[0] not in shops_cor_one_day:\n",
    "                    shops_cor_one_day[_s[0]] = {}\n",
    "                if _s2[0] not in shops_cor_one_day[_s[0]]:\n",
    "                    shops_cor_one_day[_s[0]][_s2[0]] = 0\n",
    "                shops_cor_one_day[_s[0]][_s2[0]] += 1\n",
    "    \n",
    "\n",
    "\n",
    "def copy_candidate(old_candidate):\n",
    "    new_candidate = []\n",
    "    for _nda in old_candidate:\n",
    "        new_candidate.append(_nda.copy())\n",
    "    return new_candidate\n",
    "\n",
    "            \n",
    "def statistic_candidate(candidate, valid_y = None):\n",
    "    candi_num = [len(_can) for _can in candidate]\n",
    "    can_num = np.bincount(candi_num)\n",
    "    correct = np.zeros((len(can_num),))\n",
    "    if valid_y is not None:\n",
    "        for _can, _true in zip(candidate,valid_y):\n",
    "            if _true in _can:\n",
    "                correct[len(_can)] += 1\n",
    "        return can_num, correct.astype(int)\n",
    "    else:\n",
    "        return can_num,None  \n",
    "    \n",
    "def print_statistic_candidate(candidate, valid_y = None):\n",
    "    can_num,cor_num = statistic_candidate(candidate, valid_y)\n",
    "    print \"all num\",can_num\n",
    "    if valid_y is not None:\n",
    "        print \"cor num\",cor_num\n",
    "        \n",
    "def candidate_set(candidate):\n",
    "    # 在all_rf产生的候选集合中用那一部分商场训练预测\n",
    "    many = 0\n",
    "    single = 0\n",
    "    two = 0\n",
    "    two_set = set()\n",
    "    many_set = set()\n",
    "    for _sample_index,_can in enumerate(candidate):\n",
    "        if len(_can) ==1:\n",
    "            single += 1\n",
    "        else:\n",
    "            if len(_can) == 2:\n",
    "                two_set.add(\",\".join(sorted(list(_can.astype(str)))))\n",
    "                two +=1\n",
    "            else:\n",
    "                many_set.add(\",\".join(sorted(list(_can.astype(str)))))\n",
    "                many +=1\n",
    "    print \"many size\",many\n",
    "    print \"two size\",two\n",
    "    print \"single size\",single\n",
    "    print \"two set length\",len(two_set)\n",
    "    print \"many set length\",len(many_set)\n",
    "    return two_set,many_set\n",
    "\n",
    "\n",
    "def forward_search(estimetor, trainx,trainy,fix_trainx,validx,validy,fix_validx):\n",
    "    choose = []\n",
    "    best_acc=0\n",
    "    cc = 0\n",
    "    while cc != -1:\n",
    "        cc = -1\n",
    "        for _i in range(trainx.shape[1]):\n",
    "            if _i not in choose:\n",
    "                curr = choose + [_i]\n",
    "                ptrainx = np.concatenate([trainx[:,curr], fix_trainx],axis=1)\n",
    "                pvalidx = np.concatenate([validx[:,curr], fix_validx],axis=1)\n",
    "                m.fit(ptrainx,trainy)\n",
    "                _acc = acc(m.predict(pvalidx),validy)\n",
    "                if _acc > best_acc:\n",
    "                    best_acc = _acc\n",
    "                    cc = _i\n",
    "        if cc != -1:\n",
    "            print \"choose\", cc\n",
    "            print \"best acc\", best_acc\n",
    "            choose.append(cc)\n",
    "        if best_acc == 1:\n",
    "            break\n",
    "    return choose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.972258743098\n",
      "many size 2175\n",
      "two size 1368\n",
      "single size 4063\n",
      "two set length 166\n",
      "many set length 554\n"
     ]
    }
   ],
   "source": [
    "# 候选2 个, 最小概率0.02\n",
    "candidate = get_top_k(rf_all_pba, 3, 0.02)\n",
    "candidate = [rf_all.classes_.take(_can) for _can in candidate]\n",
    "print acc_top_k(candidate, valid_y)\n",
    "\n",
    "_ =candidate_set(candidate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [],
   "source": [
    "shops_cor_10time = {}\n",
    "_ = map(lambda x:corr_analyse_in_time(x,train, shops_cor_10time,time=10),tud[[\"user_id\", \"dayofyear\"]].values)\n",
    "\n",
    "shops_cor_60time = {}\n",
    "_ = map(lambda x:corr_analyse_in_time(x,train, shops_cor_60time,time=60),tud[[\"user_id\", \"dayofyear\"]].values)\n",
    "\n",
    "shops_cor_1000time = {}\n",
    "_ = map(lambda x:corr_analyse_in_time(x,train, shops_cor_1000time,time=1000),tud[[\"user_id\", \"dayofyear\"]].values)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9722587430975546"
      ]
     },
     "execution_count": 295,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modify_candidate = copy_candidate(candidate)        \n",
    "#一天内\n",
    "_ = map(lambda x:remove_candidate(x,valid,shops_cor_10time,modify_candidate,le, stay=2, time = 20),vud[[\"user_id\",\"dayofyear\"]].values)\n",
    "acc_top_k(modify_candidate, valid_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all num [   0 4063 1368 2175]\n",
      "cor num [   0 4061 1368 1966]\n",
      "all num [   0 4063 1368 2175]\n",
      "cor num [   0 4061 1368 1966]\n",
      "all num [   0 1047  807  549  390  351  274  212  241  309 1199]\n",
      "cor num [ 0 25 32 25 15 15 22  9 14 16 56]\n"
     ]
    }
   ],
   "source": [
    "print_statistic_candidate(candidate,valid_y)\n",
    "print_statistic_candidate(modify_candidate,valid_y)\n",
    "print_statistic_candidate(modify_candidate_u,valid_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9663506227923406"
      ]
     },
     "execution_count": 284,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modify_candidate_u = copy_candidate(candidate)\n",
    "def remove_candidate_by_user(candidate,valid, train, deresase_to, le):\n",
    "    u = train.groupby(\"user_id\")[\"shop_id\"].count().reset_index().sort_values(\"shop_id\",ascending=False)\n",
    "    u.rename(columns={\"shop_id\":\"count\"},inplace=True)\n",
    "    uids = u[u[\"count\"] >= (train.dayofyear.unique().shape[0] / 2)].user_id.values\n",
    "    for _ind,_can in enumerate(candidate):\n",
    "        if len(_can) > deresase_to:\n",
    "            _uid = valid.iloc[_ind].user_id\n",
    "            if _uid in uids:\n",
    "                new_set = []\n",
    "#                 print le.transform(valid.iloc[[_ind]][\"shop_id\"].values)\n",
    "#                 print _can\n",
    "                _max = np.bincount(le.transform(train[train.user_id == _uid].shop_id.values))\n",
    "                \n",
    "                while len(new_set) < deresase_to:\n",
    "                    _m = np.argmax(_max)\n",
    "                    if _max[_m] != 0:\n",
    "                        if _m in _can:\n",
    "                            new_set.append(_m)\n",
    "                    else:\n",
    "                        break\n",
    "                    _max[_m] = 0\n",
    "                \n",
    "                if len(new_set) < deresase_to:\n",
    "                    for _c in _can:\n",
    "                        if _c not in new_set:\n",
    "                            new_set.append(_c)\n",
    "                        if len(new_set) == deresase_to:\n",
    "                            break\n",
    "                candidate[_ind] = np.asarray(new_set)\n",
    "                \n",
    "remove_candidate_by_user(modify_candidate_u, valid,train,deresase_to=2, le=le)\n",
    "acc_top_k(modify_candidate_u, valid_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "78"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(can_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "many size 0\n",
      "two size 2687\n",
      "single size 1773\n",
      "two set length 249\n",
      "many set length 0\n",
      "all num [   0 1773 2687]\n",
      "cor num [   0 1771 2556]\n"
     ]
    }
   ],
   "source": [
    "two_set,many_set = candidate_set(candidate)\n",
    "print_statistic_candidate(candidate,valid_y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "245\n"
     ]
    }
   ],
   "source": [
    "most_set = set()\n",
    "for _can in candidate:\n",
    "    if len(_can) == 10:\n",
    "        most_set.add(\",\".join(sorted(list(_can.astype(str)))))\n",
    "print len(most_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/245\n",
      "20/245\n",
      "40/245\n",
      "60/245\n",
      "80/245\n",
      "100/245\n",
      "120/245\n",
      "140/245\n",
      "160/245\n",
      "180/245\n",
      "200/245\n",
      "220/245\n",
      "240/245\n"
     ]
    }
   ],
   "source": [
    "most_dict = {}\n",
    "most_dict_indexs={}\n",
    "for _ind, _s in enumerate(most_set):\n",
    "    if _ind % 20 == 0:\n",
    "        print \"{}/{}\".format(_ind,len(most_set))\n",
    "    _rf,_indexs = many_fit(_s,train_wifi_all_x,np.concatenate([train_lonlats,train_wh],axis=1), train_b_y,train_y)\n",
    "    most_dict[_s] = _rf\n",
    "    most_dict_indexs[_s] = _indexs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "264\n",
      "98.0\n",
      "192\n"
     ]
    }
   ],
   "source": [
    "take = 5\n",
    "most_predict = []\n",
    "most_real = []\n",
    "c = 0\n",
    "s = 0\n",
    "for _sample_index, _can in enumerate(candidate):\n",
    "    if len(_can) == 10:\n",
    "        s += 1\n",
    "        _s = \",\".join(sorted(list(_can.astype(str))))\n",
    "        _indexs = most_dict_indexs[_s]\n",
    "        _rf = most_dict[_s]\n",
    "        pvalid_x = valid_wifi_all_x[[_sample_index]][:,_indexs] \n",
    "        pvalid_x = np.concatenate([pvalid_x, valid_lonlats[[_sample_index]],valid_wh[[_sample_index]]],axis=1)\n",
    "        _p1 = _rf.predict_proba(pvalid_x)[0]\n",
    "        ps=[]\n",
    "        l = zip(_p1,_can)\n",
    "        l = sorted(l,key=lambda x:-x[0])\n",
    "        _ind = 0\n",
    "        while(len(ps) != take):\n",
    "            ps.append(l[_ind][1])\n",
    "            _ind += 1\n",
    "        most_predict.append(ps)\n",
    "        most_real.append(valid_y[_sample_index])\n",
    "#         print list(_can)\n",
    "#         print ps\n",
    "#         print valid_y[_sample_index]\n",
    "        if valid_y[_sample_index] in list(_can[:take]):\n",
    "            c+=1\n",
    "#         print \"===================\"\n",
    "print s\n",
    "print acc_top_k(most_predict,most_real) * s\n",
    "print c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 找出30分钟内交易多次的根据训练集剔除不可能的候选集合\n",
    "def remove_candidate(x,valid,shops_cor_time,modify_candidate, le, time = 30, stay = 3, continue_search=True):\n",
    "    times = valid[(valid.user_id == x[0]) & (valid.dayofyear == x[1])][[\"hour_minute\",\"sample_index\"]].values\n",
    "    for _i,_s in enumerate(times):\n",
    "        for _j,_s2 in enumerate(times):\n",
    "            if _i == _j:\n",
    "                continue\n",
    "            else:\n",
    "                if abs(_s[0] - _s2[0]) > time:\n",
    "                    continue\n",
    "                if len(modify_candidate[_s[1]]) == 1:\n",
    "                    if len(modify_candidate[_s2[1]]) == 1 or len(modify_candidate[_s2[1]]) == 2 or len(modify_candidate[_s2[1]]) == 3:\n",
    "                        continue\n",
    "                    _s_c = modify_candidate[_s[1]]\n",
    "                    _ss_c = modify_candidate[_s2[1]]\n",
    "                    sid = le.inverse_transform(_s_c)[0]\n",
    "                    set1 = set(shops_cor_time[sid].keys())\n",
    "                    set2 = set(le.inverse_transform(_ss_c))\n",
    "                    sort =  zip(shops_cor_time[sid].keys(),shops_cor_time[sid].values())\n",
    "                    sort =  sorted(sort,key=lambda x: -x[1])\n",
    "                    \n",
    "                    last_set = set()\n",
    "                    _last_n = -1\n",
    "                    \n",
    "                    if continue_search:\n",
    "                        search_sid = []\n",
    "                        searched_sid = set()\n",
    "                        searched_sid.add(sid)\n",
    "                    \n",
    "                    for _sn,_n in sort:\n",
    "                        if _n == _last_n:# 和上一个数量相同的话\n",
    "                            if _sn in set2:\n",
    "                                last_set.add(_sn)\n",
    "                                search_sid.append(_sn)\n",
    "                                continue\n",
    "                        # 和上一个数量不同了\n",
    "                        if len(last_set) >= stay:\n",
    "                            break\n",
    "                        if _sn in set2:\n",
    "                            last_set.add(_sn)\n",
    "                            search_sid.append(_sn)\n",
    "                            _last_n = _n\n",
    "                            \n",
    "                    # 从新加入的为基准继续搜索寻找\n",
    "                    while continue_search and len(last_set) < stay and len(search_sid) != 0:\n",
    "                        sid = search_sid.pop(0)\n",
    "                        if sid in searched_sid:\n",
    "                            continue\n",
    "                        searched_sid.add(sid)\n",
    "                        set1 = set(shops_cor_time[sid].keys())\n",
    "                        sort =  zip(shops_cor_time[sid].keys(),shops_cor_time[sid].values())\n",
    "                        sort =  sorted(sort,key=lambda x: -x[1])\n",
    "\n",
    "                        _last_n = -1\n",
    "                        for _sn,_n in sort:\n",
    "                            if _n == _last_n:# 和上一个数量相同的话\n",
    "                                if _sn in set2 and _sn not in last_set:\n",
    "                                    last_set.add(_sn)\n",
    "                                    search_sid.append(_sn)\n",
    "                                    continue\n",
    "                                    \n",
    "                            # 和上一个数量不同了\n",
    "                            if len(last_set) >= stay:\n",
    "                                break\n",
    "                                \n",
    "                            if _sn in set2 and _sn not in last_set:\n",
    "                                last_set.add(_sn)\n",
    "                                search_sid.append(_sn)\n",
    "                                _last_n = _n\n",
    "                                \n",
    "                    while len(last_set) < stay:\n",
    "                        _can = modify_candidate[_s2[1]]\n",
    "                        _can = le.inverse_transform(_can)\n",
    "                        for _c in _can:\n",
    "                            last_set.add(_c)\n",
    "                            if len(last_set) == stay:\n",
    "                                break\n",
    "                        break\n",
    "                                \n",
    "                    modify_candidate[_s2[1]] = le.transform(list(last_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "shops_cor_60time = {}\n",
    "shops_cor_1000time = {}\n",
    "shops_cor_30time = {}\n",
    "_ = map(lambda x:corr_analyse_in_time(x,train, shops_cor_30time,time=30),tud[[\"user_id\", \"dayofyear\"]].values)\n",
    "_ = map(lambda x:corr_analyse_in_time(x,train, shops_cor_60time,time=60),tud[[\"user_id\", \"dayofyear\"]].values)\n",
    "_ = map(lambda x:corr_analyse_in_time(x,train, shops_cor_1000time,time=1000),tud[[\"user_id\", \"dayofyear\"]].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.984304932735426"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modify_candidate = copy_candidate(modify_candidate_u)        \n",
    "#一天内\n",
    "_ = map(lambda x:remove_candidate(x,valid,shops_cor_1000time,modify_candidate,le, stay=6, time = 1000),vud[[\"user_id\",\"dayofyear\"]].values)\n",
    "acc_top_k(modify_candidate, valid_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9733183856502242"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modify_candidate_h = copy_candidate(modify_candidate_u)\n",
    "def remove_candidate_by_hour(candidate,valid,ht, deresase_to, le):\n",
    "    for _ind,_can in enumerate(candidate):\n",
    "        if len(_can) > deresase_to:\n",
    "            _can = le.inverse_transform(_can)\n",
    "            _h = valid.iloc[[_ind]][\"hour\"].values[0]\n",
    "            _temp = ht[ht.hour==_h][\"shop_id\"].values\n",
    "            new_set = []\n",
    "            for _sn in _temp:\n",
    "                if _sn in _can:\n",
    "                    new_set.append(_sn)\n",
    "                if len(new_set) >= deresase_to:\n",
    "                    break\n",
    "            if len(new_set) >= deresase_to:\n",
    "                candidate[_ind] = le.transform(new_set)\n",
    "                \n",
    "remove_candidate_by_hour(modify_candidate_h, valid,ht,deresase_to=7, le=le)\n",
    "acc_top_k(modify_candidate_h, valid_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9838565022421525"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modify_candidate2 = copy_candidate(modify_candidate)        \n",
    "#60分钟内\n",
    "_ = map(lambda x:remove_candidate(x,valid,shops_cor_60time,modify_candidate2,le, stay=5, time = 60),vud[[\"user_id\",\"dayofyear\"]].values)\n",
    "acc_top_k(modify_candidate2, valid_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9827354260089686"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modify_candidate3 = copy_candidate(modify_candidate2)        \n",
    "#30分钟内\n",
    "_ = map(lambda x:remove_candidate(x,valid,shops_cor_30time,modify_candidate3,le, stay=3, time = 30),vud[[\"user_id\",\"dayofyear\"]].values)\n",
    "acc_top_k(modify_candidate3, valid_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all num [   0 1757  943  575  300  222  133  114   79   73  264]\n",
      "cor num [   0 1755  942  575  298  222  132  114   78   69  211]\n",
      "all num [   0 1757  943  599  294  218  130  112   78   70  259]\n",
      "cor num [   0 1755  942  597  292  218  130  112   77   66  206]\n",
      "all num [   0 1757  943  599  294  218  163  107   72   66  241]\n",
      "cor num [   0 1755  942  597  292  218  156  105   71   62  192]\n",
      "all num [   0 1757  943  599  294  252  130  106   72   66  241]\n",
      "cor num [   0 1755  942  597  292  244  129  104   71   62  192]\n",
      "all num [   0 1757  943  642  284  219  130  106   72   66  241]\n",
      "cor num [   0 1755  942  630  283  215  129  104   71   62  192]\n",
      "all num [   0 1757  943  642  284  219  130  452   11    6   16]\n",
      "cor num [   0 1755  942  630  283  215  129  352   10    5   13]\n"
     ]
    }
   ],
   "source": [
    "print_statistic_candidate(candidate,valid_y)\n",
    "print_statistic_candidate(modify_candidate_u,valid_y)\n",
    "print_statistic_candidate(modify_candidate,valid_y)\n",
    "print_statistic_candidate(modify_candidate2,valid_y)\n",
    "print_statistic_candidate(modify_candidate3,valid_y)\n",
    "print_statistic_candidate(modify_candidate4,valid_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "many size 1760\n",
      "two size 943\n",
      "single size 1757\n",
      "two set length 109\n",
      "many set length 958\n"
     ]
    }
   ],
   "source": [
    "two_set,many_set = candidate_set(candidate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/329\n",
      "20/329\n",
      "40/329\n",
      "60/329\n",
      "80/329\n",
      "100/329\n",
      "120/329\n",
      "140/329\n",
      "160/329\n",
      "180/329\n",
      "200/329\n",
      "220/329\n",
      "240/329\n",
      "260/329\n",
      "280/329\n",
      "300/329\n",
      "320/329\n"
     ]
    }
   ],
   "source": [
    "# 2fenlei\n",
    "two_dict = {}\n",
    "two_dict_indexs = {}\n",
    "for _ind,_s in enumerate(two_set):\n",
    "    if _ind % 20 == 0:\n",
    "        print \"{}/{}\".format(_ind,len(two_set))\n",
    "    _ss = _s.split(\",\")\n",
    "    s1 = int(_ss[0])\n",
    "    s2 = int(_ss[1])\n",
    "    \n",
    "    s1_train = train[train_b_y[:,s1] == 1]\n",
    "    s2_train = train[train_b_y[:,s2] == 1]\n",
    "    s1_valid = valid[valid_b_y[:,s1] == 1]\n",
    "    s2_valid = valid[valid_b_y[:,s2] == 1]\n",
    "\n",
    "    s1_wifi_all_x = train_wifi_all_x[train_b_y[:,s1] == 1]\n",
    "    s2_wifi_all_x = train_wifi_all_x[train_b_y[:,s2] == 1]\n",
    "    s1_indexs = choose_strong_wifi_index(-115,6,s1_wifi_all_x)\n",
    "    s2_indexs = choose_strong_wifi_index(-115,6,s2_wifi_all_x)\n",
    "    _indexs = list(set(s1_indexs).union(set(s2_indexs)))\n",
    "    _rf = RandomForestClassifier(n_jobs=-1,n_estimators=188,class_weight=\"balanced\")\n",
    "    _train_bool_index = (train_b_y[:,s1] == 1) | (train_b_y[:,s2]==1) \n",
    "    _valid_bool_index = (valid_b_y[:,s1] == 1) | (valid_b_y[:,s2]==1) \n",
    "    ptrain_x = train_wifi_all_x[_train_bool_index][:,_indexs] \n",
    "#     pvalid_x = valid_wifi_all_x[_valid_bool_index][:,_indexs] \n",
    "    ptrain_y = train_y[_train_bool_index] \n",
    "    ptrain_x = np.concatenate([ptrain_x, train_lonlats[_train_bool_index]],axis=1)\n",
    "    _rf.fit(ptrain_x,ptrain_y)\n",
    "    two_dict[_s] = _rf\n",
    "    two_dict_indexs[_s] = _indexs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "def many_fit(s, train_wifi_all_x, other_features, train_b_y, train_y):\n",
    "    _ss = s.split(\",\")\n",
    "    s1 = int(_ss[0])\n",
    "    s1_wifi_all_x = train_wifi_all_x[train_b_y[:,s1] == 1]\n",
    "    s1_indexs = choose_strong_wifi_index(-115,6,s1_wifi_all_x)\n",
    "    _indexs = set(s1_indexs)\n",
    "    _train_bool_index =  (train_b_y[:,s1] == 1)\n",
    "    for _k in range(1,len(_ss)):\n",
    "        s2 = int(_ss[_k])\n",
    "        s2_wifi_all_x = train_wifi_all_x[train_b_y[:,s2] == 1]\n",
    "        s2_indexs = choose_strong_wifi_index(-115,6,s2_wifi_all_x)\n",
    "        _indexs = _indexs.union(set(s2_indexs))\n",
    "        _train_bool_index =  _train_bool_index | (train_b_y[:,s2] == 1)\n",
    "   \n",
    "    _indexs = list(_indexs) \n",
    "    _rf = RandomForestClassifier(n_jobs=-1,n_estimators=288,class_weight=\"balanced\")\n",
    "    ptrain_x = train_wifi_all_x[_train_bool_index][:,_indexs] \n",
    "    ptrain_y = train_y[_train_bool_index] \n",
    "    ptrain_x = np.concatenate([ptrain_x, other_features[_train_bool_index]],axis=1)\n",
    "    _rf.fit(ptrain_x,ptrain_y)\n",
    "    return _rf, _indexs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/951\n",
      "20/951\n",
      "40/951\n",
      "60/951\n",
      "80/951\n",
      "100/951\n",
      "120/951\n",
      "140/951\n",
      "160/951\n",
      "180/951\n",
      "200/951\n",
      "220/951\n",
      "240/951\n",
      "260/951\n",
      "280/951\n",
      "300/951\n",
      "320/951\n",
      "340/951\n",
      "360/951\n",
      "380/951\n",
      "400/951\n",
      "420/951\n",
      "440/951\n",
      "460/951\n",
      "480/951\n",
      "500/951\n",
      "520/951\n",
      "540/951\n",
      "560/951\n",
      "580/951\n",
      "600/951\n",
      "620/951\n",
      "640/951\n",
      "660/951\n",
      "680/951\n",
      "700/951\n",
      "720/951\n",
      "740/951\n",
      "760/951\n",
      "780/951\n",
      "800/951\n",
      "820/951\n",
      "840/951\n",
      "860/951\n",
      "880/951\n",
      "900/951\n",
      "920/951\n",
      "940/951\n"
     ]
    }
   ],
   "source": [
    "#多分类\n",
    "many_dict = {}\n",
    "many_dict_indexs = {}\n",
    "for _ind,_s in enumerate(many_set):\n",
    "    if _ind % 20 == 0:\n",
    "        print \"{}/{}\".format(_ind,len(many_set))\n",
    "    _rf,_indexs = many_fit(_s,train_wifi_all_x,train_lonlats, train_b_y,train_y)\n",
    "    many_dict[_s] = _rf\n",
    "    many_dict_indexs[_s] = _indexs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "200\n",
      "400\n",
      "600\n",
      "800\n",
      "1000\n",
      "1200\n",
      "1400\n",
      "1600\n",
      "1800\n",
      "2000\n",
      "2200\n",
      "2400\n",
      "2600\n",
      "2800\n",
      "3000\n",
      "3200\n",
      "3400\n",
      "3600\n",
      "3800\n",
      "4000\n",
      "4200\n",
      "4400\n"
     ]
    }
   ],
   "source": [
    "last_predict,ones,twos,manys = predict_candidate(candidate,valid_y,two_dict,two_dict_indexs,many_dict,many_dict_indexs,2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "two 3526 3751\n",
      "two error Counter({'26,41': 32, '52,55': 18, '51,55': 18, '33,61': 9, '14,55': 8, '53,76': 6, '14,79': 5, '33,55': 5, '14,45': 5, '45,79': 4, '14,76': 4, '45,55': 4, '5,7': 3, '64,75': 3, '49,61': 3, '42,72': 3, '51,64': 3, '35,5': 3, '48,55': 3, '18,33': 3, '58,68': 2, '14,20': 2, '14,18': 2, '60,61': 2, '17,42': 2, '0,55': 2, '26,34': 2, '13,27': 2, '18,2': 2, '14,67': 2, '28,55': 2, '55,61': 2, '14,57': 2, '14,51': 2, '32,55': 2, '56,59': 2, '63,79': 1, '58,64': 1, '14,28': 1, '67,77': 1, '14,17': 1, '5,6': 1, '32,42': 1, '41,5': 1, '43,55': 1, '5,78': 1, '68,71': 1, '45,62': 1, '45,63': 1, '28,69': 1, '49,52': 1, '39,5': 1, '33,64': 1, '57,76': 1, '51,73': 1, '17,51': 1, '22,45': 1, '46,68': 1, '28,53': 1, '22,55': 1, '55,64': 1, '55,67': 1, '55,66': 1, '55,63': 1, '33,49': 1, '26,29': 1, '42,75': 1, '14,58': 1, '42,70': 1, '32,75': 1, '18,59': 1, '55,74': 1, '55,72': 1, '55,70': 1, '51,70': 1, '55,79': 1, '14,42': 1, '32,64': 1, '42,55': 1, '45,59': 1, '59,79': 1, '46,51': 1, '50,69': 1, '26,68': 1, '32,51': 1, '44,45': 1, '29,41': 1})\n"
     ]
    }
   ],
   "source": [
    "two_predict,two_real,two_error_sample_index,two_errors = twos\n",
    "print \"two\", (np.asarray(two_predict) == np.asarray(two_real)).sum(), len(two_predict)\n",
    "print \"two error\", Counter(two_errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final acc"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'bool' object has no attribute 'sum'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-231-b6ee4bd6611a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtwo_predict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtwo_real\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtwo_error_sample_index\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtwo_errors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtwos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mmany_predict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmany_real\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmany_error_sample_index\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmany_errors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmanys\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mprint\u001b[0m \u001b[0;34m\"final acc\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlast_predict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvalid_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;32mprint\u001b[0m \u001b[0;34m\"one\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mone_predict\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mone_real\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mone_predict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mprint\u001b[0m \u001b[0;34m\"two\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtwo_predict\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtwo_real\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtwo_predict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/zhongjianlv/ccf_mayi/notebook/util.py\u001b[0m in \u001b[0;36macc\u001b[0;34m(pred, y)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0macc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mpred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'bool' object has no attribute 'sum'"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "one_predict,one_real,one_error_sample_index,one_errors = ones\n",
    "\n",
    "many_predict,many_real,many_error_sample_index,many_errors = manys\n",
    "print \"final acc\", acc(np.asarray(last_predict),valid_y)\n",
    "print \"one\", (np.asarray(one_predict) == np.asarray(one_real)).sum(), len(one_predict)\n",
    "print \"two\", (np.asarray(two_predict) == np.asarray(two_real)).sum(), len(two_predict)\n",
    "print \"many\", (np.asarray(many_predict) == np.asarray(many_real)).sum(), len(many_predict)\n",
    "print \"one error\", Counter(one_errors)\n",
    "print \"two error\", Counter(two_errors)\n",
    "print \"many error\", Counter(many_errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_candidate(candidate, valid_y, two_dict, two_dict_indexes, many_dict, many_dict_indexs, predict_candi_length=-1):\n",
    "    # 在all_rf产生的候选集合中用那一部分商场训练预测\n",
    "    if predict_candi_length is None or predict_candi_length < 1:\n",
    "        predict_candi_length = -1\n",
    "    last_predict_by_candidates = []\n",
    "    single_predict = []\n",
    "    two_predict = []\n",
    "    many_predict = []\n",
    "    single_real = []\n",
    "    two_real = []\n",
    "    many_real = []\n",
    "    one_error_sample_index = []\n",
    "    two_error_sample_index = []\n",
    "    many_error_sample_index = []\n",
    "    one_errors = []\n",
    "    two_errors = []\n",
    "    many_errors = []\n",
    "    for _sample_index,_can in enumerate(candidate):\n",
    "        if _sample_index % 200 ==0:\n",
    "            print _sample_index\n",
    "        if len(_can) == 1 and (predict_candi_length == 1 or predict_candi_length == -1):\n",
    "            last_predict_by_candidates.append(_can[0])\n",
    "            single_predict.append(_can[0])\n",
    "            single_real.append(valid_y[_sample_index])\n",
    "            if _can[0] != [valid_y[_sample_index]]:\n",
    "                one_errors.append(_can[0])\n",
    "                one_error_sample_index.append(_sample_index)\n",
    "        else:\n",
    "            if len(_can) == 2 and (predict_candi_length == 2 or predict_candi_length == -1):\n",
    "                s1 = _can[0]\n",
    "                s2 = _can[1]\n",
    "                _k = \",\".join(sorted(list(_can.astype(str))))\n",
    "                _indexs = two_dict_indexes[_k]\n",
    "                _rf = two_dict[_k]\n",
    "                pvalid_x = valid_wifi_all_x[[_sample_index]][:,_indexs] \n",
    "                pvalid_x = np.concatenate([pvalid_x, valid_lonlats[[_sample_index]]],axis=1)\n",
    "                _p1 = _rf.predict(pvalid_x)[0]\n",
    "                last_predict_by_candidates.append(_p1)\n",
    "                two_predict.append(_p1)\n",
    "                two_real.append(valid_y[_sample_index])\n",
    "                if _p1 != valid_y[_sample_index]:\n",
    "                    two_errors.append(\",\".join(sorted(list(_can.astype(str)))))\n",
    "                    two_error_sample_index.append(_sample_index)\n",
    "            elif len(_can) > 2 and (predict_candi_length > 2 or predict_candi_length == -1):\n",
    "                _s = \",\".join(sorted(list(_can.astype(str))))\n",
    "                _indexs = many_dict_indexs[_s]\n",
    "                _rf = many_dict[_s]\n",
    "                pvalid_x = valid_wifi_all_x[[_sample_index]][:,_indexs] \n",
    "                pvalid_x = np.concatenate([pvalid_x, valid_lonlats[[_sample_index]]],axis=1)\n",
    "                _p1 = _rf.predict(pvalid_x)[0]\n",
    "                last_predict_by_candidates.append(_p1)\n",
    "                many_predict.append(_p1)\n",
    "                many_real.append(valid_y[_sample_index])\n",
    "                if _p1 != valid_y[_sample_index]:\n",
    "                    many_errors.append(_s)\n",
    "                    many_error_sample_index.append(_sample_index)\n",
    "    return last_predict_by_candidates, (single_predict,single_real,one_error_sample_index,one_errors),(two_predict,two_real,two_error_sample_index,two_errors),(many_predict,many_real,many_error_sample_index,many_errors)         \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
