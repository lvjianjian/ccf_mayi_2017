{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from util import *\n",
    "from sklearn.preprocessing import LabelEncoder,LabelBinarizer\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\n",
    "from sklearn.svm import SVC\n",
    "from mlxtend.classifier import StackingCVClassifier,StackingClassifier\n",
    "from sklearn.multiclass import  OneVsOneClassifier,OneVsRestClassifier\n",
    "from sklearn.model_selection import train_test_split, KFold, StratifiedKFold\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import GradientBoostingClassifier,AdaBoostClassifier\n",
    "from sklearn.preprocessing import StandardScaler,Normalizer\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "pd.set_option('display.max_colwidth',1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_all = load_train()\n",
    "test_all = load_testA()\n",
    "preprocess_basic_time(train_all)\n",
    "preprocess_basic_wifi(train_all)\n",
    "shop_info = load_shop_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_k(pba,k = 3, min_proba = 0):\n",
    "    _pba = pba.copy()\n",
    "    def top_k(x,k):\n",
    "        rs = []\n",
    "        for _ in range(k):\n",
    "            ind = np.argmax(x)\n",
    "            if x[ind] < min_proba:\n",
    "                continue\n",
    "            rs.append(ind)\n",
    "            x[ind] = 0\n",
    "        return rs\n",
    "    r = map(lambda x: top_k(x,k), _pba)\n",
    "    return r\n",
    "\n",
    "def acc_top_k(candidate, y):\n",
    "    all_size = len(candidate)\n",
    "    cor = 0\n",
    "    for _can,_true in zip(candidate,y):\n",
    "        if _true in _can:\n",
    "            cor += 1\n",
    "    return float(cor) / all_size\n",
    "\n",
    "def corr_analyse_in_time(x, train, shops_cor_one_day,time=30): # 一天交易多次， 去某个商店在time分钟内可能去哪些商店\n",
    "    shopids = train[(train.user_id == x[0]) & (train.dayofyear == x[1])][[\"shop_id\",\"hour_minute\"]].values\n",
    "    for _i,_s in enumerate(shopids):\n",
    "        for _j,_s2 in enumerate(shopids):\n",
    "            if _i == _j:\n",
    "                continue\n",
    "            else:\n",
    "                if abs(_s[1] - _s2[1]) > time:\n",
    "                    continue\n",
    "                if _s[0] not in shops_cor_one_day:\n",
    "                    shops_cor_one_day[_s[0]] = {}\n",
    "                if _s2[0] not in shops_cor_one_day[_s[0]]:\n",
    "                    shops_cor_one_day[_s[0]][_s2[0]] = 0\n",
    "                shops_cor_one_day[_s[0]][_s2[0]] += 1\n",
    "    \n",
    "\n",
    "\n",
    "def copy_candidate(old_candidate):\n",
    "    new_candidate = []\n",
    "    for _nda in old_candidate:\n",
    "        new_candidate.append(_nda.copy())\n",
    "    return new_candidate\n",
    "\n",
    "            \n",
    "def statistic_candidate(candidate, valid_y = None):\n",
    "    candi_num = [len(_can) for _can in candidate]\n",
    "    can_num = np.bincount(candi_num)\n",
    "    correct = np.zeros((len(can_num),))\n",
    "    if valid_y is not None:\n",
    "        for _can, _true in zip(candidate,valid_y):\n",
    "            if _true in _can:\n",
    "                correct[len(_can)] += 1\n",
    "        return can_num, correct.astype(int)\n",
    "    else:\n",
    "        return can_num,None  \n",
    "    \n",
    "def print_statistic_candidate(candidate, valid_y = None):\n",
    "    can_num,cor_num = statistic_candidate(candidate, valid_y)\n",
    "    print \"all num\",can_num\n",
    "    if valid_y is not None:\n",
    "        print \"cor num\",cor_num\n",
    "        \n",
    "def candidate_set(candidate):\n",
    "    # 在all_rf产生的候选集合中用那一部分商场训练预测\n",
    "    many = 0\n",
    "    single = 0\n",
    "    two = 0\n",
    "    two_set = set()\n",
    "    many_set = set()\n",
    "    for _sample_index,_can in enumerate(candidate):\n",
    "        if len(_can) ==1:\n",
    "            single += 1\n",
    "        else:\n",
    "            if len(_can) == 2:\n",
    "                two_set.add(\",\".join(sorted(list(_can.astype(str)))))\n",
    "                two +=1\n",
    "            else:\n",
    "                many_set.add(\",\".join(sorted(list(_can.astype(str)))))\n",
    "                many +=1\n",
    "    print \"many size\",many\n",
    "    print \"two size\",two\n",
    "    print \"single size\",single\n",
    "    print \"two set length\",len(two_set)\n",
    "    print \"many set length\",len(many_set)\n",
    "    return two_set,many_set\n",
    "\n",
    "\n",
    "def forward_search(estimetor, trainx,trainy,fix_trainx,validx,validy,fix_validx):\n",
    "    choose = []\n",
    "    best_acc=0\n",
    "    cc = 0\n",
    "    while cc != -1:\n",
    "        cc = -1\n",
    "        for _i in range(trainx.shape[1]):\n",
    "            if _i not in choose:\n",
    "                curr = choose + [_i]\n",
    "                ptrainx = np.concatenate([trainx[:,curr], fix_trainx],axis=1)\n",
    "                pvalidx = np.concatenate([validx[:,curr], fix_validx],axis=1)\n",
    "                estimetor.fit(ptrainx,trainy)\n",
    "                _acc = acc(estimetor.predict(pvalidx),validy)\n",
    "                if _acc > best_acc:\n",
    "                    best_acc = _acc\n",
    "                    cc = _i\n",
    "        if cc != -1:\n",
    "#             print \"choose\", cc\n",
    "#             print \"best acc\", best_acc\n",
    "            choose.append(cc)\n",
    "        if best_acc == 1:\n",
    "            break\n",
    "    return choose, best_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "mall_id = \"m_4422\" # 6587 ,9 21 36 72 77 86 \n",
    "train = train_all[train_all.mall_id == mall_id]\n",
    "# label\n",
    "y = train.shop_id.values\n",
    "le = LabelEncoder().fit(y)\n",
    "y = le.transform(y)\n",
    "#split\n",
    "_train_index, _valid_index = get_last_one_week_index(train)\n",
    "valid = train.iloc[_valid_index]\n",
    "train = train.iloc[_train_index]\n",
    "#wifi info\n",
    "df, (train_index, train_use_wifi, train_matrix), (test_index, test_use_wifi, test_matrix) = get_wifi_cache2(mall_id)\n",
    "train_wifi_all_x = train_matrix[_train_index]\n",
    "valid_wifi_all_x = train_matrix[_valid_index]\n",
    "valid_y = y[_valid_index]\n",
    "train_y = y[_train_index]\n",
    "train_lonlats = train[[\"longitude\",\"latitude\"]].values\n",
    "valid_lonlats = valid[[\"longitude\",\"latitude\"]].values\n",
    "train_wh = train[[\"weekday\",\"hour\"]].values\n",
    "valid_wh = valid[[\"weekday\",\"hour\"]].values\n",
    "train_w = train[[\"weekday\"]].values\n",
    "valid_w = valid[[\"weekday\"]].values\n",
    "train_h = train[[\"hour\"]].values\n",
    "valid_h = valid[[\"hour\"]].values\n",
    "\n",
    "train[\"dayofyear\"] = train.dt.dt.dayofyear\n",
    "valid[\"dayofyear\"] = valid.dt.dt.dayofyear\n",
    "\n",
    "indexs = choose_strong_wifi_index(-90,6,train_wifi_all_x)\n",
    "train_x = np.concatenate([train_wifi_all_x[:,indexs],train_lonlats,train_wh],axis=1)\n",
    "valid_x = np.concatenate([valid_wifi_all_x[:,indexs],valid_lonlats,valid_wh],axis=1)\n",
    "lb = LabelBinarizer().fit(y)\n",
    "train_b_y = lb.transform(train_y)\n",
    "valid_b_y = lb.transform(valid_y)\n",
    "\n",
    "train_times = train[[\"longitude\",\"latitude\",\"is_weekend\"]].values\n",
    "valid_times = valid[[\"longitude\",\"latitude\",\"is_weekend\"]].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid acc 0.772634318647\n",
      "rf oob score 0.794818383023\n"
     ]
    }
   ],
   "source": [
    "#  多分类\n",
    "rf_all = RandomForestClassifier(n_estimators=500,n_jobs=-1,class_weight=\"balanced\", random_state=2017,oob_score=True)\n",
    "_train_all_x = np.concatenate([train_wifi_all_x[:, indexs], train_lonlats, train_wh],axis=1)\n",
    "valid_all_x = np.concatenate([valid_wifi_all_x[:, indexs], valid_lonlats, valid_wh],axis=1)\n",
    "_train_y = train_y.copy()\n",
    "rf_all.fit(_train_all_x,_train_y)\n",
    "rf_all_pba = rf_all.predict_proba(valid_all_x)\n",
    "print \"valid acc\", acc(rf_all.predict(valid_all_x),valid_y)\n",
    "print \"rf oob score\", rf_all.oob_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.854991634133\n",
      "many size 0\n",
      "two size 4333\n",
      "single size 1046\n",
      "two set length 326\n",
      "many set length 0\n",
      "all num [   0 1046 4333]\n",
      "cor num [   0 1046 3553]\n"
     ]
    }
   ],
   "source": [
    "# 候选2 个, 最小概率0.02\n",
    "candidate = get_top_k(rf_all_pba, 2, 0.02)\n",
    "candidate = [rf_all.classes_.take(_can) for _can in candidate]\n",
    "print acc_top_k(candidate, valid_y)\n",
    "two_set,_= candidate_set(candidate)\n",
    "print_statistic_candidate(candidate,valid_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/326\n",
      "20/326\n",
      "40/326\n",
      "60/326\n",
      "80/326\n",
      "100/326\n",
      "120/326\n",
      "140/326\n",
      "160/326\n",
      "180/326\n",
      "200/326\n",
      "220/326\n",
      "240/326\n",
      "260/326\n",
      "280/326\n",
      "300/326\n",
      "320/326\n"
     ]
    }
   ],
   "source": [
    "# 2fenlei\n",
    "two_dict = {}\n",
    "two_dict_indexs = {}\n",
    "ratio = 0.1\n",
    "for _ind,_s in enumerate(two_set):\n",
    "    if _ind % 20 == 0:\n",
    "        print \"{}/{}\".format(_ind,len(two_set))\n",
    "    _ss = _s.split(\",\")\n",
    "    s1 = int(_ss[0])\n",
    "    s2 = int(_ss[1])\n",
    "    \n",
    "    s1_train = train[train_b_y[:,s1] == 1]\n",
    "    s2_train = train[train_b_y[:,s2] == 1]\n",
    "    s1_valid = valid[valid_b_y[:,s1] == 1]\n",
    "    s2_valid = valid[valid_b_y[:,s2] == 1]\n",
    "\n",
    "    s1_wifi_all_x = train_wifi_all_x[train_b_y[:,s1] == 1]\n",
    "    s2_wifi_all_x = train_wifi_all_x[train_b_y[:,s2] == 1]\n",
    "    sp1 = int(s1_wifi_all_x.shape[0] * ratio)\n",
    "    sp2 = int(s2_wifi_all_x.shape[0] * ratio)\n",
    "    s1_indexs = choose_strong_wifi_index(-115,sp1,s1_wifi_all_x)\n",
    "    s2_indexs = choose_strong_wifi_index(-115,sp2,s2_wifi_all_x)\n",
    "    _indexs = list(set(s1_indexs).union(set(s2_indexs)))\n",
    "    _rf = RandomForestClassifier(n_jobs=-1,n_estimators=188,random_state=2017)\n",
    "    _train_bool_index = (train_b_y[:,s1] == 1) | (train_b_y[:,s2]==1) \n",
    "    _valid_bool_index = (valid_b_y[:,s1] == 1) | (valid_b_y[:,s2]==1) \n",
    "    ptrain_x = train_wifi_all_x[_train_bool_index][:,_indexs] \n",
    "#     pvalid_x = valid_wifi_all_x[_valid_bool_index][:,_indexs] \n",
    "    ptrain_y = train_y[_train_bool_index] \n",
    "    ptrain_x = np.concatenate([ptrain_x,\n",
    "                               train_lonlats[_train_bool_index],\n",
    "                               train_times[_train_bool_index]\n",
    "                               ],axis=1)\n",
    "    _rf.fit(ptrain_x,ptrain_y)\n",
    "    two_dict[_s] = _rf\n",
    "    two_dict_indexs[_s] = _indexs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "200\n",
      "400\n",
      "600\n",
      "800\n",
      "1000\n",
      "1200\n",
      "1400\n",
      "1600\n",
      "1800\n",
      "2000\n",
      "2200\n",
      "2400\n",
      "2600\n",
      "2800\n",
      "3000\n",
      "3200\n",
      "3400\n",
      "3600\n",
      "3800\n",
      "4000\n",
      "4200\n",
      "4400\n",
      "4600\n",
      "4800\n",
      "5000\n",
      "5200\n",
      "final acc 0.776910206358\n",
      "one 1046 1046\n",
      "two 3133 4333\n",
      "one error Counter()\n",
      "two error Counter({'68,70': 31, '70,75': 26, '66,74': 26, '44,53': 21, '67,75': 19, '67,72': 18, '36,50': 18, '25,76': 13, '34,67': 11, '23,41': 11, '70,76': 10, '2,64': 9, '66,67': 9, '57,58': 8, '72,75': 8, '65,70': 7, '76,77': 7, '34,65': 6, '34,66': 6, '28,62': 6, '91,92': 6, '65,71': 6, '20,31': 5, '25,65': 5, '11,33': 5, '67,70': 4, '22,42': 4, '25,70': 4, '20,89': 4, '68,75': 3, '41,65': 3, '43,46': 3, '75,76': 3, '37,65': 3, '68,76': 3, '27,50': 3, '65,75': 3, '19,87': 3, '84,85': 2, '51,92': 2, '18,3': 2, '58,6': 2, '38,74': 2, '23,29': 2, '1,10': 2, '38,66': 2, '17,5': 2, '44,92': 2, '55,79': 2, '65,74': 2, '5,57': 2, '34,75': 2, '58,63': 1, '10,32': 1, '79,84': 1, '67,74': 1, '71,76': 1, '31,65': 1, '7,76': 1, '15,19': 1, '79,90': 1, '75,86': 1, '70,77': 1, '70,72': 1, '70,71': 1, '17,39': 1, '56,78': 1, '11,16': 1, '50,58': 1, '18,58': 1, '10,14': 1, '20,61': 1, '20,66': 1, '11,48': 1, '73,76': 1, '29,65': 1, '17,62': 1, '2,5': 1, '10,30': 1, '43,92': 1, '22,65': 1, '25,75': 1, '30,54': 1, '8,90': 1, '11,50': 1, '50,92': 1, '33,50': 1, '26,57': 1, '65,72': 1, '65,76': 1, '11,83': 1, '22,3': 1, '15,8': 1, '34,74': 1, '65,66': 1, '34,70': 1, '19,90': 1, '14,30': 1, '66,75': 1, '54,73': 1, '66,77': 1, '29,41': 1, '12,58': 1, '33,36': 1})\n"
     ]
    }
   ],
   "source": [
    "last_predict,ones,twos,_ = predict_candidate(candidate,valid_y,two_dict,two_dict_indexs,None,None,-1)\n",
    "two_predict,two_real,two_error_sample_index,two_errors = twos\n",
    "one_predict,one_real,one_error_sample_index,one_errors = ones\n",
    "print \"final acc\", acc(np.asarray(last_predict),valid_y)\n",
    "print \"one\", (np.asarray(one_predict) == np.asarray(one_real)).sum(), len(one_predict)\n",
    "print \"two\", (np.asarray(two_predict) == np.asarray(two_real)).sum(), len(two_predict)\n",
    "print \"one error\", Counter(one_errors)\n",
    "print \"two error\", Counter(two_errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_candidate(candidate, valid_y, two_dict, two_dict_indexes, many_dict, many_dict_indexs, predict_candi_length=-1):\n",
    "    # 在all_rf产生的候选集合中用那一部分商场训练预测\n",
    "    if predict_candi_length is None or predict_candi_length < 1:\n",
    "        predict_candi_length = -1\n",
    "    last_predict_by_candidates = []\n",
    "    single_predict = []\n",
    "    two_predict = []\n",
    "    many_predict = []\n",
    "    single_real = []\n",
    "    two_real = []\n",
    "    many_real = []\n",
    "    one_error_sample_index = []\n",
    "    two_error_sample_index = []\n",
    "    many_error_sample_index = []\n",
    "    one_errors = []\n",
    "    two_errors = []\n",
    "    many_errors = []\n",
    "    for _sample_index,_can in enumerate(candidate):\n",
    "        if _sample_index % 200 ==0:\n",
    "            print _sample_index\n",
    "        if len(_can) == 1 and (predict_candi_length == 1 or predict_candi_length == -1):\n",
    "            last_predict_by_candidates.append(_can[0])\n",
    "            single_predict.append(_can[0])\n",
    "            single_real.append(valid_y[_sample_index])\n",
    "            if _can[0] != [valid_y[_sample_index]]:\n",
    "                one_errors.append(_can[0])\n",
    "                one_error_sample_index.append(_sample_index)\n",
    "        else:\n",
    "            if len(_can) == 2 and (predict_candi_length == 2 or predict_candi_length == -1):\n",
    "                s1 = _can[0]\n",
    "                s2 = _can[1]\n",
    "                _k = \",\".join(sorted(list(_can.astype(str))))\n",
    "                _indexs = two_dict_indexes[_k]\n",
    "                _rf = two_dict[_k]\n",
    "                pvalid_x = valid_wifi_all_x[[_sample_index]][:,_indexs] \n",
    "                pvalid_x = np.concatenate([pvalid_x, valid_lonlats[[_sample_index]], valid_times[[_sample_index]]],axis=1)\n",
    "                _p1 = _rf.predict(pvalid_x)[0]\n",
    "                last_predict_by_candidates.append(_p1)\n",
    "                two_predict.append(_p1)\n",
    "                two_real.append(valid_y[_sample_index])\n",
    "                if _p1 != valid_y[_sample_index] and (valid_y[_sample_index] == s1 or valid_y[_sample_index] == s2):\n",
    "                    two_errors.append(\",\".join(sorted(list(_can.astype(str)))))\n",
    "                    two_error_sample_index.append(_sample_index)\n",
    "            elif len(_can) > 2 and (predict_candi_length > 2 or predict_candi_length == -1):\n",
    "                _s = \",\".join(sorted(list(_can.astype(str))))\n",
    "                _indexs = many_dict_indexs[_s]\n",
    "                _rf = many_dict[_s]\n",
    "                pvalid_x = valid_wifi_all_x[[_sample_index]][:,_indexs] \n",
    "                pvalid_x = np.concatenate([pvalid_x, valid_lonlats[[_sample_index]]],axis=1)\n",
    "                _p1 = _rf.predict(pvalid_x)[0]\n",
    "                last_predict_by_candidates.append(_p1)\n",
    "                many_predict.append(_p1)\n",
    "                many_real.append(valid_y[_sample_index])\n",
    "                if _p1 != valid_y[_sample_index]:\n",
    "                    many_errors.append(_s)\n",
    "                    many_error_sample_index.append(_sample_index)\n",
    "    return last_predict_by_candidates, (single_predict,single_real,one_error_sample_index,one_errors),(two_predict,two_real,two_error_sample_index,two_errors),(many_predict,many_real,many_error_sample_index,many_errors)         \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  1, ..., 92, 93])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_all.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['s_647507'], dtype=object)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "le.inverse_transform([66])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2fenlei + 前向搜索\n",
    "two_dict = {}\n",
    "two_dict_indexs = {}\n",
    "for _ind,_s in enumerate(two_set):\n",
    "    if _ind % 20 == 0:\n",
    "        print \"{}/{}\".format(_ind,len(two_set))\n",
    "    _ss = _s.split(\",\")\n",
    "    s1 = int(_ss[0])\n",
    "    s2 = int(_ss[1])\n",
    "    \n",
    "    s1_train = train[train_b_y[:,s1] == 1]\n",
    "    s2_train = train[train_b_y[:,s2] == 1]\n",
    "    s1_valid = valid[valid_b_y[:,s1] == 1]\n",
    "    s2_valid = valid[valid_b_y[:,s2] == 1]\n",
    "\n",
    "    s1_wifi_all_x = train_wifi_all_x[train_b_y[:,s1] == 1]\n",
    "    s2_wifi_all_x = train_wifi_all_x[train_b_y[:,s2] == 1]\n",
    "    s1_indexs = choose_strong_wifi_index(-115,6,s1_wifi_all_x)\n",
    "    s2_indexs = choose_strong_wifi_index(-115,6,s2_wifi_all_x)\n",
    "    _indexs = list(set(s1_indexs).union(set(s2_indexs)))\n",
    "    _rf = RandomForestClassifier(n_jobs=-1,n_estimators=188,random_state=2017)\n",
    "    _train_bool_index = (train_b_y[:,s1] == 1) | (train_b_y[:,s2]==1) \n",
    "    _valid_bool_index = (valid_b_y[:,s1] == 1) | (valid_b_y[:,s2]==1) \n",
    "    ptrain_x = train_wifi_all_x[_train_bool_index][:,_indexs] \n",
    "    ptrain_y = train_y[_train_bool_index] \n",
    "    \n",
    "    pvalid_x = valid_wifi_all_x[_valid_bool_index][:,_indexs]\n",
    "    pvalid_y = valid_y[_valid_bool_index]\n",
    "    \n",
    "    ptrain_x = np.concatenate([ptrain_x, train_lonlats[_train_bool_index]],axis=1)\n",
    "    pvalid_x = np.concatenate([pvalid_x, valid_lonlats[_valid_bool_index]], axis=1)\n",
    "    _rf.fit(ptrain_x,ptrain_y)\n",
    "    \n",
    "    p1 = _rf.predict(pvalid_x)\n",
    "    valid_acc = acc(p1,pvalid_y)\n",
    "    \n",
    "    \n",
    "    if valid_acc < 0.995:\n",
    "        fi = zip(_rf.feature_importances_, _indexs)\n",
    "        fi = sorted(fi,key=lambda x:-x[0])\n",
    "        find_indexs = []\n",
    "        for _f in fi[:30]:\n",
    "            if isinstance(_f[1],int):\n",
    "                find_indexs.append(_f[1])\n",
    "        choose,best = forward_search(_rf,\n",
    "                       train_wifi_all_x[_train_bool_index][:,find_indexs],\n",
    "                       train_y[_train_bool_index],\n",
    "                       train_lonlats[_train_bool_index],\n",
    "                       valid_wifi_all_x[_valid_bool_index][:,find_indexs],\n",
    "                       pvalid_y,\n",
    "                       valid_lonlats[_valid_bool_index]\n",
    "                      )\n",
    "        if best > valid_acc:\n",
    "            choose = [find_indexs[_c] for _c in choose]\n",
    "            ptrain_x = np.concatenate([train_wifi_all_x[_train_bool_index][:,choose], \n",
    "                               train_lonlats[_train_bool_index],\n",
    "                              ],axis=1)\n",
    "            _indexs = choose\n",
    "           \n",
    "        \n",
    "    _rf.fit(ptrain_x,ptrain_y)\n",
    "    two_dict[_s] = _rf\n",
    "    two_dict_indexs[_s] = _indexs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
