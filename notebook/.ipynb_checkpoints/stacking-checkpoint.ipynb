{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "from util import *\n",
    "from sklearn.preprocessing import LabelEncoder,LabelBinarizer\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\n",
    "from sklearn.svm import SVC\n",
    "from mlxtend.classifier import StackingCVClassifier,StackingClassifier\n",
    "from sklearn.multiclass import  OneVsOneClassifier,OneVsRestClassifier\n",
    "from sklearn.model_selection import train_test_split, KFold, StratifiedKFold\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import GradientBoostingClassifier,AdaBoostClassifier\n",
    "from sklearn.preprocessing import StandardScaler,Normalizer\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "pd.set_option('display.max_colwidth',1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_all = load_train()\n",
    "test_all = load_testA()\n",
    "preprocess_basic_time(train_all)\n",
    "preprocess_basic_wifi(train_all)\n",
    "shop_info = load_shop_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "util.py:750: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  train[\"i_loc\"] = range(train.shape[0])\n"
     ]
    }
   ],
   "source": [
    "mall_id = \"m_7523\" # 6587 ,9 21 36 72 77 86 \n",
    "train = train_all[train_all.mall_id == mall_id]\n",
    "# label\n",
    "y = train.shop_id.values\n",
    "le = LabelEncoder().fit(y)\n",
    "y = le.transform(y)\n",
    "#split\n",
    "_train_index, _valid_index = get_last_one_week_index(train)\n",
    "valid = train.iloc[_valid_index]\n",
    "train = train.iloc[_train_index]\n",
    "#wifi info\n",
    "df, (train_index, train_use_wifi, train_matrix), (test_index, test_use_wifi, test_matrix) = get_wifi_cache2(mall_id)\n",
    "train_wifi_all_x = train_matrix[_train_index]\n",
    "valid_wifi_all_x = train_matrix[_valid_index]\n",
    "valid_y = y[_valid_index]\n",
    "train_y = y[_train_index]\n",
    "train_lonlats = train[[\"longitude\",\"latitude\"]].values\n",
    "valid_lonlats = valid[[\"longitude\",\"latitude\"]].values\n",
    "train_wh = train[[\"weekday\",\"hour\"]].values\n",
    "valid_wh = valid[[\"weekday\",\"hour\"]].values\n",
    "train_w = train[[\"weekday\"]].values\n",
    "valid_w = valid[[\"weekday\"]].values\n",
    "train_h = train[[\"hour\"]].values\n",
    "valid_h = valid[[\"hour\"]].values\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CVEstimator(object):\n",
    "    def __expansion(trainx, trainy, cv):\n",
    "        # 对样本少的进行复制扩充\n",
    "        bin = np.bincount(trainy)\n",
    "        labels = np.unique(trainy)\n",
    "        l = np.asarray(range(np.max(labels) + 1))[bin < cv]\n",
    "        l = np.intersect1d(l, labels)\n",
    "        for _l in l:\n",
    "            n = (trainy == _l).sum()\n",
    "            n = int(np.ceil(float(cv) / n - 1))\n",
    "            trainx = np.concatenate([trainx, np.tile(trainx[trainy == _l], (n, 1))], axis=0)\n",
    "            trainy = np.concatenate([trainy, np.tile(trainy[trainy == _l], (n,))], axis=0)\n",
    "        return trainx, trainy\n",
    "    \n",
    "    def __init__(self, estimator, cv = 3, use_proba = True):\n",
    "        self.estimator = estimator\n",
    "        self.cv = cv\n",
    "        self.kf = StratifiedKFold(cv,shuffle=True)\n",
    "        from sklearn.base import clone\n",
    "        self.clfs_ = [clone(self.estimator) for _ in range(self.cv)]\n",
    "        self.use_proba = use_proba\n",
    "        self.classes_ = None\n",
    "        \n",
    "        \n",
    "    def fit(self, X,y):\n",
    "#         if np.unique(y) > 2:\n",
    "#             self.multiclass = True\n",
    "#         else:\n",
    "#             self.multiclass = False\n",
    "        origin_size = y.shape[0]\n",
    "        _x,_y = expansion(X,y,self.cv)\n",
    "        self.indexs = []\n",
    "        self.ys = []\n",
    "        self.train_predicts=[]\n",
    "        for _i,(_train_index,_test_index) in enumerate(self.kf.split(_x,_y)):\n",
    "            _train_x = _x[_train_index]\n",
    "            _train_y = _y[_train_index]\n",
    "            _test_x = _x[_test_index]\n",
    "            _test_y = _y[_test_index]\n",
    "            self.clfs_[_i].fit(_train_x, _train_y)\n",
    "            self.indexs.append(_test_index)\n",
    "            self.ys.append(_test_y)\n",
    "            if self.use_proba:\n",
    "                self.train_predicts.append(self.clfs_[_i].predict_proba(_test_x))\n",
    "            else:\n",
    "                self.train_predicts.append(self.clfs_[_i].predict(_test_x))\n",
    "            if self.classes_  is None:\n",
    "                self.classes_ = self.clfs_[_i].classes_\n",
    "            else:\n",
    "                assert((self.classes_ != self.clfs_[_i].classes_).sum()==0)\n",
    "        self.indexs = np.concatenate(self.indexs, axis=0)\n",
    "        self.train_predicts = np.concatenate(self.train_predicts, axis=0)\n",
    "        self.ys = np.concatenate(self.ys,axis=0)\n",
    "        all_train_predicts = zip(self.indexs,self.ys,self.train_predicts)\n",
    "        all_train_predicts = sorted(all_train_predicts, key=lambda x: x[0])\n",
    "        all_train_predicts = all_train_predicts[:origin_size]\n",
    "        self.train_ys = np.asarray([_l[1] for _l in all_train_predicts])\n",
    "        self.train_predicts = np.asarray([_l[2] for _l in all_train_predicts])\n",
    "        \n",
    "        \n",
    "    def get_all_train_predicts(self):\n",
    "        return self.train_predicts, self.train_ys\n",
    "        \n",
    "    def predict(self,X):\n",
    "        px = self.predict_proba(X)\n",
    "        p = np.argmax(px,axis=1)\n",
    "        return self.classes_.take(p)\n",
    "        \n",
    "        \n",
    "    def predict_proba(self,X):\n",
    "        _y = []\n",
    "        for _cls in self.clfs_:\n",
    "            _y.append(_cls.predict_proba(X))\n",
    "        p = np.max(_y,axis=0)\n",
    "        return p\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rf3 string wifi + lonlat + wh\n",
    "indexs = choose_strong_wifi_index(-90,6,train_wifi_all_x)\n",
    "_train_x3 = np.concatenate([train_wifi_all_x[:,indexs],train_lonlats,train_wh],axis=1)\n",
    "_valid_x3 = np.concatenate([valid_wifi_all_x[:,indexs],valid_lonlats,valid_wh],axis=1)\n",
    "lb = LabelBinarizer().fit(y)\n",
    "_train_b_y = lb.transform(train_y)\n",
    "_valid_b_y = lb.transform(valid_y)\n",
    "\n",
    "ss = StandardScaler().fit(_train_x3)\n",
    "_train_x3_ss = ss.transform(_train_x3)\n",
    "_valid_x3_ss = ss.transform(_valid_x3)\n",
    "norm = Normalizer().fit(_train_x3)\n",
    "_train_x3_norm = norm.transform(_train_x3)\n",
    "_valid_x3_norm = norm.transform(_valid_x3)\n",
    "\n",
    "_train_x33 = np.concatenate([train_wifi_all_x[:,indexs],train_lonlats,train_w],axis=1)\n",
    "_valid_x33 = np.concatenate([valid_wifi_all_x[:,indexs],valid_lonlats,valid_w],axis=1)\n",
    "valid_index = np.asarray(range(valid.shape[0]))\n",
    "train_index = np.asarray(range(train.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_indexs(df, sorted_wifi, sp = 0.5):\n",
    "    w= sorted_wifi\n",
    "    s = gt.shape[0]\n",
    "#     print s\n",
    "#     print w\n",
    "    strong_sig_worst = s * sp\n",
    "    for _index in range(len(w)):\n",
    "        if w[_index][1] < strong_sig_worst:\n",
    "            break\n",
    "    strong_sig_choose = _index \n",
    "    choose_strong_wifi_index = [_wi[0] for _wi in w[:strong_sig_choose]]\n",
    "    assert(choose_strong_wifi_index > 5)\n",
    "    choose_strong_wifi_index = np.array(choose_strong_wifi_index)\n",
    "\n",
    "    indexs = df[df.wifi_name.isin(choose_strong_wifi_index)].wifi_rank.values\n",
    "    return list(indexs)\n",
    "\n",
    "def get_indexs2(df, sorted_wifi, topn=20):\n",
    "    w= sorted_wifi\n",
    "    s = gt.shape[0]\n",
    "#     print s\n",
    "#     print w\n",
    "    strong_sig_choose = topn \n",
    "    choose_strong_wifi_index = [_wi[0] for _wi in w[:strong_sig_choose]]\n",
    "    assert(choose_strong_wifi_index > 5)\n",
    "    choose_strong_wifi_index = np.array(choose_strong_wifi_index)\n",
    "\n",
    "    indexs = df[df.wifi_name.isin(choose_strong_wifi_index)].wifi_rank.values\n",
    "    return list(indexs)\n",
    "\n",
    "def get_range_sorted_wifi(sorted_wifi, rank0 = 1, rankmin = 10):\n",
    "    choose = []\n",
    "    if rank0 < 1:\n",
    "        rank0 = 1\n",
    "    min_sig = 0\n",
    "    s = len(sorted_wifi)\n",
    "    while(rank0 <= rankmin and rank0-1 < s):\n",
    "        choose.append(sorted_wifi[rank0 - 1][0])\n",
    "        min_sig = sorted_wifi[rank0 - 1][1]\n",
    "        rank0+=1\n",
    "    while( rank0-1 < s and sorted_wifi[rank0-1][1] == min_sig ):\n",
    "        choose.append(sorted_wifi[rank0-1][0])\n",
    "        rank0+=1\n",
    "    return choose\n",
    "\n",
    "\n",
    "def get_intersection_size(sorted_wifi, x, top = 3):\n",
    "    wi = []\n",
    "    for _x in x[1]:\n",
    "        wi.append(_x)\n",
    "    for _x in x[2]:\n",
    "        wi.append(_x)    \n",
    "    wi = sorted(wi,key=lambda x: -x[1])\n",
    "    wis = get_range_sorted_wifi(wi,rankmin=3)\n",
    "    return np.intersect1d(sorted_wifi,wis).shape[0]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvrf = CVEstimator(RandomForestClassifier(n_jobs=-1,n_estimators=388,class_weight=\"balanced\"), use_proba=True, cv=3)\n",
    "cvrf.fit(_train_x3,train_y)\n",
    "proba_train = cvrf.predict_proba(_train_x3)\n",
    "proba_valid = cvrf.predict_proba(_valid_x3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index([u'user_id', u'shop_id', u'time_stamp', u'longitude', u'latitude',\n",
       "       u'wifi_infos', u'category_id', u'shop_longitude', u'shop_latitude',\n",
       "       u'price', u'mall_id', u'dt', u'weekday', u'hour', u'is_weekend',\n",
       "       u'basic_wifi_info', u'wifi_size', u'use_wifi_size', u'no_use_wifi_size',\n",
       "       u'use_wifi_freq', u'no_use_wifi_freq', u'i_loc'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 一天内的交易次数\n",
    "train[\"dayofyear\"] = train.dt.dt.dayofyear\n",
    "oneday_count = train.groupby([\"user_id\",\"dayofyear\"])[\"shop_id\"].count().reset_index()\n",
    "oneday_count.rename(columns={\"shop_id\":\"oneday_count\"},inplace=True)\n",
    "train_one_day_count = pd.merge(train,oneday_count,on=[\"user_id\",\"dayofyear\"])[\"oneday_count\"].values\n",
    "train_one_day_count = train_one_day_count.reshape((-1,1))\n",
    "\n",
    "valid[\"dayofyear\"] = valid.dt.dt.dayofyear\n",
    "oneday_count = valid.groupby([\"user_id\",\"dayofyear\"])[\"shop_id\"].count().reset_index()\n",
    "oneday_count.rename(columns={\"shop_id\":\"oneday_count\"},inplace=True)\n",
    "valid_one_day_count = pd.merge(valid,oneday_count,on=[\"user_id\",\"dayofyear\"])[\"oneday_count\"].values\n",
    "valid_one_day_count = valid_one_day_count.reshape((-1,1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#是否连接wifi\n",
    "train_connect_wifi = (train.basic_wifi_info.map(lambda x: len(x[1])).values > 0).astype(int).reshape(-1,1)\n",
    "valid_connect_wifi = (valid.basic_wifi_info.map(lambda x: len(x[1])).values > 0).astype(int).reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index([u'user_id', u'shop_id', u'time_stamp', u'longitude', u'latitude',\n",
       "       u'wifi_infos', u'category_id', u'shop_longitude', u'shop_latitude',\n",
       "       u'price', u'mall_id', u'dt', u'weekday', u'hour', u'is_weekend',\n",
       "       u'basic_wifi_info', u'wifi_size', u'use_wifi_size', u'no_use_wifi_size',\n",
       "       u'use_wifi_freq', u'no_use_wifi_freq', u'i_loc', u'dayofyear'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 254,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index([u'user_id', u'shop_id', u'time_stamp', u'longitude', u'latitude',\n",
       "       u'wifi_infos', u'category_id', u'shop_longitude', u'shop_latitude',\n",
       "       u'price', u'mall_id', u'dt', u'weekday', u'hour', u'is_weekend',\n",
       "       u'basic_wifi_info', u'wifi_size', u'use_wifi_size', u'no_use_wifi_size',\n",
       "       u'use_wifi_freq', u'no_use_wifi_freq', u'i_loc', u'dayofyear'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_valid = pd.concat([train,valid],axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_valid_sort = train_valid.sort_values([\"user_id\", \"dt\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_valid_sortl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([14])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "le.transform([\"s_2939231\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([], shape=(12023, 0), dtype=float64),\n",
       " array([], shape=(4460, 0), dtype=float64))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def modify_wifi(tx,vx,train,valid,modify_size=0):\n",
    "    _tx = tx.copy()\n",
    "    _vx = vx.copy()\n",
    "    assert _tx.shape[0] == train.shape[0]\n",
    "    assert _vx.shape[0] == valid.shape[0]\n",
    "    assert _tx.shape[1] == _vx.shape[1]\n",
    "    train_day_of_year = train.dt.dt.dayofyear\n",
    "    valid_day_of_year = valid.dt.dt.dayofyear\n",
    "    if modify_size >= 0:\n",
    "        _tx = _tx[:,:modify_size]\n",
    "        _vx = _vx[:,:modify_size]\n",
    "    for _index in range(_tx.shape[1]):\n",
    "        l = []\n",
    "        for _wifi_sig in _tx[:,_index]:\n",
    "            if _wifi_sig != -115:\n",
    "                l.append(_wifi_sig)\n",
    "        for _wifi_sig in _vx[:,_index]:\n",
    "            if _wifi_sig != -115:\n",
    "                l.append(_wifi_sig)\n",
    "        modify_median = np.median(l)\n",
    "#         print modify_median\n",
    "        for _d in sorted(np.unique(train_day_of_year)):\n",
    "            d_index = (train_day_of_year==_d) & (_tx[:,_index] != -115)\n",
    "#             print _d\n",
    "#             print np.median(_tx[d_index ,_index])\n",
    "            _tx[d_index ,_index] = _tx[d_index ,_index] - (np.median(_tx[d_index ,_index]) - modify_median)\n",
    "#             print np.median(_tx[d_index ,_index])\n",
    "        for _d in sorted(np.unique(valid_day_of_year)):\n",
    "            d_index = (valid_day_of_year==_d) & (_vx[:,_index] != -115)\n",
    "#             print _d\n",
    "#             print np.median(_vx[d_index ,_index])\n",
    "            _vx[d_index ,_index] = _vx[d_index ,_index] - (np.median(_vx[d_index ,_index]) - modify_median)\n",
    "#             print np.median(_vx[d_index ,_index])\n",
    "        \n",
    "    return _tx, _vx\n",
    "modify_wifi(train_wifi_all_x[:,[0]],valid_wifi_all_x[:,[0]],train,valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exp_wifi(tx,vx):\n",
    "    _tx = tx.copy()\n",
    "    _vx = vx.copy()\n",
    "    __tx = (_tx==-115).astype(int)\n",
    "    __vx = (_vx==-115).astype(int)\n",
    "    _tx = np.concatenate([_tx,__tx],axis=1)\n",
    "    _vx = np.concatenate([_vx,__vx],axis=1)\n",
    "    return _tx,_vx\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_specific_wifi_size(x,wifi_names):\n",
    "    s = 0\n",
    "    for _x in x[1]:\n",
    "        if _x[0] in wifi_names:\n",
    "            s += 1\n",
    "    for _x in x[2]:\n",
    "        if _x[0] in wifi_names:\n",
    "            s += 1\n",
    "    return s\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "wifi index [2, 3, 4, 5, 6, 7, 9, 13, 21, 27, 29, 37, 48, 49, 60, 61, 71, 79, 94]\n",
      "feature importance [(0.28333929414116538, 29), (0.20219526044930761, 79), (0.15318113730106972, 21), (0.081340665450979094, 2), (0.080702856846784923, 94), (0.041000634900827358, 3), (0.040626982168821546, 6), (0.028151764988454453, 7), (0.024396173108365209, 9), (0.013437303031272973, 4)]\n",
      "origin 0.99865470852\n",
      "valid error shape, 6\n",
      "[1 1 1 1 1 1]\n",
      "valid error pos shape 6\n",
      "valid error index\n",
      "[1929 1930 1931 1932 2174 3328]\n",
      "valid correct index\n",
      "[  82  231  621 1125 1151 1410 1460 1464 1478 1588 1718 1887 1935 2129 2130\n",
      " 2270 2376 2617 2862 2956 3415 3522 3564 3609 3641 3690 3845 4293 4337]\n",
      "\n",
      "train error shape, 1\n",
      "[1]\n",
      "train error pos shape 1\n",
      "train error index\n",
      "[9679]\n",
      "s_1346456\n",
      "\n",
      "{0: 0.9986547085201793}\n"
     ]
    }
   ],
   "source": [
    "def get_model(cv = None):\n",
    "    if cv is not None and cv != 0:\n",
    "        rf = RandomForestClassifier(n_estimators=128,\n",
    "                                     n_jobs=-1,\n",
    "                                     class_weight=\"balanced\",\n",
    "                                     min_samples_leaf=1,\n",
    "    #                                  max_features=0.1,\n",
    "                                    )\n",
    "        rf2 = RandomForestClassifier(n_estimators=220,\n",
    "                                     n_jobs=-1,\n",
    "                                     class_weight=\"balanced\",\n",
    "                                     min_samples_leaf=1,\n",
    "    #                                  max_features=0.1,\n",
    "                                    )\n",
    "        s = StackingCVClassifier([rf], \n",
    "                                 rf2,\n",
    "                                 use_probas=True,\n",
    "                                 verbose=1,\n",
    "                                 use_features_in_secondary = True,\n",
    "                                 cv = 3)\n",
    "        return s\n",
    "    else:\n",
    "        rf = RandomForestClassifier(n_estimators=388,\n",
    "                                     n_jobs=-1,\n",
    "                                     class_weight=\"balanced\",\n",
    "                                     min_samples_leaf=1,\n",
    "#                                      max_features=0.1,\n",
    "                                    )\n",
    "        return rf\n",
    "    \n",
    "def diff_matrix(matrix):\n",
    "    l = []\n",
    "    wifi_num = matrix.shape[1]\n",
    "    for i in range(wifi_num):\n",
    "        for j in range(i+1,wifi_num):\n",
    "            m1 = matrix[:,i]\n",
    "            m2 = matrix[:,j]\n",
    "            m = m1 - m2\n",
    "#             m[(m1==-115) | (m2==-115)] = -115\n",
    "            \n",
    "            l.append(m.reshape(-1,1))\n",
    "    return np.hstack(l)\n",
    "\n",
    "r = {}\n",
    "probas = []\n",
    "for i in range(_train_b_y.shape[1]):\n",
    "    i=0\n",
    "    print i\n",
    "    gt = train.iloc[train_y==lb.classes_[i]]\n",
    "    ngt = train.iloc[train_y!=lb.classes_[i]]\n",
    "    if len(gt) != 0:\n",
    "        \n",
    "        sorted_wifi = get_sorted_wifi([gt])\n",
    "        _indexs = get_indexs(df,sorted_wifi,0.1)\n",
    "       \n",
    "        cv = 0\n",
    "#         n_sorted_wifi = get_sorted_wifi([ngt])\n",
    "#         _nindexs = get_indexs2(df,n_sorted_wifi,50)\n",
    "#         _indexs = list(set(_indexs).union(set(_nindexs)))\n",
    "        prf = get_model(cv = cv)\n",
    "        otxs= []\n",
    "        ovxs = []\n",
    "#         _indexs = _indexs[:1]\n",
    "        \n",
    "#         _indexs = [2,3,6,7,71,9]\n",
    "#         _indexs=[]\n",
    "        print \"wifi index\",_indexs\n",
    "        modify_size =  (np.asarray(_indexs) < 0).sum()\n",
    "        _tx = train_wifi_all_x[:,_indexs]\n",
    "        _vx = valid_wifi_all_x[:,_indexs]\n",
    "        \n",
    "#         _tx,_vx = exp_wifi(_tx,_vx)\n",
    "        \n",
    "#         __tx,__vx = modify_wifi(_tx,_vx,train,valid,modify_size)\n",
    "#         _tx = np.concatenate([_tx,__tx],axis=1)\n",
    "#         _vx = np.concatenate([_vx,__vx],axis=1)\n",
    "        \n",
    "        \n",
    "#         from sklearn.preprocessing import PolynomialFeatures\n",
    "#         _tx = PolynomialFeatures(2).fit_transform(_tx)\n",
    "#         _vx = PolynomialFeatures(2).fit_transform(_vx)\n",
    "#         train_dis = haversine(train_lonlats[:,0],\n",
    "#                               train_lonlats[:,1],\n",
    "#                               shop_info[shop_info.shop_id == le.classes_[i]][\"shop_longitude\"].values,\n",
    "#                               shop_info[shop_info.shop_id == le.classes_[i]][\"shop_latitude\"].values).reshape(-1,1)\n",
    "    \n",
    "#         valid_dis = haversine(valid_lonlats[:,0],\n",
    "#                               valid_lonlats[:,1],\n",
    "#                               shop_info[shop_info.shop_id == le.classes_[i]][\"shop_longitude\"].values,\n",
    "#                               shop_info[shop_info.shop_id == le.classes_[i]][\"shop_latitude\"].values).reshape(-1,1)\n",
    "        \n",
    "#         train_bear = bearing(train_lonlats[:,0],\n",
    "#                               train_lonlats[:,1],\n",
    "#                               shop_info[shop_info.shop_id == le.classes_[i]][\"shop_longitude\"].values,\n",
    "#                               shop_info[shop_info.shop_id == le.classes_[i]][\"shop_latitude\"].values).reshape(-1,1)\n",
    "    \n",
    "#         valid_bear = bearing(valid_lonlats[:,0],\n",
    "#                               valid_lonlats[:,1],\n",
    "#                               shop_info[shop_info.shop_id == le.classes_[i]][\"shop_longitude\"].values,\n",
    "#                               shop_info[shop_info.shop_id == le.classes_[i]][\"shop_latitude\"].values).reshape(-1,1)\n",
    "        \n",
    "    \n",
    "#         _tx = np.concatenate([_tx,train_lonlats[:,[0,1]],],axis=1)\n",
    "#         _vx = np.concatenate([_vx,valid_lonlats[:,[0,1]],],axis=1)\n",
    "#         _indexs.append(\"lon\")\n",
    "#         _indexs.append(\"lat\")\n",
    "#         _indexs.append(\"dis\")\n",
    "#         _indexs.append(\"bearing\")\n",
    "#         _indexs.append(\"w\")\n",
    "#         _indexs.append(\"h\")\n",
    "        \n",
    "        if cv is not None and cv != 0:\n",
    "            _tx,_ty = expansion(_tx,_train_b_y[:,i],cv)\n",
    "        else:\n",
    "            _ty = _train_b_y[:,i]\n",
    "            \n",
    "        prf.fit(_tx,_ty)\n",
    "        \n",
    "#         print \"train index\", train_index[(1 == _train_b_y[:,i])]\n",
    "        \n",
    "        if hasattr(prf,\"feature_importances_\"):\n",
    "            fi = zip(prf.feature_importances_, _indexs)\n",
    "            fi = sorted(fi,key=lambda x:-x[0])\n",
    "            print \"feature importance\",fi[:10]\n",
    "            \n",
    "        \n",
    "        p = prf.predict(_vx)\n",
    "        \n",
    "        proba = prf.predict_proba(_vx)\n",
    "        probas.append(proba[:,1])\n",
    "        _acc = acc(p,_valid_b_y[:,i])\n",
    "        print \"origin\", _acc\n",
    "        r[i] = _acc\n",
    "        print \"valid error shape,\", (p != _valid_b_y[:,i]).sum()\n",
    "        print _valid_b_y[:,i][(p != _valid_b_y[:,i])]\n",
    "        print \"valid error pos shape\", (_valid_b_y[:,i][(p != _valid_b_y[:,i])]==1).sum()\n",
    "        print \"valid error index\"\n",
    "        print valid_index[(p != _valid_b_y[:,i])]\n",
    "        print \"valid correct index\"\n",
    "        print valid_index[(p == _valid_b_y[:,i]) & (_valid_b_y[:,i]==1)]\n",
    "        print \n",
    "        \n",
    "        p2 = prf.predict(_tx)\n",
    "        print \"train error shape,\", (p2 != _train_b_y[:,i]).sum()\n",
    "        print _train_b_y[:,i][(p2 != _train_b_y[:,i])]\n",
    "        print \"train error pos shape\", (_train_b_y[:,i][(p2 != _train_b_y[:,i])]==1).sum()\n",
    "        print \"train error index\"\n",
    "        print train_index[(p2 != _train_b_y[:,i])]\n",
    "        print le.classes_[i]\n",
    "        print\n",
    "    else:\n",
    "        probas.append(np.zeros((valid_y.shape[0],)))\n",
    "        r[i] = 0\n",
    "    break\n",
    "print r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9412556053811659"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#  多分类\n",
    "rf_all = RandomForestClassifier(n_estimators=500,n_jobs=-1,class_weight=\"balanced\")\n",
    "train_all_x = np.concatenate([train_wifi_all_x[:, indexs], train_lonlats, train_wh],axis=1)\n",
    "valid_all_x = np.concatenate([valid_wifi_all_x[:, indexs], valid_lonlats, valid_wh],axis=1)\n",
    "rf_all.fit(train_all_x,train_y)\n",
    "rf_all_pba = rf_all.predict_proba(valid_all_x)\n",
    "acc(rf_all.predict(valid_all_x),valid_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9885650224215247"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_top_k(pba,k = 3, min_proba = 0):\n",
    "    _pba = pba.copy()\n",
    "    def top_k(x,k):\n",
    "        rs = []\n",
    "        for _ in range(k):\n",
    "            ind = np.argmax(x)\n",
    "            if x[ind] < min_proba:\n",
    "                continue\n",
    "            rs.append(ind)\n",
    "            x[ind] = 0\n",
    "        return rs\n",
    "    r = map(lambda x: top_k(x,k), _pba)\n",
    "    return r\n",
    "\n",
    "def acc_top_k(candidate, y):\n",
    "    all_size = len(candidate)\n",
    "    cor = 0\n",
    "    for _can,_true in zip(candidate,y):\n",
    "        if _true in _can:\n",
    "            cor += 1\n",
    "    return float(cor) / all_size\n",
    "\n",
    "# 候选10 个, 最小概率0.02\n",
    "candidate = get_top_k(rf_all_pba, 20, 0.01)\n",
    "candidate = [rf_all.classes_.take(_can) for _can in candidate]\n",
    "acc_top_k(candidate, valid_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "wifi index [2, 3, 4, 5, 6, 7, 9, 13, 21, 27, 29, 37, 48, 49, 60, 61, 71, 79, 94]\n",
      "feature_importances_ [(0.24799773579960047, 29), (0.20091242405510965, 79), (0.16870032638032059, 21), (0.097880084305152923, 2), (0.072473848487338718, 94), (0.062482802218081758, 3), (0.036186256848905973, 6), (0.024396293438757266, 7), (0.021338357919517884, 9), (0.016954273037962066, 4)]\n",
      "origin 0.999103139013\n",
      "valid error shape, 4\n",
      "[1 1 1 1]\n",
      "valid error pos shape 4\n",
      "valid error index\n",
      "[1931 1932 2129 3328]\n",
      "valid correct index\n",
      "[  82  231  621 1125 1151 1410 1460 1464 1478 1588 1718 1887 1929 1930 1935\n",
      " 2130 2174 2270 2376 2617 2862 2956 3415 3522 3564 3609 3641 3690 3845 4293\n",
      " 4337]\n",
      "\n",
      "train error shape, 3\n",
      "[0 0 0]\n",
      "train error pos shape 0\n",
      "train error index\n",
      "[  74 2089 8203]\n",
      "s_1346456\n",
      "\n",
      "1\n",
      "wifi index [24, 32, 41, 50, 53, 64, 66, 88, 92, 98, 101, 223, 236, 280, 292, 300, 334, 337, 387, 484, 487, 488, 523]\n",
      "feature_importances_ [(0.2599223446938102, 88), (0.18050077509335416, 92), (0.12891290014393406, 334), (0.093719008690220512, 337), (0.086219009155676055, 41), (0.056294933015677527, 101), (0.054674159690704272, 53), (0.027962062728793244, 236), (0.023951645500629188, 50), (0.015029803260627439, 280)]\n",
      "origin 0.999551569507\n",
      "valid error shape, 2\n",
      "[1 1]\n",
      "valid error pos shape 2\n",
      "valid error index\n",
      "[1193 1551]\n",
      "valid correct index\n",
      "[ 182  617  952 1142 1324 1417 1428 1754 1882 2235 2397 2444 2450 2476 2597\n",
      " 2686 2711 2949 2960 3116 3274 3291 3345 3360 3434 3642 3718 3800 3877 4229\n",
      " 4356]\n",
      "\n",
      "train error shape, 0\n",
      "[]\n",
      "train error pos shape 0\n",
      "train error index\n",
      "[]\n",
      "s_1392063\n",
      "\n",
      "2\n",
      "wifi index [23, 26, 34, 65, 123, 133, 169, 171, 230, 255, 302, 373, 405, 408, 540, 774, 776, 783, 800, 801, 810, 837, 924, 932, 974, 1304, 1584, 1969]\n",
      "feature_importances_ [(0.11418928570467397, 801), (0.11156236542584687, 800), (0.10038609241989226, 169), (0.096894209539048648, 974), (0.086377307846917564, 776), (0.075113437622150919, 810), (0.065902380576755015, 774), (0.060982452807113646, 171), (0.057863313056262043, 1304), (0.02982335466316801, 123)]\n",
      "origin 1.0\n",
      "valid error shape, 0\n",
      "[]\n",
      "valid error pos shape 0\n",
      "valid error index\n",
      "[]\n",
      "valid correct index\n",
      "[]\n",
      "\n",
      "train error shape, 0\n",
      "[]\n",
      "train error pos shape 0\n",
      "train error index\n",
      "[]\n",
      "s_1452940\n",
      "\n",
      "3\n",
      "wifi index [56, 267, 369, 438, 521, 531, 611, 619, 681, 687, 705, 718, 848, 1004, 1744, 4479]\n",
      "feature_importances_ [(0.2327813588060344, 619), (0.21833675579856046, 611), (0.16876990591591109, 267), (0.14419665436597323, 438), (0.09450923123553677, 687), (0.074037083506647242, 705), (0.043816937570988534, 369), (0.0087950545865997549, 681), (0.0054072921653498963, 848), (0.0041077458563867682, 4479)]\n",
      "origin 1.0\n",
      "valid error shape, 0\n",
      "[]\n",
      "valid error pos shape 0\n",
      "valid error index\n",
      "[]\n",
      "valid correct index\n",
      "[4123]\n",
      "\n",
      "train error shape, 0\n",
      "[]\n",
      "train error pos shape 0\n",
      "train error index\n",
      "[]\n",
      "s_1671756\n",
      "\n",
      "4\n",
      "wifi index [258, 376, 390, 424, 427, 921]\n",
      "feature_importances_ [(0.232340704454013, 390), (0.17555626103965491, 921), (0.1206454089622319, 258), (0.10799573291142256, 424), (0.031020497188095061, 427), (0.010276446990974289, 376)]\n",
      "origin 1.0\n",
      "valid error shape, 0\n",
      "[]\n",
      "valid error pos shape 0\n",
      "valid error index\n",
      "[]\n",
      "valid correct index\n",
      "[]\n",
      "\n",
      "train error shape, 0\n",
      "[]\n",
      "train error pos shape 0\n",
      "train error index\n",
      "[]\n",
      "s_1882105\n",
      "\n",
      "5\n",
      "wifi index [20, 31, 34, 55, 57, 58, 62, 63, 117, 119, 139, 152, 244, 260, 282, 412, 423]\n",
      "feature_importances_ [(0.19888184297349568, 62), (0.19870528648979147, 63), (0.12678537635046264, 34), (0.093112019567136883, 117), (0.083370740763061071, 152), (0.075458400586663046, 55), (0.058385884978916197, 119), (0.054562398601418975, 31), (0.046757094255702625, 20), (0.02498028507822218, 58)]\n",
      "origin 0.998878923767\n",
      "valid error shape, 5\n",
      "[0 1 0 1 0]\n",
      "valid error pos shape 2\n",
      "valid error index\n",
      "[1810 2323 2522 4025 4171]\n",
      "valid correct index\n",
      "[   4   42  112  273  397  439  440  567  648  724  780  971 1137 1148 1160\n",
      " 1161 1338 1339 1342 1343 1351 1408 1409 1594 1851 1919 2030 2088 2203 2206\n",
      " 2207 2208 2282 2285 2331 2418 2559 2638 2774 2799 2904 2938 2939 3077 3118\n",
      " 3175 3176 3295 3296 3297 3339 3451 3472 3473 3474 3476 3518 3561 3693 3694\n",
      " 3710 3727 3813 3842 3850 3881 4038 4041 4220 4360 4407 4431]\n",
      "\n",
      "train error shape, 0\n",
      "[]\n",
      "train error pos shape 0\n",
      "train error index\n",
      "[]\n",
      "s_1944816\n",
      "\n",
      "6\n",
      "wifi index [58, 96, 106, 128, 130, 150, 184, 248, 258, 310, 322, 355, 384, 410, 426]\n",
      "feature_importances_ [(0.30837134311864867, 150), (0.24480349414461405, 184), (0.1130869976195519, 58), (0.10061594922102839, 128), (0.077352487022428418, 106), (0.072209516159905784, 96), (0.032692129103824984, 130), (0.024180687288343966, 248), (0.0063422599440040878, 258), (0.0063375780221984141, 322)]\n",
      "origin 0.9966367713\n",
      "valid error shape, 15\n",
      "[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "valid error pos shape 15\n",
      "valid error index\n",
      "[ 750  939 1287 1303 1444 1667 2171 2316 2548 2792 3276 3934 4406 4427 4428]\n",
      "valid correct index\n",
      "[ 754  790 2315 2345 3705 3940 3998 4231 4333 4425 4426 4429]\n",
      "\n",
      "train error shape, 0\n",
      "[]\n",
      "train error pos shape 0\n",
      "train error index\n",
      "[]\n",
      "s_1997293\n",
      "\n",
      "7\n",
      "wifi index [58, 83, 160, 192, 197, 206, 207, 211, 219, 224, 237, 331, 385, 526, 570, 628]\n",
      "feature_importances_ [(0.31382279580319011, 83), (0.23322990489592971, 219), (0.16111882094179308, 211), (0.11906574862373778, 237), (0.045995517824774339, 192), (0.038508938739108274, 206), (0.032249807723835046, 385), (0.014841551049912211, 224), (0.013535614585295445, 331), (0.010103871511661274, 160)]\n",
      "origin 0.998878923767\n",
      "valid error shape, 5\n",
      "[0 1 1 1 1]\n",
      "valid error pos shape 4\n",
      "valid error index\n",
      "[ 408 1143 1427 2503 3862]\n",
      "valid correct index\n",
      "[  73  256  263  409  410  509  551 1094 1104 1238 1239 1407 1623 1717 1952\n",
      " 2079 2086 2176 2295 2336 2438 2521 2543 2714 2788 2793 2816 2825 3009 3012\n",
      " 3067 3092 3095 3096 3150 3196 3238 3406 3407 3417 3504 3514 3664 3703 3765\n",
      " 3883 4064 4081 4156 4169 4368 4421]\n",
      "\n",
      "train error shape, 0\n",
      "[]\n",
      "train error pos shape 0\n",
      "train error index\n",
      "[]\n",
      "s_2257337\n",
      "\n",
      "8\n",
      "wifi index [2, 3, 4, 6, 7, 13, 21, 27, 29, 37, 49, 60, 61, 71, 79, 91, 94]\n",
      "feature_importances_ [(0.27543063458167838, 21), (0.24268992109503745, 29), (0.10890594959347985, 6), (0.093690155608182418, 79), (0.075336515228245901, 37), (0.059699330318265909, 2), (0.059303460617412138, 94), (0.026321076859681394, 3), (0.012932888620611125, 4), (0.011979118843978112, 61)]\n",
      "origin 0.998878923767\n",
      "valid error shape, 5\n",
      "[1 1 1 1 1]\n",
      "valid error pos shape 5\n",
      "valid error index\n",
      "[1572 1945 2050 4151 4152]\n",
      "valid correct index\n",
      "[ 167  207  401  594  685  794  805  959  966 1063 1064 1582 1626 1654 1723\n",
      " 1883 1944 2065 2177 2219 2378 2688 2689 2786 3499 3505 3692 3834 3891 4024\n",
      " 4145 4163 4190]\n",
      "\n",
      "train error shape, 1\n",
      "[0]\n",
      "train error pos shape 0\n",
      "train error index\n",
      "[6363]\n",
      "s_2423948\n",
      "\n",
      "9\n",
      "wifi index [269, 424, 427, 3756]\n",
      "feature_importances_ [(0.31223055135181327, 3756), (0.21125473714785412, 424), (0.10271951098199505, 427), (8.3860312151980682e-05, 269)]\n",
      "origin 1.0\n",
      "valid error shape, 0\n",
      "[]\n",
      "valid error pos shape 0\n",
      "valid error index\n",
      "[]\n",
      "valid correct index\n",
      "[]\n",
      "\n",
      "train error shape, 0\n",
      "[]\n",
      "train error pos shape 0\n",
      "train error index\n",
      "[]\n",
      "s_2631847\n",
      "\n",
      "10\n",
      "wifi index [10, 112, 168, 175]\n",
      "feature_importances_ [(0.31069760296595395, 175), (0.20616758626447387, 168), (0.10928903385088111, 112), (0.00013443671250553839, 10)]\n",
      "origin 1.0\n",
      "valid error shape, 0\n",
      "[]\n",
      "valid error pos shape 0\n",
      "valid error index\n",
      "[]\n",
      "valid correct index\n",
      "[]\n",
      "\n",
      "train error shape, 0\n",
      "[]\n",
      "train error pos shape 0\n",
      "train error index\n",
      "[]\n",
      "s_2651838\n",
      "\n",
      "11\n",
      "wifi index [20, 31, 34, 55, 57, 62, 63, 90, 104, 113, 137, 311]\n",
      "feature_importances_ [(0.29580074722773458, 113), (0.19293787462183393, 31), (0.16165910951187595, 90), (0.11377819031635439, 104), (0.10978817209110014, 311), (0.071968472007536574, 137), (0.023669089039437867, 62), (0.017813914699327745, 34), (0.0062625225564612213, 55), (0.0033164443759504452, 63)]\n",
      "origin 0.999775784753\n",
      "valid error shape, 1\n",
      "[1]\n",
      "valid error pos shape 1\n",
      "valid error index\n",
      "[2936]\n",
      "valid correct index\n",
      "[  54   55   64  301  417  450  532  769  882  888  900  913  944  997 1059\n",
      " 1113 1207 1246 1265 1335 1348 1537 1556 1679 1734 1865 1975 2082 2372 2375\n",
      " 2491 2502 2517 2525 2526 2610 2751 2858 2931 2937 2974 3020 3388 3457 3539\n",
      " 3640 3706 3741 3742 3829 3882 3894 3962 4022 4047 4095 4168 4171 4176 4196\n",
      " 4222 4257 4259]\n",
      "\n",
      "train error shape, 0\n",
      "[]\n",
      "train error pos shape 0\n",
      "train error index\n",
      "[]\n",
      "s_2691361\n",
      "\n",
      "12\n",
      "wifi index [58, 128, 130, 150, 248, 258, 269, 322, 355, 368, 376, 403, 410, 532, 767]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature_importances_ [(0.28133323517193271, 248), (0.18638978075067161, 410), (0.17909861708515401, 269), (0.10963205652132253, 130), (0.1040602336826016, 403), (0.049014213207248802, 368), (0.038495871776080312, 150), (0.022075472326999013, 128), (0.010900487874677499, 258), (0.0090525039733017204, 58)]\n",
      "origin 1.0\n",
      "valid error shape, 0\n",
      "[]\n",
      "valid error pos shape 0\n",
      "valid error index\n",
      "[]\n",
      "valid correct index\n",
      "[ 744  800 1181 1724 1806 2099 2398 2870 3151]\n",
      "\n",
      "train error shape, 0\n",
      "[]\n",
      "train error pos shape 0\n",
      "train error index\n",
      "[]\n",
      "s_2741830\n",
      "\n",
      "13\n",
      "wifi index [58, 96, 106, 128, 134, 150, 184, 212, 248, 258, 267, 269, 322, 369, 376, 390, 407, 424, 427, 521, 531, 616, 718, 1392, 1630, 4588]\n",
      "feature_importances_ [(0.25333911823759475, 322), (0.15864775967578959, 150), (0.13580458475977197, 376), (0.12637713082192345, 258), (0.11537930660182405, 269), (0.062742420185068079, 58), (0.032604310098987614, 427), (0.015070682267153423, 184), (0.012129578210652718, 1392), (0.011181836435782877, 390)]\n",
      "origin 1.0\n",
      "valid error shape, 0\n",
      "[]\n",
      "valid error pos shape 0\n",
      "valid error index\n",
      "[]\n",
      "valid correct index\n",
      "[]\n",
      "\n",
      "train error shape, 0\n",
      "[]\n",
      "train error pos shape 0\n",
      "train error index\n",
      "[]\n",
      "s_2795244\n",
      "\n",
      "14\n",
      "wifi index [0, 1, 12, 17, 19, 28, 36, 56]\n",
      "feature_importances_ [(0.45401265248855743, 0), (0.30540206565836214, 1), (0.13353784095461471, 17), (0.044169844195970864, 12), (0.029095758367377236, 19), (0.01373958158195043, 36), (0.013688821887604112, 28), (0.0063534348655632251, 56)]\n",
      "origin 0.995291479821\n",
      "valid error shape, 21\n",
      "[0 0 0 0 1 1 0 0 0 1 1 0 1 1 1 0 1 1 0 1 1]\n",
      "valid error pos shape 11\n",
      "valid error index\n",
      "[ 382  429  626  627  639  640 2248 2249 2251 2565 2566 3080 3097 3172 3285\n",
      " 3342 3461 3462 3934 3959 4316]\n",
      "valid correct index\n",
      "[  10   11   12   13   29   39   47   50   59  120  121  125  137  168  169\n",
      "  170  179  180  181  185  186  187  219  227  233  245  246  253  254  255\n",
      "  271  282  284  286  287  313  317  321  323  351  352  353  373  395  404\n",
      "  405  406  407  419  421  427  430  441  442  451  463  477  478  479  480\n",
      "  482  488  491  494  495  496  508  516  517  518  519  520  521  522  523\n",
      "  529  531  535  536  537  538  539  547  552  565  568  569  571  573  608\n",
      "  609  611  614  615  634  637  638  653  655  656  660  661  662  674  684\n",
      "  699  702  716  734  736  737  738  739  742  761  763  766  772  774  787\n",
      "  806  817  828  869  881  885  892  916  918  926  928  929  930  937  942\n",
      "  943  960  961  962  970  973  974  999 1000 1001 1015 1020 1040 1041 1048\n",
      " 1056 1065 1066 1067 1074 1075 1078 1079 1081 1088 1089 1105 1106 1107 1108\n",
      " 1119 1128 1129 1130 1136 1140 1144 1146 1157 1159 1164 1165 1166 1167 1168\n",
      " 1172 1178 1184 1185 1186 1187 1188 1189 1198 1203 1206 1209 1243 1244 1248\n",
      " 1250 1262 1263 1278 1285 1308 1309 1310 1311 1312 1317 1328 1329 1336 1358\n",
      " 1359 1360 1362 1379 1380 1381 1382 1384 1391 1419 1420 1424 1425 1429 1430\n",
      " 1431 1432 1433 1434 1435 1436 1437 1438 1439 1451 1477 1485 1494 1498 1499\n",
      " 1500 1501 1502 1503 1504 1515 1518 1522 1529 1530 1531 1532 1545 1569 1601\n",
      " 1602 1618 1619 1620 1621 1622 1637 1638 1639 1640 1641 1642 1643 1644 1645\n",
      " 1661 1662 1663 1664 1678 1681 1686 1694 1697 1698 1699 1700 1701 1702 1708\n",
      " 1714 1721 1735 1744 1747 1749 1750 1751 1763 1774 1775 1776 1777 1778 1779\n",
      " 1790 1791 1792 1804 1805 1811 1812 1818 1824 1825 1829 1832 1836 1847 1856\n",
      " 1857 1888 1891 1892 1893 1894 1895 1911 1924 1925 1928 1947 1948 1949 1950\n",
      " 1951 1968 1973 1974 1981 2018 2020 2021 2022 2027 2041 2043 2045 2046 2051\n",
      " 2055 2059 2060 2087 2098 2112 2135 2136 2137 2150 2151 2152 2153 2154 2155\n",
      " 2156 2157 2160 2181 2190 2209 2213 2216 2217 2232 2233 2236 2237 2243 2244\n",
      " 2245 2247 2280 2281 2324 2325 2326 2327 2329 2335 2346 2347 2349 2350 2356\n",
      " 2361 2363 2364 2365 2366 2367 2413 2414 2417 2420 2421 2422 2423 2424 2425\n",
      " 2426 2427 2428 2429 2430 2452 2457 2496 2497 2498 2499 2516 2527 2563 2564\n",
      " 2567 2568 2569 2570 2574 2592 2593 2596 2622 2635 2660 2694 2716 2720 2724\n",
      " 2725 2755 2767 2776 2777 2778 2779 2780 2794 2795 2796 2798 2803 2808 2813\n",
      " 2817 2818 2842 2847 2854 2855 2856 2857 2864 2865 2866 2867 2888 2893 2894\n",
      " 2895 2896 2899 2902 2905 2906 2907 2913 2940 2941 2942 2966 2967 2968 2986\n",
      " 2987 2988 3007 3013 3014 3015 3016 3021 3022 3041 3051 3060 3068 3069 3070\n",
      " 3071 3087 3088 3089 3090 3091 3094 3098 3099 3100 3102 3103 3171 3173 3174\n",
      " 3178 3180 3181 3182 3188 3189 3190 3191 3198 3203 3205 3210 3211 3212 3213\n",
      " 3214 3215 3216 3217 3218 3256 3257 3279 3298 3305 3315 3336 3340 3347 3351\n",
      " 3352 3353 3359 3363 3364 3365 3366 3371 3372 3386 3387 3414 3416 3441 3445\n",
      " 3447 3448 3452 3458 3467 3469 3471 3502 3510 3511 3532 3533 3551 3567 3568\n",
      " 3569 3588 3589 3590 3591 3597 3599 3614 3615 3616 3617 3618 3619 3646 3680\n",
      " 3685 3686 3701 3707 3721 3723 3748 3749 3780 3781 3795 3808 3814 3824 3825\n",
      " 3855 3866 3867 3868 3870 3871 3884 3885 3886 3896 3904 3913 3924 3930 3957\n",
      " 3958 3961 3975 4000 4001 4002 4003 4004 4005 4006 4007 4011 4042 4043 4048\n",
      " 4049 4050 4065 4073 4074 4086 4087 4119 4148 4150 4159 4179 4181 4182 4187\n",
      " 4188 4208 4209 4210 4216 4217 4240 4241 4242 4243 4250 4255 4264 4270 4278\n",
      " 4294 4295 4300 4315 4328 4334 4335 4336 4340 4341 4344 4346 4352 4353 4375\n",
      " 4405 4430 4441 4449 4450 4451 4455 4459]\n",
      "\n",
      "train error shape, 2\n",
      "[1 0]\n",
      "train error pos shape 1\n",
      "train error index\n",
      "[ 4072 11638]\n",
      "s_2939231\n",
      "\n",
      "15\n",
      "wifi index [258, 267, 269, 322, 376, 390, 521, 531, 818, 916, 921, 2621, 3650]\n",
      "feature_importances_ [(0.20000679972740193, 521), (0.19988638019058966, 376), (0.13290506452095949, 322), (0.093462547386853195, 390), (0.083994391668969839, 2621), (0.074574830795560598, 818), (0.07111355531329655, 916), (0.034461886704298179, 3650), (0.02747432290957031, 258), (0.018103858851433518, 269)]\n",
      "origin 1.0\n",
      "valid error shape, 0\n",
      "[]\n",
      "valid error pos shape 0\n",
      "valid error index\n",
      "[]\n",
      "valid correct index\n",
      "[3889 4186]\n",
      "\n",
      "train error shape, 0\n",
      "[]\n",
      "train error pos shape 0\n",
      "train error index\n",
      "[]\n",
      "s_2956845\n",
      "\n",
      "16\n",
      "wifi index [852]\n",
      "feature_importances_ [(0.634020618556701, 852)]\n",
      "origin 1.0\n",
      "valid error shape, 0\n",
      "[]\n",
      "valid error pos shape 0\n",
      "valid error index\n",
      "[]\n",
      "valid correct index\n",
      "[]\n",
      "\n",
      "train error shape, 0\n",
      "[]\n",
      "train error pos shape 0\n",
      "train error index\n",
      "[]\n",
      "s_3031981\n",
      "\n",
      "17\n",
      "wifi index [41, 53, 88, 92, 98, 101, 236, 292, 300, 352, 383, 387, 404, 414, 447, 467, 500, 686]\n",
      "feature_importances_ [(0.21149161054087975, 292), (0.14990787642776879, 414), (0.13777958886590394, 300), (0.12865895128505098, 383), (0.080807215137647936, 352), (0.071470406241787618, 101), (0.049129988934750908, 387), (0.030448668337358511, 467), (0.028113026157627494, 404), (0.026976681087368309, 88)]\n",
      "origin 0.99932735426\n",
      "valid error shape, 3\n",
      "[0 1 1]\n",
      "valid error pos shape 2\n",
      "valid error index\n",
      "[2921 3683 3974]\n",
      "valid correct index\n",
      "[ 141  362  566  593  743  845  857  870 1636 1757 2113 2434 2607 2763 3299\n",
      " 3307 3637 3777 3890 4091 4383 4445]\n",
      "\n",
      "train error shape, 1\n",
      "[0]\n",
      "train error pos shape 0\n",
      "train error index\n",
      "[245]\n",
      "s_3036726\n",
      "\n",
      "18\n",
      "wifi index [3183, 3194, 3274, 3339, 3364, 3563, 3648, 3708, 3818, 3985, 3986, 4023, 4057, 4350, 4441, 4521, 4617]\n",
      "feature_importances_ [(0.074286063713996459, 3194), (0.070270997509918234, 3818), (0.069587664601036991, 3339), (0.064074997334064743, 3648), (0.058562302929056093, 4057), (0.056701364494026095, 3708), (0.054324460615453748, 4350), (0.047980538075963877, 3183), (0.045905264251361953, 4617), (0.04567569546470078, 3364)]\n",
      "origin 1.0\n",
      "valid error shape, 0\n",
      "[]\n",
      "valid error pos shape 0\n",
      "valid error index\n",
      "[]\n",
      "valid correct index\n",
      "[]\n",
      "\n",
      "train error shape, 0\n",
      "[]\n",
      "train error pos shape 0\n",
      "train error index\n",
      "[]\n",
      "s_3081549\n",
      "\n",
      "19\n",
      "wifi index [20, 104, 240, 287, 478]\n",
      "feature_importances_ [(0.22424037950225759, 104), (0.22166572062505316, 478), (0.097919889229466456, 287), (0.082459615253500818, 240), (0.025776251059825107, 20)]\n",
      "origin 1.0\n",
      "valid error shape, 0\n",
      "[]\n",
      "valid error pos shape 0\n",
      "valid error index\n",
      "[]\n",
      "valid correct index\n",
      "[]\n",
      "\n",
      "train error shape, 0\n",
      "[]\n",
      "train error pos shape 0\n",
      "train error index\n",
      "[]\n",
      "s_3493162\n",
      "\n",
      "20\n",
      "wifi index [0, 1, 12, 28, 56, 148, 174, 267, 309, 333, 369, 438, 468, 499, 506, 511]\n",
      "feature_importances_ [(0.39532911816394145, 56), (0.24774051837122471, 28), (0.11229019453210227, 1), (0.089985660873535087, 309), (0.070625469681576264, 333), (0.024771540949571656, 0), (0.023748388052602532, 174), (0.015300306429627849, 369), (0.0090236387542442596, 267), (0.0031668018324129197, 148)]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "origin 0.999103139013\n",
      "valid error shape, 4\n",
      "[1 0 1 1]\n",
      "valid error pos shape 3\n",
      "valid error index\n",
      "[1043 1892 3137 3342]\n",
      "valid correct index\n",
      "[  24  275  429  574  751  836  837  838  981 1223 1318 1319 1320 1321 1322\n",
      " 1323 1557 1879 1913 2075 2076 2091 2602 2603 2652 2861 2918 2979 2984 3127\n",
      " 3128 3165 3166 3392 3397 3484 3491 3713 3746 3747 3776 3818 3919 4046 4342\n",
      " 4380 4384 4411]\n",
      "\n",
      "train error shape, 0\n",
      "[]\n",
      "train error pos shape 0\n",
      "train error index\n",
      "[]\n",
      "s_3534278\n",
      "\n",
      "21\n",
      "wifi index [267, 438, 531, 611, 619, 681, 687, 705, 746, 852, 915]\n",
      "feature_importances_ [(0.23970947978918924, 681), (0.19333656020990361, 915), (0.16235969548704351, 705), (0.12632168452637269, 746), (0.12219670024946876, 852), (0.065347769810405867, 687), (0.035298395304877883, 531), (0.033442123043834725, 267), (0.010301934499787779, 619), (0.0025954784720278426, 611)]\n",
      "origin 1.0\n",
      "valid error shape, 0\n",
      "[]\n",
      "valid error pos shape 0\n",
      "valid error index\n",
      "[]\n",
      "valid correct index\n",
      "[]\n",
      "\n",
      "train error shape, 0\n",
      "[]\n",
      "train error pos shape 0\n",
      "train error index\n",
      "[]\n",
      "s_3542509\n",
      "\n",
      "22\n",
      "wifi index [15, 24, 32, 45, 47, 50, 52, 66, 70, 114, 146, 151, 153, 163, 186, 187, 234, 278]\n",
      "feature_importances_ [(0.29214982069512496, 15), (0.18104433224314243, 70), (0.17426965728464694, 163), (0.088455165164216254, 151), (0.085552482279337538, 45), (0.061859074821619259, 114), (0.036117714766358409, 47), (0.014480109885196104, 24), (0.013552086551256422, 52), (0.012737940184907388, 146)]\n",
      "origin 0.99932735426\n",
      "valid error shape, 3\n",
      "[0 1 1]\n",
      "valid error pos shape 2\n",
      "valid error index\n",
      "[ 757 2107 4227]\n",
      "valid correct index\n",
      "[ 135  190  327  330  505  534  546  596  695  812  844  858  998 1008 1021\n",
      " 1288 1307 1349 1389 1416 1491 1519 1616 1707 1722 1741 1884 1922 1938 1969\n",
      " 2168 2210 2253 2271 2277 2291 2333 2380 2443 2462 2492 2549 2550 2584 2621\n",
      " 2647 2655 2666 2708 2771 2869 2885 3005 3006 3053 3111 3208 3423 3428 3464\n",
      " 3543 3630 3752 3772 3788 3879 3912 4100 4226 4244 4262 4285 4323 4457]\n",
      "\n",
      "train error shape, 7\n",
      "[0 0 0 0 0 0 0]\n",
      "train error pos shape 0\n",
      "train error index\n",
      "[ 1311  2905  3808  4044  8065 11704 11865]\n",
      "s_3622929\n",
      "\n",
      "23\n",
      "wifi index [31, 55, 57, 62, 63, 83, 139, 160, 197, 207, 224, 289, 331, 412, 423, 1067, 1299, 1367, 2133]\n",
      "feature_importances_ [(0.2991303912804581, 139), (0.14292384992757687, 160), (0.13611612779497839, 63), (0.12263352142492168, 289), (0.07157943921470325, 83), (0.06166860220339928, 197), (0.061074337235583059, 207), (0.042379297861310783, 55), (0.025744701038277105, 412), (0.0084398998129669861, 2133)]\n",
      "origin 1.0\n",
      "valid error shape, 0\n",
      "[]\n",
      "valid error pos shape 0\n",
      "valid error index\n",
      "[]\n",
      "valid correct index\n",
      "[ 248  875 2220 3370 3774 3843 3997 4160 4325 4358]\n",
      "\n",
      "train error shape, 0\n",
      "[]\n",
      "train error pos shape 0\n",
      "train error index\n",
      "[]\n",
      "s_3643138\n",
      "\n",
      "24\n",
      "wifi index [59, 67, 72, 78, 80, 136, 154, 155, 275, 319, 356, 357, 374, 397, 406, 415, 451, 505, 507, 513, 547, 564, 720, 882]\n",
      "feature_importances_ [(0.22768774117582455, 547), (0.20833687778925361, 507), (0.09701049216580461, 513), (0.086196418256014776, 275), (0.048271629076293919, 564), (0.047147960925308811, 78), (0.04417426462489308, 136), (0.030377132386956947, 154), (0.030040447799557082, 155), (0.023044147772578929, 59)]\n",
      "origin 0.999775784753\n",
      "valid error shape, 1\n",
      "[1]\n",
      "valid error pos shape 1\n",
      "valid error index\n",
      "[906]\n",
      "valid correct index\n",
      "[ 193  205  311  908 1241 1279 1289 2010 2131 2231 2381 2442 2627 2631 3034\n",
      " 3566 3600 3728 4299]\n",
      "\n",
      "train error shape, 0\n",
      "[]\n",
      "train error pos shape 0\n",
      "train error index\n",
      "[]\n",
      "s_3644057\n",
      "\n",
      "25\n",
      "wifi index [58, 96, 106, 128, 130, 150, 167, 184, 192, 206, 212, 310, 419, 426, 476]\n",
      "feature_importances_ [(0.29086068162958978, 106), (0.20272039180736789, 128), (0.13048196069169485, 58), (0.08654670442503179, 212), (0.075465838238498256, 130), (0.062037212501025008, 96), (0.060039290769730386, 167), (0.035102902217440154, 184), (0.022018339732702862, 426), (0.018258896774284154, 150)]\n",
      "origin 1.0\n",
      "valid error shape, 0\n",
      "[]\n",
      "valid error pos shape 0\n",
      "valid error index\n",
      "[]\n",
      "valid correct index\n",
      "[  53  129  341  396  433  485  554  555  556  557  558  559  560  561  562\n",
      "  700  727  755  757  914  932 1004 1201 1232 1245 1525 2006 2037 2071 2090\n",
      " 2123 2170 2212 2229 2511 2545 2683 2768 2769 2983 3057 3168 3194 3374 3507\n",
      " 3546 3570 3603 3661 3677 3714 3715 3716 3717 3791 3792 3852 3898 3965 3966\n",
      " 3967 3968 3969 3970 4118 4224 4260 4283 4332 4444]\n",
      "\n",
      "train error shape, 0\n",
      "[]\n",
      "train error pos shape 0\n",
      "train error index\n",
      "[]\n",
      "s_3658709\n",
      "\n",
      "26\n",
      "wifi index [8, 10, 20, 23, 26, 40, 65, 112, 123, 132, 133, 147, 168, 169, 171, 175, 180, 195, 202, 230, 302, 436]\n",
      "feature_importances_ [(0.30731582076675235, 8), (0.22570803394914765, 10), (0.10882456184574275, 26), (0.080070168152627741, 65), (0.049016673559883484, 40), (0.048901084691480579, 23), (0.035573625105066856, 168), (0.032782925280911708, 123), (0.025317946788870629, 133), (0.015263270878069864, 147)]\n",
      "origin 0.992600896861\n",
      "valid error shape, 33\n",
      "[1 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 1 1 1 1 0 1 0 0 0 0 0 1 0 0 1 0 0]\n",
      "valid error pos shape 10\n",
      "valid error index\n",
      "[ 194  241  493  728  927 1068 1114 1158 1470 1493 1516 1546 1554 1590 1703\n",
      " 2094 2133 2240 2264 2283 2391 2601 2637 2789 2926 3184 3385 3498 3557 3652\n",
      " 3711 4117 4412]\n",
      "valid correct index\n",
      "[  83   84   85   86   91  132  136  202  280  326  348  424  533  564  642\n",
      "  657  658  659  670  678  687  688  691  740  777  799  821  880  940  951\n",
      "  985 1026 1029 1055 1135 1170 1221 1313 1316 1405 1465 1581 1597 1632 1652\n",
      " 1688 1705 1719 1772 1773 1800 1815 1816 1828 1844 1900 1955 1972 1987 1993\n",
      " 2014 2063 2095 2114 2120 2121 2124 2179 2196 2239 2241 2263 2265 2272 2298\n",
      " 2314 2322 2387 2390 2392 2399 2435 2461 2501 2514 2595 2707 2754 2783 2802\n",
      " 2829 2835 2915 2927 2955 2958 2959 2972 2993 3030 3062 3072 3143 3159 3241\n",
      " 3294 3301 3330 3331 3333 3358 3383 3420 3421 3422 3437 3438 3477 3478 3482\n",
      " 3496 3497 3513 3547 3549 3571 3577 3584 3695 3696 3730 3758 3785 3786 3901\n",
      " 3915 3944 3945 4036 4051 4056 4057 4059 4125 4189 4195 4200 4201 4202 4204\n",
      " 4221 4253 4292 4302 4359 4369 4370 4393 4396 4400 4417 4422 4437 4452]\n",
      "\n",
      "train error shape, 0\n",
      "[]\n",
      "train error pos shape 0\n",
      "train error index\n",
      "[]\n",
      "s_3726779\n",
      "\n",
      "27\n",
      "wifi index [58, 128, 130, 150, 248, 258, 267, 269, 322, 368, 376, 390, 424, 427, 521, 532]\n",
      "feature_importances_ [(0.27591209912539433, 258), (0.19479999812615345, 424), (0.17995273865416725, 427), (0.13407349311988284, 390), (0.089227217736808639, 269), (0.054642160292624045, 376), (0.048231388384557129, 322), (0.011868447327349232, 150), (0.0034454008798825812, 58), (0.0027260186287134461, 248)]\n",
      "origin 0.999551569507\n",
      "valid error shape, 2\n",
      "[1 1]\n",
      "valid error pos shape 2\n",
      "valid error index\n",
      "[ 650 2672]\n",
      "valid correct index\n",
      "[ 428  683 1448 1552 1613 1706 1834 2523 2533 2753 2994 2995 3246 3554 3598\n",
      " 4072]\n",
      "\n",
      "train error shape, 0\n",
      "[]\n",
      "train error pos shape 0\n",
      "train error index\n",
      "[]\n",
      "s_3727061\n",
      "\n",
      "28\n",
      "wifi index [15, 24, 32, 41, 45, 50, 52, 53, 64, 66, 88, 92, 146, 153, 217, 221, 223, 229, 245, 278, 290, 317]\n",
      "feature_importances_ [(0.17527407965064687, 229), (0.16306780450324762, 32), (0.13946669122099481, 24), (0.077618139752149748, 66), (0.07535362459609328, 317), (0.056756542460189303, 41), (0.048734677802043685, 290), (0.045710981192174975, 15), (0.039320150224797616, 146), (0.03887855514597971, 50)]\n",
      "origin 0.99932735426\n",
      "valid error shape, 3\n",
      "[1 0 1]\n",
      "valid error pos shape 2\n",
      "valid error index\n",
      "[ 868 1469 3754]\n",
      "valid correct index\n",
      "[ 665  866  904 1217 1605 1726 2184 2770 2874 3036 3377 3404 3409 3418 3508\n",
      " 3509 3610 3658 3853 4029 4327]\n",
      "\n",
      "train error shape, 7\n",
      "[0 0 0 0 0 0 0]\n",
      "train error pos shape 0\n",
      "train error index\n",
      "[ 1529  3267  4088  6281  6516  7548 11292]\n",
      "s_3738103\n",
      "\n",
      "29\n",
      "wifi index [26, 65, 132, 133, 147, 171, 180, 230, 255, 257, 405, 408, 435, 452, 456, 492, 503]\n",
      "feature_importances_ [(0.2235917428594196, 171), (0.20848460536589755, 26), (0.17722685663606955, 408), (0.10703578432714253, 255), (0.057494155652143913, 405), (0.051442542040273299, 435), (0.051272539427223242, 65), (0.031089264188762636, 133), (0.030392159931377609, 257), (0.014052199152035478, 147)]\n",
      "origin 0.999103139013\n",
      "valid error shape, 4\n",
      "[1 1 1 1]\n",
      "valid error pos shape 4\n",
      "valid error index\n",
      "[ 704  705 3287 3348]\n",
      "valid correct index\n",
      "[1889 2266 2456 2560]\n",
      "\n",
      "train error shape, 0\n",
      "[]\n",
      "train error pos shape 0\n",
      "train error index\n",
      "[]\n",
      "s_3743937\n",
      "\n",
      "30\n",
      "wifi index [130, 150, 248, 368, 403, 419, 470]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature_importances_ [(0.37167739884010154, 403), (0.27376276239327729, 248), (0.23946982408275441, 368), (0.047713290728354438, 130), (0.040244462378177005, 150), (0.017422821785360341, 470), (0.0097094397919749321, 419)]\n",
      "origin 1.0\n",
      "valid error shape, 0\n",
      "[]\n",
      "valid error pos shape 0\n",
      "valid error index\n",
      "[]\n",
      "valid correct index\n",
      "[ 887 1047 1646 1761 1762 2886 3874 3909]\n",
      "\n",
      "train error shape, 0\n",
      "[]\n",
      "train error pos shape 0\n",
      "train error index\n",
      "[]\n",
      "s_3759545\n",
      "\n",
      "31\n",
      "wifi index [58, 96, 106, 167, 184, 192, 206, 211, 219, 237, 384, 517, 582]\n",
      "feature_importances_ [(0.26840577171173768, 58), (0.18230816760122576, 192), (0.16880157498349613, 211), (0.12577883177728696, 206), (0.10715696058181966, 96), (0.051039657183039787, 219), (0.031825807717845329, 167), (0.02999191057976441, 237), (0.022340677872798025, 106), (0.0057623194282350899, 517)]\n",
      "origin 1.0\n",
      "valid error shape, 0\n",
      "[]\n",
      "valid error pos shape 0\n",
      "valid error index\n",
      "[]\n",
      "valid correct index\n",
      "[ 214  225  445 1090 1764 2561 2649 3160 3259 4060 4121 4193 4322 4440]\n",
      "\n",
      "train error shape, 0\n",
      "[]\n",
      "train error pos shape 0\n",
      "train error index\n",
      "[]\n",
      "s_3779904\n",
      "\n",
      "32\n",
      "wifi index [16, 30, 35, 39, 46, 68, 73, 84, 126, 166, 199, 203, 274, 306, 353, 377, 378, 489, 591, 642, 684]\n",
      "feature_importances_ [(0.2242857877887936, 46), (0.12423594304693779, 203), (0.1129952541394687, 274), (0.11278376536250377, 30), (0.090593696190984965, 199), (0.081759216376676311, 16), (0.073460037466162087, 306), (0.048888297712802842, 39), (0.02828599365752503, 377), (0.024775072672520292, 73)]\n",
      "origin 0.99865470852\n",
      "valid error shape, 6\n",
      "[1 1 1 0 0 0]\n",
      "valid error pos shape 3\n",
      "valid error index\n",
      "[ 809 1337 3379 4108 4110 4114]\n",
      "valid correct index\n",
      "[  69  204  316  365  467  595  647  859  979 1149 1194 1283 1293 1549 1691\n",
      " 1999 2353 2643 2654 2713 2833 2877 3018 3237 3380 3454 3527 4010 4033 4237]\n",
      "\n",
      "train error shape, 0\n",
      "[]\n",
      "train error pos shape 0\n",
      "train error index\n",
      "[]\n",
      "s_3818635\n",
      "\n",
      "33\n",
      "wifi index [2, 3, 4, 6, 7, 9, 13, 21, 27, 29, 37, 42, 49, 54, 60, 61, 71, 79, 91, 94, 125, 165]\n",
      "feature_importances_ [(0.2297554378311146, 6), (0.15004738282996363, 27), (0.11359006653650637, 2), (0.092560381781522594, 61), (0.089054768931227088, 37), (0.053493487805373385, 13), (0.04374115965250943, 21), (0.03583200786950233, 3), (0.031019757060592429, 29), (0.028656890032531253, 4)]\n",
      "origin 0.995067264574\n",
      "valid error shape, 22\n",
      "[1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 0 1]\n",
      "valid error pos shape 19\n",
      "valid error index\n",
      "[   8   94  468  606  720  863  899  989 1112 1331 1484 1513 1524 2374 2634\n",
      " 2964 3251 3860 4017 4194 4219 4301]\n",
      "valid correct index\n",
      "[ 104  184  354 1093 1397 1853 2182 2352 3628 3648 3923 4016 4088]\n",
      "\n",
      "train error shape, 7\n",
      "[0 0 0 0 0 0 0]\n",
      "train error pos shape 0\n",
      "train error index\n",
      "[ 1947  3160  3502  9290 10984 11134 11398]\n",
      "s_3827392\n",
      "\n",
      "34\n",
      "wifi index [10, 26, 40, 65, 112, 123, 133, 147, 168, 171, 175, 255, 257, 302, 393, 408, 436, 452, 466, 516, 566, 612, 614, 657]\n",
      "feature_importances_ [(0.17950661597424014, 26), (0.17649218242512554, 123), (0.16085075976838867, 65), (0.14920470553724599, 393), (0.061098051431588321, 255), (0.043110346509727684, 302), (0.039562674514913743, 516), (0.036847566237652569, 171), (0.036358987756918093, 466), (0.015470340192909014, 612)]\n",
      "origin 1.0\n",
      "valid error shape, 0\n",
      "[]\n",
      "valid error pos shape 0\n",
      "valid error index\n",
      "[]\n",
      "valid correct index\n",
      "[]\n",
      "\n",
      "train error shape, 0\n",
      "[]\n",
      "train error pos shape 0\n",
      "train error index\n",
      "[]\n",
      "s_3833950\n",
      "\n",
      "35\n",
      "wifi index [31, 55, 57, 58, 62, 63, 83, 96, 139, 160, 224, 244, 266, 423, 540]\n",
      "feature_importances_ [(0.31043195518080557, 266), (0.17885650689372293, 244), (0.17863641874731817, 83), (0.12107879127793354, 57), (0.048027519197265708, 55), (0.043484234228146075, 96), (0.025922253475950251, 423), (0.023255883844369409, 31), (0.022415520519221136, 58), (0.011138007975029748, 63)]\n",
      "origin 0.99932735426\n",
      "valid error shape, 3\n",
      "[1 0 0]\n",
      "valid error pos shape 1\n",
      "valid error index\n",
      "[1469 2503 2679]\n",
      "valid correct index\n",
      "[ 577  701 2611 3802]\n",
      "\n",
      "train error shape, 1\n",
      "[0]\n",
      "train error pos shape 0\n",
      "train error index\n",
      "[5555]\n",
      "s_3902873\n",
      "\n",
      "36\n",
      "37\n",
      "wifi index [130, 150, 248, 258, 267, 269, 322, 368, 369, 376, 390, 403, 410, 424, 427, 532, 1023]\n",
      "feature_importances_ [(0.26970707552818346, 269), (0.21702742806819747, 248), (0.19880971060888533, 368), (0.081180452971867573, 258), (0.079557709548685171, 410), (0.060610438048110567, 322), (0.036876051596640924, 532), (0.026479852516334848, 376), (0.0087388333897208542, 130), (0.0076313336552803522, 150)]\n",
      "origin 1.0\n",
      "valid error shape, 0\n",
      "[]\n",
      "valid error pos shape 0\n",
      "valid error index\n",
      "[]\n",
      "valid correct index\n",
      "[ 475 1220 1685 1692 1977 3101 3334 3743 3744 3745 3841]\n",
      "\n",
      "train error shape, 0\n",
      "[]\n",
      "train error pos shape 0\n",
      "train error index\n",
      "[]\n",
      "s_3907298\n",
      "\n",
      "38\n",
      "wifi index [31, 55, 62, 83, 244, 266, 289, 412, 423]\n",
      "feature_importances_ [(0.16544962077916028, 423), (0.14215969658153388, 244), (0.12104547694659767, 289), (0.11030707059163815, 55), (0.025549071587836654, 412), (0.018047416688896294, 266), (0.010084284726534675, 83), (0.00013015865242191335, 62), (1.0708600019711032e-05, 31)]\n",
      "origin 1.0\n",
      "valid error shape, 0\n",
      "[]\n",
      "valid error pos shape 0\n",
      "valid error index\n",
      "[]\n",
      "valid correct index\n",
      "[]\n",
      "\n",
      "train error shape, 0\n",
      "[]\n",
      "train error pos shape 0\n",
      "train error index\n",
      "[]\n",
      "s_3931209\n",
      "\n",
      "39\n",
      "wifi index [10, 20, 31, 34, 55, 57, 58, 63, 83, 96, 134, 139, 160, 197, 207, 211, 219, 224, 266, 289, 321, 331, 384, 526]\n",
      "feature_importances_ [(0.20240964454488392, 160), (0.15029677356858179, 83), (0.13659415717163767, 139), (0.088590509672638332, 207), (0.068098516970128961, 197), (0.057714861445986976, 321), (0.047320059911642308, 57), (0.035985256143387823, 31), (0.031160689920268884, 34), (0.029640438597773676, 134)]\n",
      "origin 0.999775784753\n",
      "valid error shape, 1\n",
      "[0]\n",
      "valid error pos shape 0\n",
      "valid error index\n",
      "[1468]\n",
      "valid correct index\n",
      "[  56   89 2195 2834 4442]\n",
      "\n",
      "train error shape, 1\n",
      "[0]\n",
      "train error pos shape 0\n",
      "train error index\n",
      "[7043]\n",
      "s_3939053\n",
      "\n",
      "40\n",
      "wifi index [10, 20, 23, 34, 55, 57, 62, 63, 74, 90, 104, 134, 240, 287, 407, 478, 1747]\n",
      "feature_importances_ [(0.25613748302442069, 287), (0.21346454431312953, 240), (0.2044134179987723, 478), (0.15062946715093131, 104), (0.065310208576276091, 20), (0.046506857264118809, 90), (0.024329177534164338, 134), (0.011610297529682345, 34), (0.0078883204660335389, 407), (0.0053413864176938049, 23)]\n",
      "origin 0.99932735426\n",
      "valid error shape, 3\n",
      "[1 1 1]\n",
      "valid error pos shape 3\n",
      "valid error index\n",
      "[  97 1810 2748]\n",
      "valid correct index\n",
      "[1271 1673 1936 2261 2630 3076 3292 3310 3425 3756 4320 4373]\n",
      "\n",
      "train error shape, 0\n",
      "[]\n",
      "train error pos shape 0\n",
      "train error index\n",
      "[]\n",
      "s_4005274\n",
      "\n",
      "41\n",
      "wifi index [8, 10, 20, 23, 26, 31, 34, 40, 57, 65, 74, 112, 123, 132, 133, 134, 147, 168, 175, 180, 195, 255]\n",
      "feature_importances_ [(0.22021164226172507, 8), (0.22012145070052719, 10), (0.15366665310675992, 23), (0.080841845338139678, 20), (0.072083963024192954, 26), (0.06963201900581234, 40), (0.056391972949259794, 132), (0.020143190250485211, 57), (0.017758512447785497, 255), (0.015203297398087129, 112)]\n",
      "origin 0.986098654709\n",
      "valid error shape, 62\n",
      "[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "valid error pos shape 62\n",
      "valid error index\n",
      "[  38  124  235  241  408  493  643  709  898  927 1114 1158 1394 1468 1470\n",
      " 1493 1516 1554 1590 1703 1704 1733 1783 1784 1785 1898 2074 2080 2081 2094\n",
      " 2193 2463 2677 2734 2789 2791 2821 2831 2887 3080 3131 3144 3283 3349 3354\n",
      " 3385 3400 3517 3652 3755 3757 4062 4094 4103 4117 4128 4129 4130 4177 4412\n",
      " 4435 4454]\n",
      "valid correct index\n",
      "[   6    7   49   80   81   88  106  130  175  192  209  210  239  240  249\n",
      "  252  277  312  332  347  356  360  382  447  452  457  461  472  600  622\n",
      "  644  651  652  669  696  713  822  826  854  856  873  897  901  919  920\n",
      "  923  924  964  990  995 1042 1060 1070 1083 1101 1103 1126 1162 1177 1182\n",
      " 1247 1252 1255 1256 1257 1259 1269 1304 1341 1344 1377 1452 1479 1492 1534\n",
      " 1546 1547 1548 1553 1555 1565 1580 1595 1634 1656 1657 1658 1742 1743 1753\n",
      " 1765 1771 1837 1838 1840 1870 1871 1874 1901 1921 1926 1967 1992 2017 2033\n",
      " 2034 2035 2036 2040 2078 2084 2108 2132 2175 2189 2199 2202 2222 2274 2338\n",
      " 2362 2377 2391 2412 2480 2488 2494 2504 2505 2510 2513 2520 2529 2532 2544\n",
      " 2571 2576 2605 2609 2619 2637 2644 2673 2675 2676 2679 2682 2687 2693 2731\n",
      " 2735 2775 2790 2830 2832 2838 2844 2863 2880 2881 2884 2898 2901 2911 2926\n",
      " 2935 2982 2989 3023 3040 3073 3081 3082 3083 3119 3149 3169 3184 3186 3202\n",
      " 3269 3278 3281 3282 3284 3288 3306 3368 3369 3375 3381 3398 3399 3401 3402\n",
      " 3403 3419 3436 3490 3516 3530 3531 3552 3553 3555 3557 3562 3639 3670 3676\n",
      " 3679 3726 3807 3810 3819 3859 3880 3914 3917 3943 3946 3973 4039 4069 4104\n",
      " 4131 4136 4173 4191 4203 4215 4236 4254 4268 4298 4330 4338 4355 4364 4367\n",
      " 4376 4390 4395 4453]\n",
      "\n",
      "train error shape, 0\n",
      "[]\n",
      "train error pos shape 0\n",
      "train error index\n",
      "[]\n",
      "s_4009954\n",
      "\n",
      "42\n",
      "wifi index [107, 161, 173, 182, 208, 228, 231, 238, 242, 265, 268, 271, 285, 286, 293, 343, 349, 365, 401, 434, 437, 442, 450, 481]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature_importances_ [(0.18206771882718509, 161), (0.14868216551324726, 182), (0.099272115173929878, 208), (0.078974893726704923, 173), (0.076005454891502386, 107), (0.065842062875068871, 231), (0.06491321716241813, 242), (0.048528697153713156, 265), (0.03713596304066874, 238), (0.03611371210783991, 285)]\n",
      "origin 0.997757847534\n",
      "valid error shape, 10\n",
      "[1 1 1 0 1 1 1 1 0 1]\n",
      "valid error pos shape 8\n",
      "valid error index\n",
      "[ 290  415  471 1230 1511 1589 1940 1941 3698 4155]\n",
      "valid correct index\n",
      "[   1  134  171  172  173  174  211  258  297  367  438  476  528  549  576\n",
      "  719  788  813  827  886 1012 1046 1116 1226 1385 1456 1459 1471 1659 1690\n",
      " 1725 1759 1769 1795 1852 1908 1910 1923 1971 2012 2025 2061 2097 2214 2255\n",
      " 2257 2269 2299 2300 2301 2321 2348 2410 2465 2761 2850 2943 3010 3093 3105\n",
      " 3179 3209 3236 3244 3248 3249 3255 3265 3314 3335 3412 3525 3612 3672 3700\n",
      " 3709 3734 3899 3947 4213 4290 4389]\n",
      "\n",
      "train error shape, 2\n",
      "[0 0]\n",
      "train error pos shape 0\n",
      "train error index\n",
      "[ 2515 10853]\n",
      "s_433337\n",
      "\n",
      "43\n",
      "wifi index [2, 3, 4, 5, 6, 7, 9, 13, 14, 33, 42, 48, 95, 213, 326, 433]\n",
      "feature_importances_ [(0.36837066585536299, 14), (0.20660691722137012, 95), (0.1364353733507874, 213), (0.095652228561860009, 42), (0.045477214722501484, 5), (0.042572462402027098, 2), (0.036080064625008811, 326), (0.029582620254495921, 4), (0.011690814669049619, 433), (0.0070235228676649476, 3)]\n",
      "origin 0.999103139013\n",
      "valid error shape, 4\n",
      "[0 1 1 1]\n",
      "valid error pos shape 3\n",
      "valid error index\n",
      "[ 630 2355 3052 3247]\n",
      "valid correct index\n",
      "[  20  111  127  329  542  631  646  649  896 1027 1110 1413 1443 1625 1651\n",
      " 1689 1709 1822 1939 1990 2134 2145 2169 2191 2254 2373 2598 3167 3304 3321\n",
      " 3489 3535 3659 3798 3955 3964 4034 4120 4154 4212 4225 4265 4281 4291 4326\n",
      " 4343 4398 4415]\n",
      "\n",
      "train error shape, 0\n",
      "[]\n",
      "train error pos shape 0\n",
      "train error index\n",
      "[]\n",
      "s_490562\n",
      "\n",
      "44\n",
      "wifi index [59, 67, 72, 78, 80, 109, 136, 143, 154, 155, 239, 249, 253, 275, 301, 319, 336, 374, 400, 458, 569]\n",
      "feature_importances_ [(0.22225894592753406, 67), (0.18950099078740718, 72), (0.1640320358849624, 59), (0.10705949856551425, 301), (0.075573049311161239, 239), (0.072271979404590236, 80), (0.052560976995489041, 155), (0.039692588288802455, 78), (0.01477890005111362, 109), (0.014103300860162368, 143)]\n",
      "origin 0.999551569507\n",
      "valid error shape, 2\n",
      "[1 1]\n",
      "valid error pos shape 2\n",
      "valid error index\n",
      "[ 758 4021]\n",
      "valid correct index\n",
      "[  31  102  138  379 1045 1610 1767 1830 2056 2453 2985 3000 3142 3501 3790]\n",
      "\n",
      "train error shape, 0\n",
      "[]\n",
      "train error pos shape 0\n",
      "train error index\n",
      "[]\n",
      "s_490980\n",
      "\n",
      "45\n",
      "wifi index [59, 67, 72, 78, 80, 136, 143, 154, 155, 205, 239, 249, 275, 319, 356, 357, 367, 374, 397, 406, 415, 431, 445, 451, 465, 501]\n",
      "feature_importances_ [(0.18065906197817444, 154), (0.12509215719979244, 275), (0.098388738219854999, 155), (0.07819732719923092, 80), (0.068126006841724832, 406), (0.049452693924619789, 249), (0.042702145058197465, 59), (0.038478533050055309, 136), (0.035909224875180487, 67), (0.033622763768831568, 357)]\n",
      "origin 0.997085201794\n",
      "valid error shape, 13\n",
      "[0 1 1 1 1 0 0 0 0 1 1 1 0]\n",
      "valid error pos shape 7\n",
      "valid error index\n",
      "[ 376 1173 1174 1175 1176 1279 2131 2646 2650 2933 4054 4061 4351]\n",
      "valid correct index\n",
      "[  46  100  223  525  527  597  598  698  775 1084 1085 1086 1087 1196 1314\n",
      " 1372 1507 2039 2279 2658 2920 2921 2922 2923 2924 3195 3488 3653 3684 3767\n",
      " 3908 4063 4071]\n",
      "\n",
      "train error shape, 1\n",
      "[0]\n",
      "train error pos shape 0\n",
      "train error index\n",
      "[8471]\n",
      "s_491277\n",
      "\n",
      "46\n",
      "wifi index [88, 92, 98, 101, 103, 116, 122, 233, 236, 254, 256, 295, 300, 330, 345, 349, 352, 383, 404, 422, 500, 556, 643, 696, 738]\n",
      "feature_importances_ [(0.20126831853946023, 256), (0.16751294691793531, 422), (0.12562178496823026, 345), (0.088380796757704091, 98), (0.04213746518389562, 233), (0.038563710307145185, 349), (0.036709945994286397, 500), (0.033681297119594791, 254), (0.030749312239236392, 352), (0.028837295082547822, 295)]\n",
      "origin 1.0\n",
      "valid error shape, 0\n",
      "[]\n",
      "valid error pos shape 0\n",
      "valid error index\n",
      "[]\n",
      "valid correct index\n",
      "[ 773  792  810  823  865 1200 1447 1455 1611 1629 1674 1886 2096 2139 2140\n",
      " 2185 2224 2357 2629 2671 2704 2828 2990 3394 3887]\n",
      "\n",
      "train error shape, 0\n",
      "[]\n",
      "train error pos shape 0\n",
      "train error index\n",
      "[]\n",
      "s_493201\n",
      "\n",
      "47\n",
      "wifi index [59, 67, 72, 80, 143, 155, 179, 235, 239, 253, 277, 283, 301, 336, 358, 429, 458, 874, 2263, 2547]\n",
      "feature_importances_ [(0.23572600182070397, 458), (0.16634850598680981, 336), (0.15171810272515956, 429), (0.11112117004096242, 874), (0.085578972491558242, 358), (0.043925951022495101, 80), (0.035474333356660469, 72), (0.024252134782838126, 67), (0.024109680694359564, 2263), (0.023333054028564854, 253)]\n",
      "origin 1.0\n",
      "valid error shape, 0\n",
      "[]\n",
      "valid error pos shape 0\n",
      "valid error index\n",
      "[]\n",
      "valid correct index\n",
      "[]\n",
      "\n",
      "train error shape, 0\n",
      "[]\n",
      "train error pos shape 0\n",
      "train error index\n",
      "[]\n",
      "s_496413\n",
      "\n",
      "48\n",
      "wifi index [2, 3, 4, 5, 7, 11, 13, 14, 16, 33, 38, 42, 82, 95, 99, 102, 118, 204]\n",
      "feature_importances_ [(0.35185905362168257, 5), (0.12759298555640186, 204), (0.099960331500218824, 4), (0.093070107590530179, 14), (0.067373905908550122, 2), (0.064382561849957859, 33), (0.06164388702837198, 38), (0.045838153924074317, 42), (0.014691534821358369, 16), (0.011757223740364276, 3)]\n",
      "origin 0.99932735426\n",
      "valid error shape, 3\n",
      "[1 1 1]\n",
      "valid error pos shape 3\n",
      "valid error index\n",
      "[ 343 3267 3712]\n",
      "valid correct index\n",
      "[  15  355  371  372  402  526  681  689  861  879  945 1092 1340 1364 1536\n",
      " 1591 1607 1799 1933 1982 2066 2183 2223 2587 2600 2665 3024 3408 3453 3460\n",
      " 3512 3649 3725 4015 4147 4258 4277]\n",
      "\n",
      "train error shape, 0\n",
      "[]\n",
      "train error pos shape 0\n",
      "train error index\n",
      "[]\n",
      "s_497803\n",
      "\n",
      "49\n",
      "wifi index [2, 3, 4, 6, 7, 13, 21, 27, 29, 37, 49, 54, 60, 61, 71, 91, 94, 125, 165, 232, 394, 432]\n",
      "feature_importances_ [(0.28580126656337318, 6), (0.15136516820622858, 27), (0.14231846028337178, 61), (0.1098591931982365, 2), (0.10544697394622919, 37), (0.05021892357174635, 3), (0.043731386065237562, 21), (0.01704098066002158, 4), (0.013974786225060315, 7), (0.010364753408859752, 49)]\n",
      "origin 0.997533632287\n",
      "valid error shape, 11\n",
      "[1 0 0 1 1 1 1 1 1 1 1]\n",
      "valid error pos shape 9\n",
      "valid error index\n",
      "[  79  466  899 1399 1401 1671 1672 1680 3028 3029 3253]\n",
      "valid correct index\n",
      "[  35   44  604  610  721 1249 1396 1400 1402 1453 1497 1682 1768 1899 2557\n",
      " 2678 2809 2843 3126 3657 3760 3922 4089 4138 4239 4402]\n",
      "\n",
      "train error shape, 1\n",
      "[0]\n",
      "train error pos shape 0\n",
      "train error index\n",
      "[10504]\n",
      "s_501724\n",
      "\n",
      "50\n",
      "wifi index [15, 45, 47, 136, 186, 218, 251, 284, 318, 328, 338, 342, 344, 363, 391, 444]\n",
      "feature_importances_ [(0.29370446197573213, 251), (0.24162848791912547, 284), (0.1627910684258162, 318), (0.082026260366433568, 338), (0.068212944776038148, 363), (0.039926594576442979, 218), (0.033421739073270604, 344), (0.031549162414947617, 328), (0.012383098099119264, 444), (0.010606124272672671, 342)]\n",
      "origin 0.999551569507\n",
      "valid error shape, 2\n",
      "[1 0]\n",
      "valid error pos shape 1\n",
      "valid error index\n",
      "[ 867 3011]\n",
      "valid correct index\n",
      "[   2  177  283  289  298  422  831  834 1095 1180 1199 1210 1261 1457 1520\n",
      " 1593 1603 1647 1648 1649 1650 2024 2371 2388 2389 2432 2500 2541 2579 2648\n",
      " 2772 2840 2848 3059 3225 3227 3396 3560 3613 3704 3903 4008 4066 4124 4276\n",
      " 4318 4350 4366 4371]\n",
      "\n",
      "train error shape, 0\n",
      "[]\n",
      "train error pos shape 0\n",
      "train error index\n",
      "[]\n",
      "s_504009\n",
      "\n",
      "51\n",
      "wifi index [4, 5, 11, 13, 16, 18, 22, 25, 33, 35, 38, 43, 82, 99, 102, 105, 118, 194, 204]\n",
      "feature_importances_ [(0.26883376736440789, 82), (0.17885719165761438, 38), (0.16216783635642534, 99), (0.10896762955570893, 118), (0.1033320883824934, 102), (0.027683385348693527, 11), (0.02733200462798523, 5), (0.021039061802320052, 13), (0.019509977213462898, 4), (0.019228399490853511, 25)]\n",
      "origin 0.995515695067\n",
      "valid error shape, 20\n",
      "[1 1 1 1 0 0 1 0 0 0 1 1 0 0 1 1 1 0 0 0]\n",
      "valid error pos shape 10\n",
      "valid error index\n",
      "[ 145  146  151  165  896 1591 1758 1940 1976 2143 2159 2259 2961 3391 3960\n",
      " 4067 4068 4079 4225 4265]\n",
      "valid correct index\n",
      "[  16  113  115  131  142  143  144  147  148  149  150  152  153  154  155\n",
      "  156  157  158  159  160  161  162  163  164  166  276  361  393  455  530\n",
      "  575  711  712  731  732  765  890  891 1121 1155 1228 1290 1350 1390 1392\n",
      " 1393 1440 1441 1475 1481 1488 1489 1508 1509 1570 1571 1577 1578 1592 1598\n",
      " 1833 1839 1873 2005 2077 2138 2226 2227 2330 2411 2416 2436 2519 2546 2547\n",
      " 2656 2657 2669 2670 2718 2801 2826 2914 2996 3045 3271 3302 3356 3459 3503\n",
      " 3556 3638 3764 3826 3837 3902 4126 4127 4289 4408 4409]\n",
      "\n",
      "train error shape, 17\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "train error pos shape 0\n",
      "train error index\n",
      "[  125   779   887  2500  2721  4049  4157  4662  4833  4854  5686  5960\n",
      "  8927  9285 10203 10570 11830]\n",
      "s_506583\n",
      "\n",
      "52\n",
      "wifi index [2, 3, 4, 5, 6, 7, 9, 13, 14, 21, 27, 29, 37, 42, 48, 49, 60, 61, 71, 91, 232, 273]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature_importances_ [(0.2547135637993615, 2), (0.15818682679544871, 3), (0.10360872057581703, 6), (0.098685435293334012, 7), (0.066886456354804072, 71), (0.054720698227628171, 9), (0.054507963838970726, 27), (0.03679310297486435, 4), (0.022815863944859485, 60), (0.021469252646613889, 49)]\n",
      "origin 0.991928251121\n",
      "valid error shape, 36\n",
      "[1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 0 1 0 0 1 1 1 1 1 0 1 1 1 1 1 0 1 1 1 0 1]\n",
      "valid error pos shape 29\n",
      "valid error index\n",
      "[  17   40  122  279  465  544  801  846 1097 1099 1544 1630 1752 1758 1770\n",
      " 1846 1885 1979 2048 2403 2472 2474 2484 2909 2965 2973 3026 3157 3413 3480\n",
      " 3506 3689 3827 3828 4078 4305]\n",
      "valid correct index\n",
      "[  27   67   98  176  267  349  414  459  462  473  474  492  524  543  753\n",
      "  782  783  784 1031 1208 1361 1398 1445 1473 1486 1490 1523 1566 1631 1756\n",
      " 1788 1826 1827 1937 1989 1996 2032 2178 2228 2431 2437 2466 2467 2482 2642\n",
      " 2746 2759 3117 3123 3192 3204 3229 3243 3258 3260 3430 3443 3444 3492 3750\n",
      " 3796 4012 4045 4105 4106 4143 4144 4272 4273 4313]\n",
      "\n",
      "train error shape, 0\n",
      "[]\n",
      "train error pos shape 0\n",
      "train error index\n",
      "[]\n",
      "s_506597\n",
      "\n",
      "53\n",
      "wifi index [15, 24, 32, 41, 45, 50, 52, 53, 64, 66, 88, 92, 101, 217, 223, 272, 280, 294, 303, 347, 425]\n",
      "feature_importances_ [(0.26154054438570212, 53), (0.16546950093205828, 50), (0.13797771636925538, 41), (0.10694190865484561, 64), (0.048197164762837863, 66), (0.045797813365987955, 32), (0.038816214283433695, 272), (0.033547498767336481, 24), (0.030436630196661759, 223), (0.021742930596640796, 88)]\n",
      "origin 0.996188340807\n",
      "valid error shape, 17\n",
      "[1 1 1 1 0 1 0 1 1 0 1 0 0 1 1 1 0]\n",
      "valid error pos shape 11\n",
      "valid error index\n",
      "[ 504  993 1145 1388 1918 1983 2386 2742 2784 2853 2934 2969 3227 3620 3809\n",
      " 4093 4319]\n",
      "valid correct index\n",
      "[   0   32  203  302  303  319  374  413  584  592  636  717  796  843  907\n",
      " 1298 1299 1345 1376 1450 1505 1559 1789 1793 1860 1905 2000 2127 2200 2296\n",
      " 2305 2473 2489 2583 2719 2723 2736 2871 2897 2951 3056 3122 3125 3155 3183\n",
      " 3289 3327 3355 3520 3548 3816 3836 3920 3933 4053 4158 4165 4198 4199 4248\n",
      " 4249 4269 4284 4308 4377 4424 4448]\n",
      "\n",
      "train error shape, 6\n",
      "[0 0 0 0 0 0]\n",
      "train error pos shape 0\n",
      "train error index\n",
      "[ 2179  2213  2877  6036  8632 10011]\n",
      "s_506646\n",
      "\n",
      "54\n",
      "wifi index [15, 24, 32, 41, 45, 50, 52, 53, 64, 66, 88, 92, 146, 153, 217, 221, 229, 245, 290, 308]\n",
      "feature_importances_ [(0.23297323345582199, 32), (0.21968905888745907, 221), (0.20777796489071507, 24), (0.10712306675127481, 308), (0.054982341963924201, 15), (0.03894920749968122, 245), (0.036487813349366946, 153), (0.023856546633197549, 66), (0.016809039512541812, 50), (0.016008975040983999, 217)]\n",
      "origin 0.999551569507\n",
      "valid error shape, 2\n",
      "[1 1]\n",
      "valid error pos shape 2\n",
      "valid error index\n",
      "[3636 3927]\n",
      "valid correct index\n",
      "[  60  107  215  216  221  222  232  489  623  741  768  820  862 1073 1268\n",
      " 1291 1373 1415 1942 2186 2268 2276 2440 2475 2710 3035 3055 3163 3223 3275\n",
      " 3433 3559 3592 3593 3688 3833 3892 3907 4028 4077 4246 4251 4303 4372]\n",
      "\n",
      "train error shape, 0\n",
      "[]\n",
      "train error pos shape 0\n",
      "train error index\n",
      "[]\n",
      "s_507000\n",
      "\n",
      "55\n",
      "wifi index [2, 3, 4, 5, 6, 7, 9, 13, 14, 21, 27, 29, 33, 37, 42, 48, 49, 60, 71, 77, 86, 95, 193, 220]\n",
      "feature_importances_ [(0.20809440471677304, 9), (0.1764775338081804, 2), (0.1034777889713035, 7), (0.09455642907694492, 3), (0.06437393301757767, 13), (0.060136498287285227, 4), (0.041874002810080919, 48), (0.034414146602679593, 14), (0.034337869876084962, 5), (0.029013763146229665, 6)]\n",
      "origin 0.982735426009\n",
      "valid error shape, 77\n",
      "[1 1 0 1 0 1 1 1 0 1 1 1 1 1 1 1 1 0 0 1 1 1 0 0 0 1 1 0 0 0 1 1 0 0 0 1 1\n",
      " 1 1 1 1 1 1 0 1 1 1 1 0 1 0 1 0 0 0 0 1 1 1 1 0 0 1 0 0 0 0 1 1 1 1 1 1 1\n",
      " 1 1 0]\n",
      "valid error pos shape 49\n",
      "valid error index\n",
      "[  78   96  122  250  267  412  431  432  543  587  588  591  630  632  884\n",
      "  958 1033 1097 1099 1230 1276 1395 1566 1630 1631 1693 1695 1752 1826 1827\n",
      " 1846 1867 1885 1909 1931 1979 2248 2249 2267 2297 2354 2393 2439 2474 2539\n",
      " 2552 2575 2623 2669 2712 2747 2824 2973 3026 3027 3028 3112 3115 3373 3378\n",
      " 3413 3534 3542 3564 3689 4067 4068 4078 4079 4108 4111 4114 4115 4153 4192\n",
      " 4263 4305]\n",
      "valid correct index\n",
      "[   3    5   25   26   30   45   51   68   75  128  212  217  228  238  243\n",
      "  244  247  264  265  268  278  281  292  304  324  358  359  366  370  390\n",
      "  391  392  400  435  444  446  460  470  487  490  541  570  583  589  590\n",
      "  601  603  625  633  635  654  666  671  690  708  714  722  729  749  767\n",
      "  793  802  829  841  842  848  849  864  874  876  922  931  954  956  963\n",
      "  976  978  983 1003 1030 1035 1098 1100 1115 1147 1153 1154 1169 1171 1192\n",
      " 1215 1219 1224 1227 1229 1233 1251 1267 1270 1272 1280 1286 1296 1301 1315\n",
      " 1325 1333 1334 1366 1414 1454 1476 1483 1517 1564 1567 1583 1600 1614 1635\n",
      " 1683 1739 1801 1802 1813 1814 1821 1849 1855 1877 1880 1896 1906 1915 1916\n",
      " 1953 1957 1958 1980 1988 2004 2015 2029 2048 2062 2069 2083 2100 2111 2116\n",
      " 2119 2166 2221 2225 2250 2251 2252 2258 2290 2312 2369 2379 2395 2401 2402\n",
      " 2406 2407 2408 2445 2446 2447 2449 2469 2478 2485 2486 2509 2534 2538 2551\n",
      " 2553 2573 2580 2612 2618 2628 2639 2640 2664 2681 2690 2695 2698 2699 2700\n",
      " 2705 2717 2729 2732 2737 2738 2740 2741 2743 2762 2805 2806 2815 2837 2876\n",
      " 2878 2882 2910 2930 2950 2954 2961 2965 3002 3019 3063 3064 3065 3074 3084\n",
      " 3106 3109 3129 3130 3136 3152 3153 3156 3232 3242 3252 3264 3303 3325 3326\n",
      " 3329 3357 3384 3410 3426 3431 3442 3487 3500 3506 3523 3536 3541 3558 3563\n",
      " 3572 3574 3575 3576 3582 3583 3601 3602 3629 3632 3645 3650 3651 3660 3673\n",
      " 3678 3687 3697 3724 3732 3733 3735 3751 3753 3762 3770 3775 3804 3830 3835\n",
      " 3846 3849 3854 3856 3858 3869 3905 3921 3935 3939 3948 3952 3953 3954 4013\n",
      " 4023 4070 4080 4084 4107 4109 4110 4112 4113 4116 4141 4175 4178 4184 4234\n",
      " 4247 4256 4274 4275 4287 4297 4311 4312 4378 4379 4381 4386 4391 4419 4423\n",
      " 4436 4458]\n",
      "\n",
      "train error shape, 15\n",
      "[0 0 0 0 1 0 0 0 0 0 0 0 0 0 0]\n",
      "train error pos shape 1\n",
      "train error index\n",
      "[  466   606  1265  1335  2203  3193  3478  3814  4695  6527  7214  8639\n",
      " 10503 11086 11789]\n",
      "s_508287\n",
      "\n",
      "56\n",
      "wifi index [6, 54, 59, 67, 72, 78, 80, 109, 120, 121, 138, 140, 156, 239, 264, 301]\n",
      "feature_importances_ [(0.29436763161407697, 109), (0.16952639151074847, 72), (0.16946014486482447, 156), (0.085625211796462403, 140), (0.080669320620774101, 67), (0.076881633725962559, 59), (0.043479321812673943, 138), (0.028835393846207417, 54), (0.015189971492974434, 120), (0.011321914034866046, 121)]\n",
      "origin 0.99932735426\n",
      "valid error shape, 3\n",
      "[1 1 1]\n",
      "valid error pos shape 3\n",
      "valid error index\n",
      "[ 191 1909 3805]\n",
      "valid correct index\n",
      "[ 197  206  269  550  680  745  746  748  771  818  967 1002 1034 1102 1156\n",
      " 1216 1234 1292 1543 1579 1609 1819 1917 1943 2001 2115 2234 2451 2471 2540\n",
      " 2650 2662 2845 3050 3272 3446 3526 3534 4183 4414 4416]\n",
      "\n",
      "train error shape, 1\n",
      "[0]\n",
      "train error pos shape 0\n",
      "train error index\n",
      "[5673]\n",
      "s_509630\n",
      "\n",
      "57\n",
      "wifi index [15, 24, 45, 47, 52, 70, 131, 151, 164, 186, 187, 216, 243, 247, 472, 491, 549, 629, 631, 649]\n",
      "feature_importances_ [(0.18294704772514106, 549), (0.15795264683210103, 47), (0.11843512678113882, 247), (0.099529414748914871, 216), (0.071069292827316236, 631), (0.069922615644321021, 131), (0.067400379870669727, 629), (0.049255517530760379, 243), (0.04125683926808113, 649), (0.036061767698899162, 15)]\n",
      "origin 0.998206278027\n",
      "valid error shape, 8\n",
      "[1 0 0 1 1 1 1 1]\n",
      "valid error pos shape 6\n",
      "valid error index\n",
      "[ 458  939 1287 2620 2969 3405 3949 4019]\n",
      "valid correct index\n",
      "[  48  103  220  234  411  500  501  620  710  733  747  752  808  953 1016\n",
      " 1190 1231 1369 1370 1371 1383 1463 1540 1541 1562 1563 1576 1615 1808 1854\n",
      " 1965 2057 2058 2092 2093 2310 2334 2404 2419 2487 2581 2594 2696 2697 2919\n",
      " 2925 2992 3185 3206 3787 3848 3928 3929 3950 4157 4167 4230 4238 4280 4307\n",
      " 4388 4433]\n",
      "\n",
      "train error shape, 1\n",
      "[0]\n",
      "train error pos shape 0\n",
      "train error index\n",
      "[8459]\n",
      "s_510225\n",
      "\n",
      "58\n",
      "wifi index [44, 51, 75, 76, 81, 85, 89, 98, 103, 115, 116, 122, 145, 198, 226, 254, 332, 360]\n",
      "feature_importances_ [(0.3067399339757792, 44), (0.17692921018482571, 75), (0.12571704364266495, 76), (0.11485444186160912, 89), (0.092345563459534347, 115), (0.058409105181576368, 116), (0.041057139366525462, 85), (0.016829639636857924, 51), (0.014516488021237605, 103), (0.012139255259111126, 98)]\n",
      "origin 0.998430493274\n",
      "valid error shape, 7\n",
      "[1 1 1 1 1 1 1]\n",
      "valid error pos shape 7\n",
      "valid error index\n",
      "[ 306  309 1684 1696 1897 3318 3319]\n",
      "valid correct index\n",
      "[  14   33   76   92  101  110  230  251  294  305  308  334  357  375  394\n",
      "  416  599  607  706  715  785  814  825  871  921  925  955  980  984 1011\n",
      " 1076 1120 1124 1150 1195 1240 1354 1356 1386 1418 1423 1467 1542 1587 1612\n",
      " 1730 1731 1736 1817 1845 1859 1956 1959 1964 1966 2109 2161 2230 2260 2307\n",
      " 2320 2384 2405 2433 2479 2535 2591 2608 2663 2706 2727 2851 2903 2916 2997\n",
      " 3008 3044 3054 3108 3124 3135 3177 3221 3222 3224 3245 3268 3286 3346 3393\n",
      " 3625 3631 3675 3699 3719 3782 3793 3797 3817 3831 3847 3897 3918 3999 4026\n",
      " 4037 4055 4137 4172 4205 4266 4321 4394 4404 4447]\n",
      "\n",
      "train error shape, 1\n",
      "[0]\n",
      "train error pos shape 0\n",
      "train error index\n",
      "[393]\n",
      "s_510334\n",
      "\n",
      "59\n",
      "wifi index [2, 6, 27, 29, 37, 54, 59, 61, 67, 72, 78, 91, 109, 120, 121, 125, 138, 140, 156, 165, 264, 457]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature_importances_ [(0.24417666524833967, 138), (0.12969544379134659, 120), (0.12020783815534332, 109), (0.11054365930995373, 264), (0.10503481544771373, 121), (0.091144293234543525, 54), (0.033119751137346914, 140), (0.025433044674331357, 125), (0.024797187084979525, 27), (0.023377640311210295, 156)]\n",
      "origin 0.99932735426\n",
      "valid error shape, 3\n",
      "[1 1 0]\n",
      "valid error pos shape 2\n",
      "valid error index\n",
      "[ 386  388 1033]\n",
      "valid correct index\n",
      "[  19  139  140  226  387  389  434  553  605  707  781 1038 1050 1355 1357\n",
      " 1550 1585 1670 1831 2165 2194 2197 2273 2562 2572 2674 2811 2883 2900 3121\n",
      " 3367 3605 3761 3820 3822 3823 4090 4097 4271 4385]\n",
      "\n",
      "train error shape, 0\n",
      "[]\n",
      "train error pos shape 0\n",
      "train error index\n",
      "[]\n",
      "s_512700\n",
      "\n",
      "60\n",
      "wifi index [2, 3, 4, 6, 7, 13, 21, 27, 29, 37, 49, 54, 61, 91, 120, 121, 125, 140, 165, 605]\n",
      "feature_importances_ [(0.24258985521617693, 165), (0.18352055535555944, 125), (0.13601052039856276, 6), (0.11604941832002125, 91), (0.060695276894087359, 61), (0.043638518602418658, 54), (0.042366070694789232, 121), (0.042183680983341985, 2), (0.033776644421177768, 37), (0.032788052224110582, 27)]\n",
      "origin 0.999551569507\n",
      "valid error shape, 2\n",
      "[1 1]\n",
      "valid error pos shape 2\n",
      "valid error index\n",
      "[3821 4235]\n",
      "valid correct index\n",
      "[ 133  257  947  994 1013 1138 1326 1633 1716 1858 1869 2031 2044 2311 2382\n",
      " 2385 2604 2624 2653 2852 3134 3154 3239 3266 4420]\n",
      "\n",
      "train error shape, 0\n",
      "[]\n",
      "train error pos shape 0\n",
      "train error index\n",
      "[]\n",
      "s_514913\n",
      "\n",
      "61\n",
      "wifi index [2, 3, 4, 6, 7, 9, 13, 21, 27, 29, 37, 49, 54, 60, 61, 71, 79, 91, 94, 120, 125, 165, 232]\n",
      "feature_importances_ [(0.23156563487491569, 6), (0.17768292104015901, 27), (0.093683309978479748, 61), (0.084375120810593701, 2), (0.054451071451739344, 37), (0.043968703359485897, 4), (0.040358687177527011, 3), (0.037203223163113877, 21), (0.034581025377516346, 29), (0.034229449592347083, 13)]\n",
      "origin 0.994843049327\n",
      "valid error shape, 23\n",
      "[0 1 1 0 1 0 0 0 1 0 0 1 1 0 0 0 1 1 1 1 1 1 1]\n",
      "valid error pos shape 13\n",
      "valid error index\n",
      "[   8   18   94  354  466  468  470  863  917 1093 1524 1876 2122 2374 2634\n",
      " 2964 3025 3027 3254 4027 4149 4219 4403]\n",
      "valid correct index\n",
      "[ 183  779  786  795 1032 1051 1052 1305 1624 1738 1787 1872 1986 1994 2125\n",
      " 2313 2351 2483 2641 2680 2747 2787 2963 2976 4017]\n",
      "\n",
      "train error shape, 0\n",
      "[]\n",
      "train error pos shape 0\n",
      "train error index\n",
      "[]\n",
      "s_515010\n",
      "\n",
      "62\n",
      "wifi index [59, 67, 72, 78, 80, 136, 154, 155, 205, 276, 291, 312, 319, 323, 354, 379, 397, 400, 498, 513]\n",
      "feature_importances_ [(0.33210182513308739, 276), (0.14235788919352302, 291), (0.13962648964197685, 312), (0.072268013173141604, 136), (0.065674382132899431, 78), (0.056211378972707129, 379), (0.040010866910030599, 59), (0.03573159786893413, 323), (0.034776585721502759, 205), (0.022361577661665775, 80)]\n",
      "origin 0.999551569507\n",
      "valid error shape, 2\n",
      "[1 1]\n",
      "valid error pos shape 2\n",
      "valid error index\n",
      "[3981 3984]\n",
      "valid correct index\n",
      "[  34  291  499  673  756  803  946  950 1132 1558 1617 1665 1755 1766 1963\n",
      " 2073 2172 2328 2645 2646 2785 3199 3200 3220 3313 3450 3682 3736 3806 3857\n",
      " 3976 3977 3978 3979 3980 3982 3983 3985 3986 3987 3988 3989 3990 3991 3992\n",
      " 3993 3994 3995 3996 4438 4439]\n",
      "\n",
      "train error shape, 21\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "train error pos shape 0\n",
      "train error index\n",
      "[   44    45   213  1163  1458  1845  1872  2406  2885  3119  4826  5003\n",
      "  6480  6536  6541  6690  9202  9614  9806  9897 11646]\n",
      "s_515753\n",
      "\n",
      "63\n",
      "wifi index [67, 72, 78, 80, 129, 143, 154, 155, 176, 235, 241, 253, 263, 270, 277, 281, 283, 336, 358, 429, 458]\n",
      "feature_importances_ [(0.26223388560757732, 235), (0.1761966429854239, 277), (0.12825044161725724, 143), (0.089126073028495928, 336), (0.071975625606343602, 283), (0.066994641506899821, 80), (0.051358221298716866, 270), (0.033621219096343608, 176), (0.02308969126425077, 241), (0.018800393736767076, 263)]\n",
      "origin 0.99932735426\n",
      "valid error shape, 3\n",
      "[0 0 1]\n",
      "valid error pos shape 1\n",
      "valid error index\n",
      "[ 588 2103 2701]\n",
      "valid correct index\n",
      "[ 116  242  295  310  448  498  613  883  975 1024 1025 1072 1163 1266 1302\n",
      " 1404 1608 1677 1803 2003 2038 2332 2702 2822 2823 2892 2991 3277 3280 3350\n",
      " 3626 3861 3931 4020 4076 4082 4282]\n",
      "\n",
      "train error shape, 0\n",
      "[]\n",
      "train error pos shape 0\n",
      "train error index\n",
      "[]\n",
      "s_517510\n",
      "\n",
      "64\n",
      "wifi index [11, 16, 18, 22, 25, 30, 35, 38, 39, 43, 46, 69, 73, 81, 85, 87, 105, 172, 181, 194]\n",
      "feature_importances_ [(0.2601332220117854, 18), (0.21266283790087431, 11), (0.17085100845768222, 22), (0.07956972296218455, 35), (0.069199811117521187, 25), (0.04589941080299579, 69), (0.033825503666449389, 43), (0.023150729691656383, 46), (0.019013927335237404, 105), (0.017320248820268312, 81)]\n",
      "origin 0.994618834081\n",
      "valid error shape, 24\n",
      "[0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 1 1 0 0 0 0 0 0 1]\n",
      "valid error pos shape 5\n",
      "valid error index\n",
      "[ 127  338  368  631 1062 1110 1538 2169 2192 2582 2598 2633 3158 3550 3798\n",
      " 3971 3972 4079 4128 4129 4130 4174 4343 4397]\n",
      "valid correct index\n",
      "[  21   36   37   57   65   77   99  114  196  218  229  236  296  314  331\n",
      "  339  340  363  377  384  420  426  483  484  486  506  511  540  579  580\n",
      "  581  692  693  807  819  852  909  968  969  972  986 1005 1006 1007 1019\n",
      " 1028 1049 1058 1127 1152 1235 1260 1275 1332 1403 1442 1458 1462 1482 1506\n",
      " 1512 1514 1521 1526 1574 1660 1676 1737 1748 1786 1796 1835 1866 1868 1875\n",
      " 1890 1907 1912 1914 1961 2007 2013 2016 2023 2026 2028 2047 2049 2054 2142\n",
      " 2143 2146 2147 2148 2149 2158 2211 2242 2292 2306 2318 2341 2342 2343 2344\n",
      " 2359 2360 2370 2383 2400 2470 2506 2507 2508 2530 2558 2577 2590 2613 2668\n",
      " 2703 2709 2715 2728 2739 2760 2765 2766 2836 2875 2879 2890 2891 2944 2945\n",
      " 2946 2947 2981 2999 3001 3037 3038 3043 3066 3078 3079 3104 3107 3141 3161\n",
      " 3170 3193 3197 3207 3228 3230 3308 3309 3324 3343 3391 3411 3432 3435 3439\n",
      " 3440 3481 3524 3581 3594 3595 3596 3611 3621 3634 3643 3644 3663 3668 3669\n",
      " 3698 3702 3731 3812 3838 3926 4009 4075 4099 4132 4133 4134 4135 4166 4228\n",
      " 4286 4362 4363 4399]\n",
      "\n",
      "train error shape, 14\n",
      "[0 0 0 0 0 0 0 0 1 0 0 0 0 0]\n",
      "train error pos shape 1\n",
      "train error index\n",
      "[ 3251  5120  5883  7415  7486  7495  8546  8655  8904  9140  9340 10553\n",
      " 11337 11747]\n",
      "s_517764\n",
      "\n",
      "65\n",
      "wifi index [59, 67, 72, 78, 80, 136, 143, 154, 155, 239, 241, 249, 253, 270, 281, 283, 336, 354, 429]\n",
      "feature_importances_ [(0.30301798816980852, 143), (0.19366804476344762, 80), (0.15425928553043985, 241), (0.077656677082787728, 253), (0.068380441499195935, 155), (0.051768365212758058, 59), (0.038785169444182685, 249), (0.038030089906572452, 67), (0.016951650340651835, 78), (0.015629122365337766, 72)]\n",
      "origin 0.999551569507\n",
      "valid error shape, 2\n",
      "[1 1]\n",
      "valid error pos shape 2\n",
      "valid error index\n",
      "[627 629]\n",
      "valid correct index\n",
      "[ 300  376  378  464  626  628  791 1023 1123 1202 1222 1237 1568 1575 1627\n",
      " 1727 1946 2167 2198 2218 2275 2773 2812 3201 3235 3455 3708 3769 3789 3811\n",
      " 3873 3951 4014 4218 4314 4317 4331 4413]\n",
      "\n",
      "train error shape, 0\n",
      "[]\n",
      "train error pos shape 0\n",
      "train error index\n",
      "[]\n",
      "s_517795\n",
      "\n",
      "66\n",
      "wifi index [93, 97, 100, 124, 127, 141, 142, 144, 149, 162, 170, 190, 210, 313, 370, 389, 428, 443]\n",
      "feature_importances_ [(0.21835962501322365, 97), (0.16314671334539513, 100), (0.10015346966218311, 127), (0.099257430417493922, 124), (0.097752532075906964, 93), (0.071795233920257662, 142), (0.051534113547237609, 149), (0.048359370158605391, 162), (0.04475453610215735, 144), (0.029701045854898123, 141)]\n",
      "origin 0.999103139013\n",
      "valid error shape, 4\n",
      "[0 1 1 1]\n",
      "valid error pos shape 3\n",
      "valid error index\n",
      "[ 860 2616 3778 3779]\n",
      "valid correct index\n",
      "[  23   87  105  274  315  318  320  342  454  585  726  730  811  847  855\n",
      "  911  948 1022 1054 1057 1071 1122 1133 1141 1213 1258 1284 1347 1368 1375\n",
      " 1378 1449 1466 1528 1584 1596 1604 1687 1711 1712 1732 1760 1820 1850 1904\n",
      " 1998 2002 2067 2068 2117 2118 2128 2144 2162 2278 2317 2415 2454 2477 2493\n",
      " 2515 2518 2585 2586 2599 2615 2636 2659 2721 2800 2804 2820 2917 2928 2929\n",
      " 2970 2971 2975 3017 3048 3049 3085 3146 3226 3240 3311 3320 3338 3395 3495\n",
      " 3515 3528 3604 3627 3665 3671 3783 3794 3799 3832 3839 3872 3910 3942 4032\n",
      " 4035 4058 4085 4098 4122 4140 4146 4206 4324 4387 4410 4443]\n",
      "\n",
      "train error shape, 1\n",
      "[0]\n",
      "train error pos shape 0\n",
      "train error index\n",
      "[8964]\n",
      "s_522897\n",
      "\n",
      "67\n",
      "wifi index [15, 24, 32, 41, 45, 47, 50, 52, 64, 66, 70, 131, 146, 151, 153, 164, 186, 187, 209, 216, 243, 247, 307, 348]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature_importances_ [(0.26708557170037228, 52), (0.16847913202216749, 164), (0.12019079266038923, 15), (0.11347458517265152, 47), (0.052416211892258563, 209), (0.051628928256347857, 131), (0.042208058127057642, 32), (0.028876635255445829, 45), (0.027916830230687237, 24), (0.025976464751924059, 307)]\n",
      "origin 0.996188340807\n",
      "valid error shape, 17\n",
      "[0 0 0 0 1 0 1 1 1 1 0 0 1 0 0 0 0]\n",
      "valid error pos shape 6\n",
      "valid error index\n",
      "[  41  203  319  584  723  878 1841 1842 2846 3011 3056 3183 3262 3327 3337\n",
      " 3520 4319]\n",
      "valid correct index\n",
      "[ 108  109  201  213  285  344  345  383  423  548  586  725  797  798  804\n",
      "  815  832  872 1069 1273 1274 1300 1327 1330 1363 1374 1412 1496 1527 1668\n",
      " 1669 2053 2085 2337 2458 2460 2536 2537 2684 2685 2810 2859 2860 2908 2912\n",
      " 2948 3270 3389 3390 3483 3538 3540 3606 3607 3647 3654 3655 3739 3740 3763\n",
      " 3815 3851 4180 4245 4252 4329 4339 4345 4357 4446]\n",
      "\n",
      "train error shape, 5\n",
      "[0 0 0 0 0]\n",
      "train error pos shape 0\n",
      "train error index\n",
      "[ 3646  4828  7476 10425 11565]\n",
      "s_525418\n",
      "\n",
      "68\n",
      "wifi index [30, 39, 44, 51, 68, 75, 76, 84, 88, 89, 92, 98, 103, 111, 122, 145, 198, 226, 233, 254, 256, 292, 295, 332, 350, 360]\n",
      "feature_importances_ [(0.17058262438544652, 51), (0.1189855802096732, 226), (0.11174212409248953, 44), (0.095414765169451451, 145), (0.081368177425440974, 98), (0.08070979371055681, 295), (0.071878237190959704, 198), (0.056713720207260121, 122), (0.054805575017499851, 103), (0.022556602827072905, 332)]\n",
      "origin 0.99865470852\n",
      "valid error shape, 6\n",
      "[0 1 1 1 1 1]\n",
      "valid error pos shape 5\n",
      "valid error index\n",
      "[ 471  619 1236 1960 3114 3486]\n",
      "valid correct index\n",
      "[ 333  618  735  840 1353 1446 1561 1715 1797 1823 1934 1970 2180 2528 2531\n",
      " 2606 2692 2749 3061 3187 3273 3312 3470 3720 3773 3876 3941 4211 4288 4401]\n",
      "\n",
      "train error shape, 0\n",
      "[]\n",
      "train error pos shape 0\n",
      "train error index\n",
      "[]\n",
      "s_525779\n",
      "\n",
      "69\n",
      "wifi index [15, 45, 47, 70, 136, 186, 218, 247, 251, 284, 319, 328, 338, 342, 344, 391, 573, 595, 608, 912, 999, 2282]\n",
      "feature_importances_ [(0.27160584933546839, 391), (0.14330841705767269, 342), (0.12400510561599108, 218), (0.089347498847973786, 328), (0.059422122570006126, 186), (0.054364564250369306, 344), (0.048166418567714753, 319), (0.042580589847990437, 15), (0.022065911837711664, 912), (0.020867610300717005, 45)]\n",
      "origin 1.0\n",
      "valid error shape, 0\n",
      "[]\n",
      "valid error pos shape 0\n",
      "valid error index\n",
      "[]\n",
      "valid correct index\n",
      "[2070 3573]\n",
      "\n",
      "train error shape, 0\n",
      "[]\n",
      "train error pos shape 0\n",
      "train error index\n",
      "[]\n",
      "s_528553\n",
      "\n",
      "70\n",
      "wifi index [16, 30, 39, 68, 73, 103, 107, 126, 166, 173, 177, 198, 222, 268, 297, 315, 353, 378]\n",
      "feature_importances_ [(0.27104944374747303, 39), (0.19098626697840182, 68), (0.13339312427215239, 222), (0.12606846791774415, 297), (0.10086155657255717, 177), (0.074681422615771417, 315), (0.039460590630412254, 107), (0.027817191928246249, 30), (0.01081113008226223, 16), (0.0085066669129275681, 166)]\n",
      "origin 0.99932735426\n",
      "valid error shape, 3\n",
      "[0 0 1]\n",
      "valid error pos shape 1\n",
      "valid error index\n",
      "[ 305  306 3290]\n",
      "valid correct index\n",
      "[  95  189  437  443  507  572  582  694  910 1014 1091 1117 1134 1197 1352\n",
      " 1387 1533 2164 2358 2589 2932 3132 3138 3521 3674 3722 3937 4139 4232]\n",
      "\n",
      "train error shape, 0\n",
      "[]\n",
      "train error pos shape 0\n",
      "train error index\n",
      "[]\n",
      "s_540782\n",
      "\n",
      "71\n",
      "wifi index [39, 44, 51, 81, 88, 93, 100, 103, 142, 162, 198, 254, 256, 313, 361, 417, 443, 454, 469]\n",
      "feature_importances_ [(0.28576802637912663, 361), (0.23482243468442923, 313), (0.12171950045675294, 417), (0.097720045522405818, 469), (0.058123905829122489, 93), (0.051432525068964501, 100), (0.046627638896575244, 454), (0.033216366246377825, 198), (0.030348086004947674, 254), (0.0087295577306963391, 142)]\n",
      "origin 1.0\n",
      "valid error shape, 0\n",
      "[]\n",
      "valid error pos shape 0\n",
      "valid error index\n",
      "[]\n",
      "valid correct index\n",
      "[  52  860  992 1295 1411 1655 1745 1962 2072 2173 2396 2722 2807 2819 3033\n",
      " 3162 3465 3493 3494 3865 4267]\n",
      "\n",
      "train error shape, 0\n",
      "[]\n",
      "train error pos shape 0\n",
      "train error index\n",
      "[]\n",
      "s_542833\n",
      "\n",
      "72\n",
      "wifi index [228, 271, 286, 330, 335, 366, 381, 386, 402, 409, 416, 437, 459, 460, 504, 518, 522, 530, 555, 624, 647, 672]\n",
      "feature_importances_ [(0.28128572790384326, 228), (0.18294992874071173, 366), (0.12177022259458478, 381), (0.10300685106421269, 271), (0.060781780190195069, 402), (0.040517005937589425, 386), (0.039360658748718226, 409), (0.031100321319320234, 518), (0.025078864990426896, 504), (0.01601580575891258, 530)]\n",
      "origin 0.999103139013\n",
      "valid error shape, 4\n",
      "[1 1 1 1]\n",
      "valid error pos shape 4\n",
      "valid error index\n",
      "[ 398 1053 1281 1862]\n",
      "valid correct index\n",
      "[ 117  224  325  346  385  403  612  624  853  991 1039 1560 1599 1628 1798\n",
      " 1861 2455 2889 2998 3075 3466 3519 3586 3906 3936 4096 4142 4382]\n",
      "\n",
      "train error shape, 0\n",
      "[]\n",
      "train error pos shape 0\n",
      "train error index\n",
      "[]\n",
      "s_570869\n",
      "\n",
      "73\n",
      "wifi index [5, 11, 16, 18, 22, 25, 30, 35, 38, 43, 46, 69, 73, 82, 87, 515]\n",
      "feature_importances_ [(0.27440013043548228, 25), (0.24046647974531682, 11), (0.15269094711315176, 18), (0.099441199217863013, 43), (0.083071227502561282, 22), (0.049584109973623239, 35), (0.038337697042833907, 38), (0.024270108131810332, 46), (0.01210642561858659, 30), (0.0099678873146189888, 69)]\n",
      "origin 0.998878923767\n",
      "valid error shape, 5\n",
      "[1 1 1 1 1]\n",
      "valid error pos shape 5\n",
      "valid error index\n",
      "[2286 2287 2289 2490 4434]\n",
      "valid correct index\n",
      "[  58  200  237  481  616  677  770  835 1253 1294 1406 1720 1807 1920 1976\n",
      " 1985 2101 2110 2288 2294 2394 2481 2667 2691 2750 2781 2782 2814 2841 2953\n",
      " 3120 3231 3424 3449 3456 3529 3545 3729 3916 3932 4040 4164 4197 4214 4223\n",
      " 4349 4456]\n",
      "\n",
      "train error shape, 1\n",
      "[0]\n",
      "train error pos shape 0\n",
      "train error index\n",
      "[8856]\n",
      "s_572085\n",
      "\n",
      "74\n",
      "wifi index [158, 178, 179, 196, 201, 214, 225, 227, 250, 261, 262, 288, 299, 380, 382, 388, 395, 398, 399, 413, 446, 475, 483, 494]\n",
      "feature_importances_ [(0.22417273176903796, 178), (0.18026748344488996, 196), (0.10500999553925877, 179), (0.094630545423731283, 225), (0.08789614479458667, 214), (0.06582767902746807, 262), (0.059796548991630574, 261), (0.054025915863246066, 250), (0.036239932434423895, 299), (0.017253804059785185, 380)]\n",
      "origin 0.996412556054\n",
      "valid error shape, 16\n",
      "[0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0]\n",
      "valid error pos shape 1\n",
      "valid error index\n",
      "[ 171  174  788  813  886  902 1012 1046 1116 1910 2061 2255 2943 3093 3700\n",
      " 4213]\n",
      "valid correct index\n",
      "[  93  118  178  262  288  449  602  641  663  679  697  718  816  824  833\n",
      "  903  933  934  935  936  949 1044 1096 1218 1426 1480 1487 1666 1675 1780\n",
      " 1878 1903 2011 2042 2126 2141 2188 2204 2205 2215 2238 2293 2308 2339 2839\n",
      " 2868 2872 2873 2957 2962 3113 3145 3219 3317 3322 3323 3376 3429 3544 3585\n",
      " 3608 3738 3801 3878 3895 3911 4018 4279 4296]\n",
      "\n",
      "train error shape, 2\n",
      "[0 0]\n",
      "train error pos shape 0\n",
      "train error index\n",
      "[  65 6892]\n",
      "s_621251\n",
      "\n",
      "75\n",
      "wifi index [16, 30, 39, 46, 51, 68, 73, 84, 111, 122, 126, 145, 166, 177, 199, 203, 314, 353, 371]\n",
      "feature_importances_ [(0.22931723928432066, 84), (0.21260035573337083, 111), (0.13960189636789666, 30), (0.097398603748353296, 126), (0.078617150197482888, 39), (0.063653120545568043, 16), (0.057109926924897234, 51), (0.034879688117900959, 73), (0.029025786772747128, 68), (0.013702692239158853, 145)]\n",
      "origin 0.999103139013\n",
      "valid error shape, 4\n",
      "[0 1 0 1]\n",
      "valid error pos shape 2\n",
      "valid error index\n",
      "[ 467  703 2833 4392]\n",
      "valid correct index\n",
      "[   9   28   61   74  199  208  260  261  307  322  328  335  336  337  338\n",
      "  350  364  368  510  545  675  676  682  764  776  778  789  830  965  977\n",
      " 1061 1062 1077 1082 1109 1204 1205 1225 1367 1538 1539 1606 1728 1729 1848\n",
      " 1881 2008 2089 2104 2105 2106 2187 2192 2284 2304 2319 2409 2468 2582 2726\n",
      " 2744 2745 2756 2757 2827 2849 2952 2977 2978 3031 3032 3110 3139 3140 3147\n",
      " 3148 3341 3344 3537 3550 3635 3768 3840 3863 3864 3900 3938 4044 4092 4101\n",
      " 4102 4174 4365 4432]\n",
      "\n",
      "train error shape, 2\n",
      "[0 0]\n",
      "train error pos shape 0\n",
      "train error index\n",
      "[ 8904 11154]\n",
      "s_627691\n",
      "\n",
      "76\n",
      "wifi index [15, 24, 32, 41, 50, 53, 64, 66, 88, 92, 98, 101, 217, 223, 236, 280, 294, 298, 340, 346, 375]\n",
      "feature_importances_ [(0.19284336655614734, 41), (0.14240176454996289, 53), (0.10864106004754752, 66), (0.067919372764440336, 50), (0.062285773961371277, 340), (0.057209952968099421, 24), (0.056967754616135735, 64), (0.054056879498399747, 298), (0.041096870245954382, 346), (0.037149203441283349, 280)]\n",
      "origin 0.998430493274\n",
      "valid error shape, 7\n",
      "[0 0 1 0 1 0 1]\n",
      "valid error pos shape 3\n",
      "valid error index\n",
      "[ 993 1388 2163 2981 3261 3291 4319]\n",
      "valid correct index\n",
      "[  22   41  119  126  259  672  877  878 1009 1111 1254 1809 1902 1918 2009\n",
      " 2309 2542 2758 2764 2853 2980 3003 3039 3133 3316 3337 3485 3633 3656 3737\n",
      " 3771 4083 4309 4310 4348 4361]\n",
      "\n",
      "train error shape, 3\n",
      "[0 0 0]\n",
      "train error pos shape 0\n",
      "train error index\n",
      "[ 802 3124 5691]\n",
      "s_643862\n",
      "\n",
      "77\n",
      "wifi index [15, 24, 32, 45, 47, 52, 70, 114, 131, 151, 163, 186, 187, 218, 234, 243, 296, 304, 328, 392]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature_importances_ [(0.28036368761853264, 114), (0.18659235406040467, 70), (0.14691152287962106, 234), (0.1139230708785347, 15), (0.065262950931481864, 296), (0.061155742976449927, 304), (0.039362693367261281, 45), (0.031150408470246235, 47), (0.017091025555194209, 392), (0.015715653965501349, 186)]\n",
      "origin 0.99932735426\n",
      "valid error shape, 3\n",
      "[1 0 1]\n",
      "valid error pos shape 2\n",
      "valid error index\n",
      "[  72  411 2064]\n",
      "valid correct index\n",
      "[  43  195  198  270  299  399  453  456  469  664  762  839  912  915  938\n",
      "  957  982 1017 1018 1118 1282 1297 1306 1461 1472 1474 1495 1510 1586 1710\n",
      " 1740 1927 1995 2256 2262 2340 2386 2524 2588 2651 3086 3164 3565 3587 3844\n",
      " 3875 3925 4233 4354]\n",
      "\n",
      "train error shape, 0\n",
      "[]\n",
      "train error pos shape 0\n",
      "train error index\n",
      "[]\n",
      "s_792077\n",
      "\n",
      "78\n",
      "wifi index [20, 23, 31, 34, 55, 57, 62, 63, 90, 104, 113, 134, 137, 240, 260, 287, 423]\n",
      "feature_importances_ [(0.28744142581992238, 90), (0.21232266375572076, 104), (0.1206757961839052, 240), (0.11377023185555625, 137), (0.068907339543413437, 287), (0.05207914763726499, 34), (0.04524805722297727, 20), (0.032090709448928383, 31), (0.03187722974184657, 113), (0.0087145013282513822, 62)]\n",
      "origin 0.99932735426\n",
      "valid error shape, 3\n",
      "[1 0 1]\n",
      "valid error pos shape 2\n",
      "valid error index\n",
      "[1978 2937 4052]\n",
      "valid correct index\n",
      "[  90  381  645  668  759  760 1131 1139 1183 1242 1264 1277 1346 1535 1573\n",
      " 1653 1713 1991 2368 2464 2522 2626 2733 2752 3047 3233 3234 3463 3622 3623\n",
      " 3624 3681 3893 3963 4162 4170]\n",
      "\n",
      "train error shape, 0\n",
      "[]\n",
      "train error pos shape 0\n",
      "train error index\n",
      "[]\n",
      "s_819792\n",
      "\n",
      "79\n",
      "wifi index [110, 129, 157, 158, 159, 176, 179, 183, 185, 191, 200, 201, 205, 215, 227, 259, 263, 281, 288, 316, 320, 372, 461]\n",
      "feature_importances_ [(0.20301495937673011, 110), (0.11539844496758057, 159), (0.10242818929814652, 157), (0.07048785752445165, 129), (0.069580921421432823, 200), (0.067721106982131768, 183), (0.064398962305402069, 185), (0.059875416637497077, 191), (0.05108542293478719, 205), (0.047344025028024157, 215)]\n",
      "origin 0.997757847534\n",
      "valid error shape, 10\n",
      "[1 1 1 1 1 1 1 1 0 1]\n",
      "valid error pos shape 9\n",
      "valid error index\n",
      "[  70 1036 1037 1781 2102 2246 2632 3300 3462 4351]\n",
      "valid correct index\n",
      "[  62   63   66   71  123  188  266  272  293  369  380  418  425  436  497\n",
      "  502  503  512  513  514  515  563  578  667  686  850  851  889  893  894\n",
      "  895  905  941  987  988  996 1010 1080 1179 1191 1211 1212 1214 1365 1421\n",
      " 1422 1746 1782 1794 1843 1863 1864 1954 1984 1997 2019 2052 2103 2201 2302\n",
      " 2303 2441 2448 2459 2495 2512 2578 2614 2625 2661 2730 2797 3004 3042 3046\n",
      " 3058 3250 3263 3293 3332 3361 3362 3382 3427 3468 3475 3479 3578 3579 3580\n",
      " 3662 3666 3667 3691 3759 3766 3784 3803 3888 3956 4030 4031 4161 4185 4207\n",
      " 4261 4304 4306 4347 4374 4418]\n",
      "\n",
      "train error shape, 9\n",
      "[0 0 0 0 0 0 0 0 1]\n",
      "train error pos shape 1\n",
      "train error index\n",
      "[  808   834  5065  5659  6531  8636 10436 11391 11639]\n",
      "s_856830\n",
      "\n",
      "{0: 0.9991031390134529, 1: 0.9995515695067264, 2: 1.0, 3: 1.0, 4: 1.0, 5: 0.9988789237668162, 6: 0.9966367713004485, 7: 0.9988789237668162, 8: 0.9988789237668162, 9: 1.0, 10: 1.0, 11: 0.9997757847533633, 12: 1.0, 13: 1.0, 14: 0.9952914798206278, 15: 1.0, 16: 1.0, 17: 0.9993273542600897, 18: 1.0, 19: 1.0, 20: 0.9991031390134529, 21: 1.0, 22: 0.9993273542600897, 23: 1.0, 24: 0.9997757847533633, 25: 1.0, 26: 0.9926008968609865, 27: 0.9995515695067264, 28: 0.9993273542600897, 29: 0.9991031390134529, 30: 1.0, 31: 1.0, 32: 0.9986547085201793, 33: 0.9950672645739911, 34: 1.0, 35: 0.9993273542600897, 36: 0, 37: 1.0, 38: 1.0, 39: 0.9997757847533633, 40: 0.9993273542600897, 41: 0.9860986547085202, 42: 0.9977578475336323, 43: 0.9991031390134529, 44: 0.9995515695067264, 45: 0.997085201793722, 46: 1.0, 47: 1.0, 48: 0.9993273542600897, 49: 0.9975336322869955, 50: 0.9995515695067264, 51: 0.9955156950672646, 52: 0.9919282511210762, 53: 0.9961883408071749, 54: 0.9995515695067264, 55: 0.9827354260089686, 56: 0.9993273542600897, 57: 0.9982062780269059, 58: 0.9984304932735426, 59: 0.9993273542600897, 60: 0.9995515695067264, 61: 0.9948430493273542, 62: 0.9995515695067264, 63: 0.9993273542600897, 64: 0.9946188340807175, 65: 0.9995515695067264, 66: 0.9991031390134529, 67: 0.9961883408071749, 68: 0.9986547085201793, 69: 1.0, 70: 0.9993273542600897, 71: 1.0, 72: 0.9991031390134529, 73: 0.9988789237668162, 74: 0.9964125560538116, 75: 0.9991031390134529, 76: 0.9984304932735426, 77: 0.9993273542600897, 78: 0.9993273542600897, 79: 0.9977578475336323}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.944170403587444"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = {}\n",
    "probas = []\n",
    "all_choose = {}\n",
    "all_rfs = {}\n",
    "for i in range(_train_b_y.shape[1]):\n",
    "#     i=52\n",
    "    print i\n",
    "    gt = train.iloc[train_y==lb.classes_[i]]\n",
    "    ngt = train.iloc[train_y!=lb.classes_[i]]\n",
    "    if len(gt) != 0:\n",
    "        \n",
    "        sorted_wifi = get_sorted_wifi([gt])\n",
    "        _indexs = get_indexs(df,sorted_wifi,0.1)\n",
    "       \n",
    "        cv = 0\n",
    "        n_sorted_wifi = get_sorted_wifi([ngt])\n",
    "        _nindexs = get_indexs2(df,n_sorted_wifi,50)\n",
    "#         _indexs = list(set(_indexs).union(set(_nindexs)))\n",
    "        prf = get_model(cv = cv)\n",
    "        otxs= []\n",
    "        ovxs = []\n",
    "#         _indexs = _indexs[:1]\n",
    "        \n",
    "#         _indexs = [6,27]\n",
    "#         _indexs=[]\n",
    "        print \"wifi index\",_indexs\n",
    "        modify_size =  (np.asarray(_indexs) < 0).sum()\n",
    "        _tx = train_wifi_all_x[:,_indexs]\n",
    "        _vx = valid_wifi_all_x[:,_indexs]\n",
    "        \n",
    "#         _tx,_vx = exp_wifi(_tx,_vx)\n",
    "        \n",
    "#         __tx,__vx = modify_wifi(_tx,_vx,train,valid,modify_size)\n",
    "#         _tx = np.concatenate([_tx,__tx],axis=1)\n",
    "#         _vx = np.concatenate([_vx,__vx],axis=1)\n",
    "        \n",
    "        \n",
    "#         from sklearn.preprocessing import PolynomialFeatures\n",
    "#         _tx = PolynomialFeatures(2).fit_transform(_tx)\n",
    "#         _vx = PolynomialFeatures(2).fit_transform(_vx)\n",
    "        \n",
    "        \n",
    "        \n",
    "#         _indexs.append(\"w\")\n",
    "#         _indexs.append(\"h\")\n",
    "        \n",
    "        if cv is not None and cv != 0:\n",
    "            _tx,_ty = expansion(_tx,_train_b_y[:,i],cv)\n",
    "        else:\n",
    "            _ty = _train_b_y[:,i]\n",
    "            \n",
    "        prf.fit(_tx,_ty)\n",
    "        \n",
    "#         print \"train index\", train_index[(1 == _train_b_y[:,i])]\n",
    "        \n",
    "        if hasattr(prf,\"feature_importances_\"):\n",
    "            fi = zip(prf.feature_importances_, _indexs)\n",
    "            fi = sorted(fi,key=lambda x:-x[0])\n",
    "            print \"feature_importances_\", fi[:10]\n",
    "            choose = zip(prf.feature_importances_, range(_tx.shape[1]))\n",
    "            choose = sorted(choose,key=lambda x:-x[0])\n",
    "            c = []\n",
    "            c2 = []\n",
    "            # 选择wifi\n",
    "            for _c,_f in zip(choose,fi):\n",
    "                if _c[0] >= 0.05 and len(c) < 6:\n",
    "                    c.append(_c[1])\n",
    "                    c2.append(_f[1])\n",
    "                else:\n",
    "                    break\n",
    "            prf = RandomForestClassifier(n_jobs=-1,n_estimators=120,class_weight=\"balanced\")\n",
    "            _tx = _tx[:,c]\n",
    "            _vx = _vx[:,c]\n",
    "            all_choose[i]=c2\n",
    "        \n",
    "        _tx = np.concatenate([_tx,train_lonlats[:,[0,1]]],axis=1)\n",
    "        _vx = np.concatenate([_vx,valid_lonlats[:,[0,1]]],axis=1)\n",
    "        \n",
    "        prf.fit(_tx,_ty)\n",
    "        all_rfs[i] = prf\n",
    "        p = prf.predict(_vx)\n",
    "        p2 = prf.predict(_tx)\n",
    "        proba = prf.predict_proba(_vx)\n",
    "        probas.append(proba[:,1])\n",
    "        _acc = acc(p,_valid_b_y[:,i])\n",
    "        print \"origin\", _acc\n",
    "        r[i] = _acc\n",
    "        print \"valid error shape,\", (p != _valid_b_y[:,i]).sum()\n",
    "        print _valid_b_y[:,i][(p != _valid_b_y[:,i])]\n",
    "        print \"valid error pos shape\", (_valid_b_y[:,i][(p != _valid_b_y[:,i])]==1).sum()\n",
    "        print \"valid error index\"\n",
    "        print valid_index[(p != _valid_b_y[:,i])]\n",
    "        print \"valid correct index\"\n",
    "        print valid_index[(p == _valid_b_y[:,i]) & (_valid_b_y[:,i]==1)]\n",
    "        print \n",
    "        \n",
    "        print \"train error shape,\", (p2 != _train_b_y[:,i]).sum()\n",
    "        print _train_b_y[:,i][(p2 != _train_b_y[:,i])]\n",
    "        print \"train error pos shape\", (_train_b_y[:,i][(p2 != _train_b_y[:,i])]==1).sum()\n",
    "        print \"train error index\"\n",
    "        print train_index[(p2 != _train_b_y[:,i])]\n",
    "        print le.classes_[i]\n",
    "        print\n",
    "    else:\n",
    "        probas.append(np.zeros((valid_y.shape[0],)))\n",
    "        r[i] = 0\n",
    "#     break\n",
    "print r\n",
    "ovr_pba = np.vstack(probas).T\n",
    "last_p = lb.classes_.take(np.argmax(ovr_pba,axis=1))\n",
    "acc(last_p, valid_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9717488789237668"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 候选10 个, 最小概率0.02\n",
    "candidate2 = get_top_k(ovr_pba, 10, 0.01)\n",
    "acc_top_k(candidate2, valid_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[\"minute\"] = train.dt.dt.minute\n",
    "train[\"hour_minute\"] = train.hour * 60 + train.minute\n",
    "valid[\"minute\"] = valid.dt.dt.minute\n",
    "valid[\"hour_minute\"] = valid.hour * 60 + valid.minute\n",
    "train[\"dayofyear\"] = train.dt.dt.dayofyear\n",
    "valid[\"dayofyear\"] = valid.dt.dt.dayofyear\n",
    "valid[\"sample_index\"] = range(valid.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "tud = train.groupby([\"user_id\", \"dayofyear\"])[\"shop_id\"].count().reset_index()\n",
    "tud = tud[tud.shop_id >=2]\n",
    "vud = valid.groupby([\"user_id\", \"dayofyear\"])[\"longitude\"].count().reset_index()\n",
    "vud = vud[vud.longitude >= 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def corr_analyse_in_time(x, train, shops_cor_one_day,time=30): # 一天交易多次， 去某个商店在time分钟内可能去哪些商店\n",
    "    shopids = train[(train.user_id == x[0]) & (train.dayofyear == x[1])][[\"shop_id\",\"hour_minute\"]].values\n",
    "    for _i,_s in enumerate(shopids):\n",
    "        for _j,_s2 in enumerate(shopids):\n",
    "            if _i == _j:\n",
    "                continue\n",
    "            else:\n",
    "                if abs(_s[1] - _s2[1]) > time:\n",
    "                    continue\n",
    "                if _s[0] not in shops_cor_one_day:\n",
    "                    shops_cor_one_day[_s[0]] = {}\n",
    "                if _s2[0] not in shops_cor_one_day[_s[0]]:\n",
    "                    shops_cor_one_day[_s[0]][_s2[0]] = 0\n",
    "                shops_cor_one_day[_s[0]][_s2[0]] += 1\n",
    "    \n",
    "\n",
    "\n",
    "def copy_candidate(old_candidate):\n",
    "    new_candidate = []\n",
    "    for _nda in old_candidate:\n",
    "        new_candidate.append(_nda.copy())\n",
    "    return new_candidate\n",
    "\n",
    "# 找出30分钟内交易多次的根据训练集剔除不可能的候选集合\n",
    "def remove_candidate(x,valid,shops_cor_time,modify_candidate, le, time = 30, stay = 3, continue_search=True):\n",
    "    times = valid[(valid.user_id == x[0]) & (valid.dayofyear == x[1])][[\"hour_minute\",\"sample_index\"]].values\n",
    "    for _i,_s in enumerate(times):\n",
    "        for _j,_s2 in enumerate(times):\n",
    "            if _i == _j:\n",
    "                continue\n",
    "            else:\n",
    "                if abs(_s[0] - _s2[0]) > time:\n",
    "                    continue\n",
    "                if len(modify_candidate[_s[1]]) == 1:\n",
    "                    if len(modify_candidate[_s2[1]]) == 1 or len(modify_candidate[_s2[1]]) == 2 or len(modify_candidate[_s2[1]]) == 3:\n",
    "                        continue\n",
    "                    _s_c = modify_candidate[_s[1]]\n",
    "                    _ss_c = modify_candidate[_s2[1]]\n",
    "#                     print \"==========================\"\n",
    "#                     print _s\n",
    "#                     print _s2\n",
    "#                     print _s_c\n",
    "#                     print _ss_c\n",
    "#                     print \n",
    "                    sid = le.inverse_transform(_s_c)[0]\n",
    "#                     print sid\n",
    "                    set1 = set(shops_cor_time[sid].keys())\n",
    "                    set2 = set(le.inverse_transform(_ss_c))\n",
    "                    sort =  zip(shops_cor_time[sid].keys(),shops_cor_time[sid].values())\n",
    "                    sort =  sorted(sort,key=lambda x: -x[1])\n",
    "                    \n",
    "                    last_set = set()\n",
    "                    _last_n = -1\n",
    "                    \n",
    "                    if continue_search:\n",
    "                        search_sid = []\n",
    "                        searched_sid = set()\n",
    "                        searched_sid.add(sid)\n",
    "                    \n",
    "                    for _sn,_n in sort:\n",
    "                        if _n == _last_n:# 和上一个数量相同的话\n",
    "                            if _sn in set2:\n",
    "                                last_set.add(_sn)\n",
    "                                search_sid.append(_sn)\n",
    "                                continue\n",
    "                        # 和上一个数量不同了\n",
    "                        if len(last_set) >= stay:\n",
    "                            break\n",
    "                        if _sn in set2:\n",
    "                            last_set.add(_sn)\n",
    "                            search_sid.append(_sn)\n",
    "                            _last_n = _n\n",
    "                            \n",
    "                    # 从新加入的为基准继续搜索寻找\n",
    "                    while continue_search and len(last_set) < stay and len(search_sid) != 0:\n",
    "                        sid = search_sid.pop(0)\n",
    "                        if sid in searched_sid:\n",
    "                            continue\n",
    "                        searched_sid.add(sid)\n",
    "                        set1 = set(shops_cor_time[sid].keys())\n",
    "                        sort =  zip(shops_cor_time[sid].keys(),shops_cor_time[sid].values())\n",
    "                        sort =  sorted(sort,key=lambda x: -x[1])\n",
    "\n",
    "                        _last_n = -1\n",
    "                        for _sn,_n in sort:\n",
    "                            if _n == _last_n:# 和上一个数量相同的话\n",
    "                                if _sn in set2 and _sn not in last_set:\n",
    "                                    last_set.add(_sn)\n",
    "                                    search_sid.append(_sn)\n",
    "                                    continue\n",
    "                                    \n",
    "                            # 和上一个数量不同了\n",
    "                            if len(last_set) >= stay:\n",
    "                                break\n",
    "                                \n",
    "                            if _sn in set2 and _sn not in last_set:\n",
    "                                last_set.add(_sn)\n",
    "                                search_sid.append(_sn)\n",
    "                                _last_n = _n\n",
    "\n",
    "                    \n",
    "#                     print sort\n",
    "#                     print set2    \n",
    "#                     print last_set\n",
    "#                     print \"================================\"\n",
    "                    if len(last_set) != 0:\n",
    "                        modify_candidate[_s2[1]] = le.transform(list(last_set))\n",
    "            \n",
    "def statistic_candidate(candidate, valid_y = None):\n",
    "    candi_num = [len(_can) for _can in candidate]\n",
    "    can_num = np.bincount(candi_num)\n",
    "    correct = np.zeros((len(can_num),))\n",
    "    if valid_y is not None:\n",
    "        for _can, _true in zip(candidate,valid_y):\n",
    "            if _true in _can:\n",
    "                correct[len(_can)] += 1\n",
    "        return can_num, correct.astype(int)\n",
    "    else:\n",
    "        return can_num,None  \n",
    "    \n",
    "def print_statistic_candidate(candidate, valid_y = None):\n",
    "    can_num,cor_num = statistic_candidate(candidate, valid_y)\n",
    "    print \"all num\",can_num\n",
    "    if valid_y is not None:\n",
    "        print \"cor num\",cor_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_candidate(candidate, valid_y, two_dict, many_dict, predict_candi_length=-1):\n",
    "    # 在all_rf产生的候选集合中用那一部分商场训练预测\n",
    "    last_predict_by_candidates = []\n",
    "    single_predict = []\n",
    "    two_predict = []\n",
    "    many_predict = []\n",
    "    single_real = []\n",
    "    two_real = []\n",
    "    many_real = []\n",
    "    one_error_sample_index = []\n",
    "    two_error_sample_index = []\n",
    "    many_error_sample_index = []\n",
    "    one_errors = []\n",
    "    two_errors = []\n",
    "    many_errors = []\n",
    "    for _sample_index,_can in enumerate(candidate):\n",
    "        if _sample_index % 200 ==0:\n",
    "            print _sample_index\n",
    "        if len(_can) == 1 and (predict_candi_length == 1 or predict_candi_length == -1):\n",
    "            last_predict_by_candidates.append(_can[0])\n",
    "            single_predict.append(_can[0])\n",
    "            single_real.append(valid_y[_sample_index])\n",
    "            if _can[0] != [valid_y[_sample_index]]:\n",
    "                one_errors.append(_can[0])\n",
    "                one_error_sample_index.append(_sample_index)\n",
    "        else:\n",
    "            if len(_can) == 2 and (predict_candi_length == 2 or predict_candi_length == -1):\n",
    "                s1 = _can[0]\n",
    "                s2 = _can[1]\n",
    "                _indexs = list(set(all_choose[s1]).union(set(all_choose[s2])))\n",
    "                _rf = two_dict[\",\".join(sorted(list(_can.astype(str))))]\n",
    "                pvalid_x = valid_wifi_all_x[[_sample_index]][:,_indexs] \n",
    "                pvalid_x = np.concatenate([pvalid_x, valid_lonlats[[_sample_index]]],axis=1)\n",
    "                _p1 = _rf.predict(pvalid_x)[0]\n",
    "                last_predict_by_candidates.append(_p1)\n",
    "                two_predict.append(_p1)\n",
    "                two_real.append(valid_y[_sample_index])\n",
    "                if _p1 != valid_y[_sample_index]:\n",
    "                    two_errors.append(\",\".join(sorted(list(_can.astype(str)))))\n",
    "                    two_error_sample_index.append(_sample_index)\n",
    "            elif len(_can) > 2 and (predict_candi_length > 2 or predict_candi_length == -1):\n",
    "                _s = \",\".join(sorted(list(_can.astype(str))))\n",
    "                _ss = _s.split(\",\")\n",
    "                s1 = int(_ss[0])\n",
    "                _indexs = set(all_choose[s1])\n",
    "                for _k in range(1,len(_ss)):\n",
    "                    s2 = int(_ss[_k])\n",
    "                    _indexs = _indexs.union(set(all_choose[s2]))\n",
    "                _indexs = list(_indexs) \n",
    "                _rf = many_dict[_s]\n",
    "                pvalid_x = valid_wifi_all_x[[_sample_index]][:,_indexs] \n",
    "                pvalid_x = np.concatenate([pvalid_x, valid_lonlats[[_sample_index]]],axis=1)\n",
    "                _p1 = _rf.predict(pvalid_x)[0]\n",
    "                last_predict_by_candidates.append(_p1)\n",
    "                many_predict.append(_p1)\n",
    "                many_real.append(valid_y[_sample_index])\n",
    "                if _p1 != valid_y[_sample_index]:\n",
    "                    many_errors.append(\",\".join(sorted(list(_can.astype(str)))))\n",
    "                    many_error_sample_index.append(_sample_index)\n",
    "    return last_predict_by_candidates, (single_predict,single_real,one_error_sample_index,one_errors),(two_predict,two_real,two_error_sample_index,two_errors),(many_predict,many_real,many_error_sample_index,many_errors)         \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc(np.asarray(last_predict_by_candidates),valid_y)\n",
    "print \"single\", (np.asarray(single_predict) == np.asarray(single_real)).sum(), len(single_predict)\n",
    "print \"two\", (np.asarray(two_predict) == np.asarray(two_real)).sum(), len(two_predict)\n",
    "print \"many\", (np.asarray(many_predict) == np.asarray(many_real)).sum(), len(many_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all num [   0 2265 1097  382  188  155  104   60   57   30  122]\n",
      "cor num [   0 2261 1097  381  188  154  100   53   49   23   85]\n"
     ]
    }
   ],
   "source": [
    "print_statistic_candidate(candidate,valid_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [],
   "source": [
    "def candidate_set(candidate):\n",
    "    # 在all_rf产生的候选集合中用那一部分商场训练预测\n",
    "    many = 0\n",
    "    single = 0\n",
    "    two = 0\n",
    "    two_set = set()\n",
    "    many_set = set()\n",
    "    for _sample_index,_can in enumerate(candidate):\n",
    "        if len(_can) ==1:\n",
    "            single += 1\n",
    "        else:\n",
    "            if len(_can) == 2:\n",
    "                two_set.add(\",\".join(sorted(list(_can.astype(str)))))\n",
    "                two +=1\n",
    "            else:\n",
    "                many_set.add(\",\".join(sorted(list(_can.astype(str)))))\n",
    "                many +=1\n",
    "    print \"many size\",many\n",
    "    print \"two size\",two\n",
    "    print \"single size\",single\n",
    "    print \"two set length\",len(two_set)\n",
    "    print \"many set length\",len(many_set)\n",
    "    return two_set,many_set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "recall 0.984529147982\n",
      "many size 1098\n",
      "two size 1097\n",
      "single size 2265\n",
      "two set length 99\n",
      "many set length 617\n",
      "0/99\n",
      "20/99\n",
      "40/99\n",
      "60/99\n",
      "80/99\n",
      "0\n",
      "200\n",
      "400\n",
      "600\n",
      "800\n",
      "1000\n",
      "1200\n",
      "1400\n",
      "1600\n",
      "1800\n",
      "2000\n",
      "2200\n",
      "2400\n",
      "2600\n",
      "2800\n",
      "3000\n",
      "3200\n",
      "3400\n",
      "3600\n",
      "3800\n",
      "4000\n",
      "4200\n",
      "4400\n",
      "two 1027 1097\n"
     ]
    }
   ],
   "source": [
    "# 候选10 个, 最小概率0.02\n",
    "candidate = get_top_k(rf_all_pba, 10, 0.02)\n",
    "candidate = [rf_all.classes_.take(_can) for _can in candidate]\n",
    "print \"recall\", acc_top_k(candidate, valid_y)\n",
    "two_set,many_set = candidate_set(candidate)\n",
    "two_dict = {}\n",
    "for _ind,_s in enumerate(two_set):\n",
    "    if _ind % 20 == 0:\n",
    "        print \"{}/{}\".format(_ind,len(two_set))\n",
    "    _ss = _s.split(\",\")\n",
    "    s1 = int(_ss[0])\n",
    "    s2 = int(_ss[1])\n",
    "    _indexs = list(set(all_choose[s1]).union(set(all_choose[s2]))) \n",
    "    _rf = RandomForestClassifier(n_jobs=-1,n_estimators=188)\n",
    "    _train_bool_index = (_train_b_y[:,s1] == 1) | (_train_b_y[:,s2]==1) \n",
    "    ptrain_x = train_wifi_all_x[_train_bool_index][:,_indexs] \n",
    "    ptrain_y = train_y[_train_bool_index] \n",
    "    ptrain_x = np.concatenate([ptrain_x, train_lonlats[_train_bool_index]],axis=1)\n",
    "    _rf.fit(ptrain_x,ptrain_y)\n",
    "    two_dict[_s] = _rf\n",
    "last_predict,_,twos,_ = predict_candidate(candidate,valid_y,two_dict,None,2)\n",
    "two_predict,two_real,two_error_sample_index,two_errors = twos\n",
    "print \"two\", (np.asarray(two_predict) == np.asarray(two_real)).sum(), len(two_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 555,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'13,27': 1,\n",
       "         '14,20': 1,\n",
       "         '14,46': 1,\n",
       "         '14,55': 1,\n",
       "         '22,57': 1,\n",
       "         '26,41': 24,\n",
       "         '28,53': 1,\n",
       "         '39,7': 1,\n",
       "         '44,56': 2,\n",
       "         '51,64': 3,\n",
       "         '52,55': 30,\n",
       "         '55,70': 1,\n",
       "         '58,68': 3})"
      ]
     },
     "execution_count": 555,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(two_errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9845291479820628"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modify_candidate_u = copy_candidate(candidate)\n",
    "def remove_candidate_by_user(candidate,valid, train, deresase_to, le):\n",
    "    u = train.groupby(\"user_id\")[\"shop_id\"].count().reset_index().sort_values(\"shop_id\",ascending=False)\n",
    "    u.rename(columns={\"shop_id\":\"count\"},inplace=True)\n",
    "    uids = u[u[\"count\"] >= (train.dayofyear.unique().shape[0] / 2)].user_id.values\n",
    "    for _ind,_can in enumerate(candidate):\n",
    "        if len(_can) > deresase_to:\n",
    "            _uid = valid.iloc[_ind].user_id\n",
    "            if _uid in uids:\n",
    "                new_set = []\n",
    "#                 print le.transform(valid.iloc[[_ind]][\"shop_id\"].values)\n",
    "#                 print _can\n",
    "                _max = np.bincount(le.transform(train[train.user_id == _uid].shop_id.values))\n",
    "                \n",
    "                while len(new_set) < deresase_to:\n",
    "                    _m = np.argmax(_max)\n",
    "                    if _max[_m] != 0:\n",
    "                        if _m in _can:\n",
    "                            new_set.append(_m)\n",
    "                    else:\n",
    "                        break\n",
    "                    _max[_m] = 0\n",
    "                \n",
    "                if len(new_set) == deresase_to:\n",
    "#                     print new_set\n",
    "                    candidate[_ind] = np.asarray(new_set)\n",
    "#                 print \"==================\"\n",
    "                \n",
    "remove_candidate_by_user(modify_candidate_u, valid,train,deresase_to=3, le=le)\n",
    "acc_top_k(modify_candidate_u, valid_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "shops_cor_60time = {}\n",
    "shops_cor_1000time = {}\n",
    "shops_cor_30time = {}\n",
    "_ = map(lambda x:corr_analyse_in_time(x,train, shops_cor_30time,time=30),tud[[\"user_id\", \"dayofyear\"]].values)\n",
    "_ = map(lambda x:corr_analyse_in_time(x,train, shops_cor_60time,time=60),tud[[\"user_id\", \"dayofyear\"]].values)\n",
    "_ = map(lambda x:corr_analyse_in_time(x,train, shops_cor_1000time,time=1000),tud[[\"user_id\", \"dayofyear\"]].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9836322869955157"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modify_candidate = copy_candidate(modify_candidate_u)        \n",
    "#一天内\n",
    "_ = map(lambda x:remove_candidate(x,valid,shops_cor_1000time,modify_candidate,le, stay=6, time = 1000),vud[[\"user_id\",\"dayofyear\"]].values)\n",
    "acc_top_k(modify_candidate, valid_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9834080717488789"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modify_candidate2 = copy_candidate(modify_candidate)        \n",
    "#60分钟内\n",
    "_ = map(lambda x:remove_candidate(x,valid,shops_cor_60time,modify_candidate2,le, stay=5, time = 60),vud[[\"user_id\",\"dayofyear\"]].values)\n",
    "acc_top_k(modify_candidate2, valid_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9827354260089686"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modify_candidate3 = copy_candidate(modify_candidate2)        \n",
    "#30分钟内\n",
    "_ = map(lambda x:remove_candidate(x,valid,shops_cor_30time,modify_candidate3,le, stay=3, time = 30),vud[[\"user_id\",\"dayofyear\"]].values)\n",
    "acc_top_k(modify_candidate3, valid_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 按hour 对最可能去的商店进行排序\n",
    "ht = train.groupby([\"shop_id\",\"hour\"])[\"longitude\"].count().reset_index().sort_values([\"hour\",\"longitude\"],ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9800448430493274"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modify_candidate4 = copy_candidate(modify_candidate3)\n",
    "def remove_candidate_by_hour(candidate,valid,ht, deresase_to, le):\n",
    "    for _ind,_can in enumerate(candidate):\n",
    "        if len(_can) > deresase_to:\n",
    "            _can = le.inverse_transform(_can)\n",
    "            _h = valid.iloc[[_ind]][\"hour\"].values[0]\n",
    "            _temp = ht[ht.hour==_h][\"shop_id\"].values\n",
    "            new_set = []\n",
    "            for _sn in _temp:\n",
    "                if _sn in _can:\n",
    "                    new_set.append(_sn)\n",
    "                if len(new_set) >= deresase_to:\n",
    "                    break\n",
    "            if len(new_set) >=deresase_to:\n",
    "                candidate[_ind] = le.transform(new_set)\n",
    "                \n",
    "remove_candidate_by_hour(modify_candidate4, valid,ht,deresase_to=7, le=le)\n",
    "acc_top_k(modify_candidate4, valid_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   0 2265 1097  382  188  155  104   60   57   30  122]\n",
      "[   0 2267 1098  391  186  151  129   53   49   26  110]\n",
      "[   0 2272 1100  392  182  171  106   52   49   26  110]\n",
      "[   0 2273 1100  408  178  158  106   52   49   26  110]\n",
      "[   0 2273 1100  408  178  158  106  223    4    4    6]\n"
     ]
    }
   ],
   "source": [
    "print statistic_candidate(candidate)\n",
    "print statistic_candidate(modify_candidate)\n",
    "print statistic_candidate(modify_candidate2)\n",
    "print statistic_candidate(modify_candidate3)\n",
    "print statistic_candidate(modify_candidate4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9452914798206278"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#在 all_rf 产生的候选集合中用ovr预测\n",
    "last_predict_by_candidates = []\n",
    "for _sample_index,_can in enumerate(candidate):\n",
    "    if len(_can) ==1:\n",
    "        last_predict_by_candidates.append(_can[0])\n",
    "    else:\n",
    "        _ps = []\n",
    "        for _c in _can:\n",
    "            _ps.append(ovr_pba[_sample_index,_c])\n",
    "        _p = _can.take(np.argmax(_ps))\n",
    "        last_predict_by_candidates.append(_p)\n",
    "acc(np.asarray(last_predict_by_candidates),valid_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9430493273542601"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#在 all_rf 产生的候选集合中用ovr预测\n",
    "last_predict_by_candidates = []\n",
    "for _sample_index,_can in enumerate(modify_candidate4):\n",
    "    if len(_can) ==1:\n",
    "        last_predict_by_candidates.append(_can[0])\n",
    "    else:\n",
    "        _ps = []\n",
    "        for _c in _can:\n",
    "            _ps.append(ovr_pba[_sample_index,_c])\n",
    "        _p = _can.take(np.argmax(_ps))\n",
    "        last_predict_by_candidates.append(_p)\n",
    "acc(np.asarray(last_predict_by_candidates),valid_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/600\n",
      "20/600\n",
      "40/600\n",
      "60/600\n",
      "80/600\n",
      "100/600\n",
      "120/600\n",
      "140/600\n",
      "160/600\n",
      "180/600\n",
      "200/600\n",
      "220/600\n",
      "240/600\n",
      "260/600\n",
      "280/600\n",
      "300/600\n",
      "320/600\n",
      "340/600\n",
      "360/600\n",
      "380/600\n",
      "400/600\n",
      "420/600\n",
      "440/600\n",
      "460/600\n",
      "480/600\n",
      "500/600\n",
      "520/600\n",
      "540/600\n",
      "560/600\n",
      "580/600\n"
     ]
    }
   ],
   "source": [
    "many_dict = {}\n",
    "for _ind,_s in enumerate(many_set):\n",
    "    if _ind % 20 == 0:\n",
    "        print \"{}/{}\".format(_ind,len(many_set))\n",
    "    _ss = _s.split(\",\")\n",
    "    s1 = int(_ss[0])\n",
    "    _indexs = set(all_choose[s1])\n",
    "    _train_bool_index =  (_train_b_y[:,s1] == 1)\n",
    "    for _k in range(1,len(_ss)):\n",
    "        s2 = int(_ss[_k])\n",
    "        _indexs = _indexs.union(set(all_choose[s2]))\n",
    "        _train_bool_index =  _train_bool_index | (_train_b_y[:,s2] == 1)\n",
    "   \n",
    "    _indexs = list(_indexs) \n",
    "    _rf = RandomForestClassifier(n_jobs=-1,n_estimators=288,class_weight=\"balanced\")\n",
    "    ptrain_x = train_wifi_all_x[_train_bool_index][:,_indexs] \n",
    "    ptrain_y = train_y[_train_bool_index] \n",
    "    ptrain_x = np.concatenate([ptrain_x, train_lonlats[_train_bool_index]],axis=1)\n",
    "    _rf.fit(ptrain_x,ptrain_y)\n",
    "    many_dict[_s] = _rf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0,1\n",
      "0,2\n",
      "0,3\n",
      "0,4\n",
      "0,5\n",
      "0,6\n",
      "0,7\n",
      "0,8\n",
      "0,9\n",
      "0,10\n",
      "0,11\n",
      "0,12\n",
      "0,13\n",
      "0,14\n",
      "0,15\n",
      "0,16\n",
      "0,17\n",
      "0,18\n",
      "0,19\n",
      "0,20\n",
      "0,21\n",
      "0,22\n",
      "0,23\n",
      "0,24\n",
      "0,25\n",
      "0,26\n",
      "0,27\n",
      "0,28\n",
      "0,29\n",
      "0,30\n",
      "0,31\n",
      "0,32\n",
      "0,33\n",
      "0,34\n",
      "0,35\n",
      "0,37\n",
      "0,38\n",
      "0,39\n",
      "0,40\n",
      "0,41\n",
      "0,42\n",
      "0,43\n",
      "0,44\n",
      "0,45\n",
      "0,46\n",
      "0,47\n",
      "0,48\n",
      "0,49\n",
      "0,50\n",
      "0,51\n",
      "0,52\n",
      "0,53\n",
      "0,54\n",
      "0,55\n",
      "0,56\n",
      "0,57\n",
      "0,58\n",
      "0,59\n",
      "0,60\n",
      "0,61\n",
      "0,62\n",
      "0,63\n",
      "0,64\n",
      "0,65\n",
      "0,66\n",
      "0,67\n",
      "0,68\n",
      "0,69\n",
      "0,70\n",
      "0,71\n",
      "0,72\n",
      "0,73\n",
      "0,74\n",
      "0,75\n",
      "0,76\n",
      "0,77\n",
      "0,78\n",
      "0,79\n",
      "1,2\n",
      "1,3\n",
      "1,4\n",
      "1,5\n",
      "1,6\n",
      "1,7\n",
      "1,8\n",
      "1,9\n",
      "1,10\n",
      "1,11\n",
      "1,12\n",
      "1,13\n",
      "1,14\n",
      "1,15\n",
      "1,16\n",
      "1,17\n",
      "1,18\n",
      "1,19\n",
      "1,20\n",
      "1,21\n",
      "1,22\n",
      "1,23\n",
      "1,24\n",
      "1,25\n",
      "1,26\n",
      "1,27\n",
      "1,28\n",
      "1,29\n",
      "1,30\n",
      "1,31\n",
      "1,32\n",
      "1,33\n",
      "1,34\n",
      "1,35\n",
      "1,37\n",
      "1,38\n",
      "1,39\n",
      "1,40\n",
      "1,41\n",
      "1,42\n",
      "1,43\n",
      "1,44\n",
      "1,45\n",
      "1,46\n",
      "1,47\n",
      "1,48\n",
      "1,49\n",
      "1,50\n",
      "1,51\n",
      "1,52\n",
      "1,53\n",
      "1,54\n",
      "1,55\n",
      "1,56\n",
      "1,57\n",
      "1,58\n",
      "1,59\n",
      "1,60\n",
      "1,61\n",
      "1,62\n",
      "1,63\n",
      "1,64\n",
      "1,65\n",
      "1,66\n",
      "1,67\n",
      "1,68\n",
      "1,69\n",
      "1,70\n",
      "1,71\n",
      "1,72\n",
      "1,73\n",
      "1,74\n",
      "1,75\n",
      "1,76\n",
      "1,77\n",
      "1,78\n",
      "1,79\n",
      "2,3\n",
      "2,4\n",
      "2,5\n",
      "2,6\n",
      "2,7\n",
      "2,8\n",
      "2,9\n",
      "2,10\n",
      "2,11\n",
      "2,12\n",
      "2,13\n",
      "2,14\n",
      "2,15\n",
      "2,16\n",
      "2,17\n",
      "2,18\n",
      "2,19\n",
      "2,20\n",
      "2,21\n",
      "2,22\n",
      "2,23\n",
      "2,24\n",
      "2,25\n",
      "2,26\n",
      "2,27\n",
      "2,28\n",
      "2,29\n",
      "2,30\n",
      "2,31\n",
      "2,32\n",
      "2,33\n",
      "2,34\n",
      "2,35\n",
      "2,37\n",
      "2,38\n",
      "2,39\n",
      "2,40\n",
      "2,41\n",
      "2,42\n",
      "2,43\n",
      "2,44\n",
      "2,45\n",
      "2,46\n",
      "2,47\n",
      "2,48\n",
      "2,49\n",
      "2,50\n",
      "2,51\n",
      "2,52\n",
      "2,53\n",
      "2,54\n",
      "2,55\n",
      "2,56\n",
      "2,57\n",
      "2,58\n",
      "2,59\n",
      "2,60\n",
      "2,61\n",
      "2,62\n",
      "2,63\n",
      "2,64\n",
      "2,65\n",
      "2,66\n",
      "2,67\n",
      "2,68\n",
      "2,69\n",
      "2,70\n",
      "2,71\n",
      "2,72\n",
      "2,73\n",
      "2,74\n",
      "2,75\n",
      "2,76\n",
      "2,77\n",
      "2,78\n",
      "2,79\n",
      "3,4\n",
      "3,5\n",
      "3,6\n",
      "3,7\n",
      "3,8\n",
      "3,9\n",
      "3,10\n",
      "3,11\n",
      "3,12\n",
      "3,13\n",
      "3,14\n",
      "3,15\n",
      "3,16\n",
      "3,17\n",
      "3,18\n",
      "3,19\n",
      "3,20\n",
      "3,21\n",
      "3,22\n",
      "3,23\n",
      "3,24\n",
      "3,25\n",
      "3,26\n",
      "3,27\n",
      "3,28\n",
      "3,29\n",
      "3,30\n",
      "3,31\n",
      "3,32\n",
      "3,33\n",
      "3,34\n",
      "3,35\n",
      "3,37\n",
      "3,38\n",
      "3,39\n",
      "3,40\n",
      "3,41\n",
      "3,42\n",
      "3,43\n",
      "3,44\n",
      "3,45\n",
      "3,46\n",
      "3,47\n",
      "3,48\n",
      "3,49\n",
      "3,50\n",
      "3,51\n",
      "3,52\n",
      "3,53\n",
      "3,54\n",
      "3,55\n",
      "3,56\n",
      "3,57\n",
      "3,58\n",
      "3,59\n",
      "3,60\n",
      "3,61\n",
      "3,62\n",
      "3,63\n",
      "3,64\n",
      "3,65\n",
      "3,66\n",
      "3,67\n",
      "3,68\n",
      "3,69\n",
      "3,70\n",
      "3,71\n",
      "3,72\n",
      "3,73\n",
      "3,74\n",
      "3,75\n",
      "3,76\n",
      "3,77\n",
      "3,78\n",
      "3,79\n",
      "4,5\n",
      "4,6\n",
      "4,7\n",
      "4,8\n",
      "4,9\n",
      "4,10\n",
      "4,11\n",
      "4,12\n",
      "4,13\n",
      "4,14\n",
      "4,15\n",
      "4,16\n",
      "4,17\n",
      "4,18\n",
      "4,19\n",
      "4,20\n",
      "4,21\n",
      "4,22\n",
      "4,23\n",
      "4,24\n",
      "4,25\n",
      "4,26\n",
      "4,27\n",
      "4,28\n",
      "4,29\n",
      "4,30\n",
      "4,31\n",
      "4,32\n",
      "4,33\n",
      "4,34\n",
      "4,35\n",
      "4,37\n",
      "4,38\n",
      "4,39\n",
      "4,40\n",
      "4,41\n",
      "4,42\n",
      "4,43\n",
      "4,44\n",
      "4,45\n",
      "4,46\n",
      "4,47\n",
      "4,48\n",
      "4,49\n",
      "4,50\n",
      "4,51\n",
      "4,52\n",
      "4,53\n",
      "4,54\n",
      "4,55\n",
      "4,56\n",
      "4,57\n",
      "4,58\n",
      "4,59\n",
      "4,60\n",
      "4,61\n",
      "4,62\n",
      "4,63\n",
      "4,64\n",
      "4,65\n",
      "4,66\n",
      "4,67\n",
      "4,68\n",
      "4,69\n",
      "4,70\n",
      "4,71\n",
      "4,72\n",
      "4,73\n",
      "4,74\n",
      "4,75\n",
      "4,76\n",
      "4,77\n",
      "4,78\n",
      "4,79\n",
      "5,6\n",
      "5,7\n",
      "5,8\n",
      "5,9\n",
      "5,10\n",
      "5,11\n",
      "5,12\n",
      "5,13\n",
      "5,14\n",
      "5,15\n",
      "5,16\n",
      "5,17\n",
      "5,18\n",
      "5,19\n",
      "5,20\n",
      "5,21\n",
      "5,22\n",
      "5,23\n",
      "5,24\n",
      "5,25\n",
      "5,26\n",
      "5,27\n",
      "5,28\n",
      "5,29\n",
      "5,30\n",
      "5,31\n",
      "5,32\n",
      "5,33\n",
      "5,34\n",
      "5,35\n",
      "5,37\n",
      "5,38\n",
      "5,39\n",
      "5,40\n",
      "5,41\n",
      "5,42\n",
      "5,43\n",
      "5,44\n",
      "5,45\n",
      "5,46\n",
      "5,47\n",
      "5,48\n",
      "5,49\n",
      "5,50\n",
      "5,51\n",
      "5,52\n",
      "5,53\n",
      "5,54\n",
      "5,55\n",
      "5,56\n",
      "5,57\n",
      "5,58\n",
      "5,59\n",
      "5,60\n",
      "5,61\n",
      "5,62\n",
      "5,63\n",
      "5,64\n",
      "5,65\n",
      "5,66\n",
      "5,67\n",
      "5,68\n",
      "5,69\n",
      "5,70\n",
      "5,71\n",
      "5,72\n",
      "5,73\n",
      "5,74\n",
      "5,75\n",
      "5,76\n",
      "5,77\n",
      "5,78\n",
      "5,79\n",
      "6,7\n",
      "6,8\n",
      "6,9\n",
      "6,10\n",
      "6,11\n",
      "6,12\n",
      "6,13\n",
      "6,14\n",
      "6,15\n",
      "6,16\n",
      "6,17\n",
      "6,18\n",
      "6,19\n",
      "6,20\n",
      "6,21\n",
      "6,22\n",
      "6,23\n",
      "6,24\n",
      "6,25\n",
      "6,26\n",
      "6,27\n",
      "6,28\n",
      "6,29\n",
      "6,30\n",
      "6,31\n",
      "6,32\n",
      "6,33\n",
      "6,34\n",
      "6,35\n",
      "6,37\n",
      "6,38\n",
      "6,39\n",
      "6,40\n",
      "6,41\n",
      "6,42\n",
      "6,43\n",
      "6,44\n",
      "6,45\n",
      "6,46\n",
      "6,47\n",
      "6,48\n",
      "6,49\n",
      "6,50\n",
      "6,51\n",
      "6,52\n",
      "6,53\n",
      "6,54\n",
      "6,55\n",
      "6,56\n",
      "6,57\n",
      "6,58\n",
      "6,59\n",
      "6,60\n",
      "6,61\n",
      "6,62\n",
      "6,63\n",
      "6,64\n",
      "6,65\n",
      "6,66\n",
      "6,67\n",
      "6,68\n",
      "6,69\n",
      "6,70\n",
      "6,71\n",
      "6,72\n",
      "6,73\n",
      "6,74\n",
      "6,75\n",
      "6,76\n",
      "6,77\n",
      "6,78\n",
      "6,79\n",
      "7,8\n",
      "7,9\n",
      "7,10\n",
      "7,11\n",
      "7,12\n",
      "7,13\n",
      "7,14\n",
      "7,15\n",
      "7,16\n",
      "7,17\n",
      "7,18\n",
      "7,19\n",
      "7,20\n",
      "7,21\n",
      "7,22\n",
      "7,23\n",
      "7,24\n",
      "7,25\n",
      "7,26\n",
      "7,27\n",
      "7,28\n",
      "7,29\n",
      "7,30\n",
      "7,31\n",
      "7,32\n",
      "7,33\n",
      "7,34\n",
      "7,35\n",
      "7,37\n",
      "7,38\n",
      "7,39\n",
      "7,40\n",
      "7,41\n",
      "7,42\n",
      "7,43\n",
      "7,44\n",
      "7,45\n",
      "7,46\n",
      "7,47\n",
      "7,48\n",
      "7,49\n",
      "7,50\n",
      "7,51\n",
      "7,52\n",
      "7,53\n",
      "7,54\n",
      "7,55\n",
      "7,56\n",
      "7,57\n",
      "7,58\n",
      "7,59\n",
      "7,60\n",
      "7,61\n",
      "7,62\n",
      "7,63\n",
      "7,64\n",
      "7,65\n",
      "7,66\n",
      "7,67\n",
      "7,68\n",
      "7,69\n",
      "7,70\n",
      "7,71\n",
      "7,72\n",
      "7,73\n",
      "7,74\n",
      "7,75\n",
      "7,76\n",
      "7,77\n",
      "7,78\n",
      "7,79\n",
      "8,9\n",
      "8,10\n",
      "8,11\n",
      "8,12\n",
      "8,13\n",
      "8,14\n",
      "8,15\n",
      "8,16\n",
      "8,17\n",
      "8,18\n",
      "8,19\n",
      "8,20\n",
      "8,21\n",
      "8,22\n",
      "8,23\n",
      "8,24\n",
      "8,25\n",
      "8,26\n",
      "8,27\n",
      "8,28\n",
      "8,29\n",
      "8,30\n",
      "8,31\n",
      "8,32\n",
      "8,33\n",
      "8,34\n",
      "8,35\n",
      "8,37\n",
      "8,38\n",
      "8,39\n",
      "8,40\n",
      "8,41\n",
      "8,42\n",
      "8,43\n",
      "8,44\n",
      "8,45\n",
      "8,46\n",
      "8,47\n",
      "8,48\n",
      "8,49\n",
      "8,50\n",
      "8,51\n",
      "8,52\n",
      "8,53\n",
      "8,54\n",
      "8,55\n",
      "8,56\n",
      "8,57\n",
      "8,58\n",
      "8,59\n",
      "8,60\n",
      "8,61\n",
      "8,62\n",
      "8,63\n",
      "8,64\n",
      "8,65\n",
      "8,66\n",
      "8,67\n",
      "8,68\n",
      "8,69\n",
      "8,70\n",
      "8,71\n",
      "8,72\n",
      "8,73\n",
      "8,74\n",
      "8,75\n",
      "8,76\n",
      "8,77\n",
      "8,78\n",
      "8,79\n",
      "9,10\n",
      "9,11\n",
      "9,12\n",
      "9,13\n",
      "9,14\n",
      "9,15\n",
      "9,16\n",
      "9,17\n",
      "9,18\n",
      "9,19\n",
      "9,20\n",
      "9,21\n",
      "9,22\n",
      "9,23\n",
      "9,24\n",
      "9,25\n",
      "9,26\n",
      "9,27\n",
      "9,28\n",
      "9,29\n",
      "9,30\n",
      "9,31\n",
      "9,32\n",
      "9,33\n",
      "9,34\n",
      "9,35\n",
      "9,37\n",
      "9,38\n",
      "9,39\n",
      "9,40\n",
      "9,41\n",
      "9,42\n",
      "9,43\n",
      "9,44\n",
      "9,45\n",
      "9,46\n",
      "9,47\n",
      "9,48\n",
      "9,49\n",
      "9,50\n",
      "9,51\n",
      "9,52\n",
      "9,53\n",
      "9,54\n",
      "9,55\n",
      "9,56\n",
      "9,57\n",
      "9,58\n",
      "9,59\n",
      "9,60\n",
      "9,61\n",
      "9,62\n",
      "9,63\n",
      "9,64\n",
      "9,65\n",
      "9,66\n",
      "9,67\n",
      "9,68\n",
      "9,69\n",
      "9,70\n",
      "9,71\n",
      "9,72\n",
      "9,73\n",
      "9,74\n",
      "9,75\n",
      "9,76\n",
      "9,77\n",
      "9,78\n",
      "9,79\n",
      "10,11\n",
      "10,12\n",
      "10,13\n",
      "10,14\n",
      "10,15\n",
      "10,16\n",
      "10,17\n",
      "10,18\n",
      "10,19\n",
      "10,20\n",
      "10,21\n",
      "10,22\n",
      "10,23\n",
      "10,24\n",
      "10,25\n",
      "10,26\n",
      "10,27\n",
      "10,28\n",
      "10,29\n",
      "10,30\n",
      "10,31\n",
      "10,32\n",
      "10,33\n",
      "10,34\n",
      "10,35\n",
      "10,37\n",
      "10,38\n",
      "10,39\n",
      "10,40\n",
      "10,41\n",
      "10,42\n",
      "10,43\n",
      "10,44\n",
      "10,45\n",
      "10,46\n",
      "10,47\n",
      "10,48\n",
      "10,49\n",
      "10,50\n",
      "10,51\n",
      "10,52\n",
      "10,53\n",
      "10,54\n",
      "10,55\n",
      "10,56\n",
      "10,57\n",
      "10,58\n",
      "10,59\n",
      "10,60\n",
      "10,61\n",
      "10,62\n",
      "10,63\n",
      "10,64\n",
      "10,65\n",
      "10,66\n",
      "10,67\n",
      "10,68\n",
      "10,69\n",
      "10,70\n",
      "10,71\n",
      "10,72\n",
      "10,73\n",
      "10,74\n",
      "10,75\n",
      "10,76\n",
      "10,77\n",
      "10,78\n",
      "10,79\n",
      "11,12\n",
      "11,13\n",
      "11,14\n",
      "11,15\n",
      "11,16\n",
      "11,17\n",
      "11,18\n",
      "11,19\n",
      "11,20\n",
      "11,21\n",
      "11,22\n",
      "11,23\n",
      "11,24\n",
      "11,25\n",
      "11,26\n",
      "11,27\n",
      "11,28\n",
      "11,29\n",
      "11,30\n",
      "11,31\n",
      "11,32\n",
      "11,33\n",
      "11,34\n",
      "11,35\n",
      "11,37\n",
      "11,38\n",
      "11,39\n",
      "11,40\n",
      "11,41\n",
      "11,42\n",
      "11,43\n",
      "11,44\n",
      "11,45\n",
      "11,46\n",
      "11,47\n",
      "11,48\n",
      "11,49\n",
      "11,50\n",
      "11,51\n",
      "11,52\n",
      "11,53\n",
      "11,54\n",
      "11,55\n",
      "11,56\n",
      "11,57\n",
      "11,58\n",
      "11,59\n",
      "11,60\n",
      "11,61\n",
      "11,62\n",
      "11,63\n",
      "11,64\n",
      "11,65\n",
      "11,66\n",
      "11,67\n",
      "11,68\n",
      "11,69\n",
      "11,70\n",
      "11,71\n",
      "11,72\n",
      "11,73\n",
      "11,74\n",
      "11,75\n",
      "11,76\n",
      "11,77\n",
      "11,78\n",
      "11,79\n",
      "12,13\n",
      "12,14\n",
      "12,15\n",
      "12,16\n",
      "12,17\n",
      "12,18\n",
      "12,19\n",
      "12,20\n",
      "12,21\n",
      "12,22\n",
      "12,23\n",
      "12,24\n",
      "12,25\n",
      "12,26\n",
      "12,27\n",
      "12,28\n",
      "12,29\n",
      "12,30\n",
      "12,31\n",
      "12,32\n",
      "12,33\n",
      "12,34\n",
      "12,35\n",
      "12,37\n",
      "12,38\n",
      "12,39\n",
      "12,40\n",
      "12,41\n",
      "12,42\n",
      "12,43\n",
      "12,44\n",
      "12,45\n",
      "12,46\n",
      "12,47\n",
      "12,48\n",
      "12,49\n",
      "12,50\n",
      "12,51\n",
      "12,52\n",
      "12,53\n",
      "12,54\n",
      "12,55\n",
      "12,56\n",
      "12,57\n",
      "12,58\n",
      "12,59\n",
      "12,60\n",
      "12,61\n",
      "12,62\n",
      "12,63\n",
      "12,64\n",
      "12,65\n",
      "12,66\n",
      "12,67\n",
      "12,68\n",
      "12,69\n",
      "12,70\n",
      "12,71\n",
      "12,72\n",
      "12,73\n",
      "12,74\n",
      "12,75\n",
      "12,76\n",
      "12,77\n",
      "12,78\n",
      "12,79\n",
      "13,14\n",
      "13,15\n",
      "13,16\n",
      "13,17\n",
      "13,18\n",
      "13,19\n",
      "13,20\n",
      "13,21\n",
      "13,22\n",
      "13,23\n",
      "13,24\n",
      "13,25\n",
      "13,26\n",
      "13,27\n",
      "13,28\n",
      "13,29\n",
      "13,30\n",
      "13,31\n",
      "13,32\n",
      "13,33\n",
      "13,34\n",
      "13,35\n",
      "13,37\n",
      "13,38\n",
      "13,39\n",
      "13,40\n",
      "13,41\n",
      "13,42\n",
      "13,43\n",
      "13,44\n",
      "13,45\n",
      "13,46\n",
      "13,47\n",
      "13,48\n",
      "13,49\n",
      "13,50\n",
      "13,51\n",
      "13,52\n",
      "13,53\n",
      "13,54\n",
      "13,55\n",
      "13,56\n",
      "13,57\n",
      "13,58\n",
      "13,59\n",
      "13,60\n",
      "13,61\n",
      "13,62\n",
      "13,63\n",
      "13,64\n",
      "13,65\n",
      "13,66\n",
      "13,67\n",
      "13,68\n",
      "13,69\n",
      "13,70\n",
      "13,71\n",
      "13,72\n",
      "13,73\n",
      "13,74\n",
      "13,75\n",
      "13,76\n",
      "13,77\n",
      "13,78\n",
      "13,79\n",
      "14,15\n",
      "14,16\n",
      "14,17\n",
      "14,18\n",
      "14,19\n",
      "14,20\n",
      "14,21\n",
      "14,22\n",
      "14,23\n",
      "14,24\n",
      "14,25\n",
      "14,26\n",
      "14,27\n",
      "14,28\n",
      "14,29\n",
      "14,30\n",
      "14,31\n",
      "14,32\n",
      "14,33\n",
      "14,34\n",
      "14,35\n",
      "14,37\n",
      "14,38\n",
      "14,39\n",
      "14,40\n",
      "14,41\n",
      "14,42\n",
      "14,43\n",
      "14,44\n",
      "14,45\n",
      "14,46\n",
      "14,47\n",
      "14,48\n",
      "14,49\n",
      "14,50\n",
      "14,51\n",
      "14,52\n",
      "14,53\n",
      "14,54\n",
      "14,55\n",
      "14,56\n",
      "14,57\n",
      "14,58\n",
      "14,59\n",
      "14,60\n",
      "14,61\n",
      "14,62\n",
      "14,63\n",
      "14,64\n",
      "14,65\n",
      "14,66\n",
      "14,67\n",
      "14,68\n",
      "14,69\n",
      "14,70\n",
      "14,71\n",
      "14,72\n",
      "14,73\n",
      "14,74\n",
      "14,75\n",
      "14,76\n",
      "14,77\n",
      "14,78\n",
      "14,79\n",
      "15,16\n",
      "15,17\n",
      "15,18\n",
      "15,19\n",
      "15,20\n",
      "15,21\n",
      "15,22\n",
      "15,23\n",
      "15,24\n",
      "15,25\n",
      "15,26\n",
      "15,27\n",
      "15,28\n",
      "15,29\n",
      "15,30\n",
      "15,31\n",
      "15,32\n",
      "15,33\n",
      "15,34\n",
      "15,35\n",
      "15,37\n",
      "15,38\n",
      "15,39\n",
      "15,40\n",
      "15,41\n",
      "15,42\n",
      "15,43\n",
      "15,44\n",
      "15,45\n",
      "15,46\n",
      "15,47\n",
      "15,48\n",
      "15,49\n",
      "15,50\n",
      "15,51\n",
      "15,52\n",
      "15,53\n",
      "15,54\n",
      "15,55\n",
      "15,56\n",
      "15,57\n",
      "15,58\n",
      "15,59\n",
      "15,60\n",
      "15,61\n",
      "15,62\n",
      "15,63\n",
      "15,64\n",
      "15,65\n",
      "15,66\n",
      "15,67\n",
      "15,68\n",
      "15,69\n",
      "15,70\n",
      "15,71\n",
      "15,72\n",
      "15,73\n",
      "15,74\n",
      "15,75\n",
      "15,76\n",
      "15,77\n",
      "15,78\n",
      "15,79\n",
      "16,17\n",
      "16,18\n",
      "16,19\n",
      "16,20\n",
      "16,21\n",
      "16,22\n",
      "16,23\n",
      "16,24\n",
      "16,25\n",
      "16,26\n",
      "16,27\n",
      "16,28\n",
      "16,29\n",
      "16,30\n",
      "16,31\n",
      "16,32\n",
      "16,33\n",
      "16,34\n",
      "16,35\n",
      "16,37\n",
      "16,38\n",
      "16,39\n",
      "16,40\n",
      "16,41\n",
      "16,42\n",
      "16,43\n",
      "16,44\n",
      "16,45\n",
      "16,46\n",
      "16,47\n",
      "16,48\n",
      "16,49\n",
      "16,50\n",
      "16,51\n",
      "16,52\n",
      "16,53\n",
      "16,54\n",
      "16,55\n",
      "16,56\n",
      "16,57\n",
      "16,58\n",
      "16,59\n",
      "16,60\n",
      "16,61\n",
      "16,62\n",
      "16,63\n",
      "16,64\n",
      "16,65\n",
      "16,66\n",
      "16,67\n",
      "16,68\n",
      "16,69\n",
      "16,70\n",
      "16,71\n",
      "16,72\n",
      "16,73\n",
      "16,74\n",
      "16,75\n",
      "16,76\n",
      "16,77\n",
      "16,78\n",
      "16,79\n",
      "17,18\n",
      "17,19\n",
      "17,20\n",
      "17,21\n",
      "17,22\n",
      "17,23\n",
      "17,24\n",
      "17,25\n",
      "17,26\n",
      "17,27\n",
      "17,28\n",
      "17,29\n",
      "17,30\n",
      "17,31\n",
      "17,32\n",
      "17,33\n",
      "17,34\n",
      "17,35\n",
      "17,37\n",
      "17,38\n",
      "17,39\n",
      "17,40\n",
      "17,41\n",
      "17,42\n",
      "17,43\n",
      "17,44\n",
      "17,45\n",
      "17,46\n",
      "17,47\n",
      "17,48\n",
      "17,49\n",
      "17,50\n",
      "17,51\n",
      "17,52\n",
      "17,53\n",
      "17,54\n",
      "17,55\n",
      "17,56\n",
      "17,57\n",
      "17,58\n",
      "17,59\n",
      "17,60\n",
      "17,61\n",
      "17,62\n",
      "17,63\n",
      "17,64\n",
      "17,65\n",
      "17,66\n",
      "17,67\n",
      "17,68\n",
      "17,69\n",
      "17,70\n",
      "17,71\n",
      "17,72\n",
      "17,73\n",
      "17,74\n",
      "17,75\n",
      "17,76\n",
      "17,77\n",
      "17,78\n",
      "17,79\n",
      "18,19\n",
      "18,20\n",
      "18,21\n",
      "18,22\n",
      "18,23\n",
      "18,24\n",
      "18,25\n",
      "18,26\n",
      "18,27\n",
      "18,28\n",
      "18,29\n",
      "18,30\n",
      "18,31\n",
      "18,32\n",
      "18,33\n",
      "18,34\n",
      "18,35\n",
      "18,37\n",
      "18,38\n",
      "18,39\n",
      "18,40\n",
      "18,41\n",
      "18,42\n",
      "18,43\n",
      "18,44\n",
      "18,45\n",
      "18,46\n",
      "18,47\n",
      "18,48\n",
      "18,49\n",
      "18,50\n",
      "18,51\n",
      "18,52\n",
      "18,53\n",
      "18,54\n",
      "18,55\n",
      "18,56\n",
      "18,57\n",
      "18,58\n",
      "18,59\n",
      "18,60\n",
      "18,61\n",
      "18,62\n",
      "18,63\n",
      "18,64\n",
      "18,65\n",
      "18,66\n",
      "18,67\n",
      "18,68\n",
      "18,69\n",
      "18,70\n",
      "18,71\n",
      "18,72\n",
      "18,73\n",
      "18,74\n",
      "18,75\n",
      "18,76\n",
      "18,77\n",
      "18,78\n",
      "18,79\n",
      "19,20\n",
      "19,21\n",
      "19,22\n",
      "19,23\n",
      "19,24\n",
      "19,25\n",
      "19,26\n",
      "19,27\n",
      "19,28\n",
      "19,29\n",
      "19,30\n",
      "19,31\n",
      "19,32\n",
      "19,33\n",
      "19,34\n",
      "19,35\n",
      "19,37\n",
      "19,38\n",
      "19,39\n",
      "19,40\n",
      "19,41\n",
      "19,42\n",
      "19,43\n",
      "19,44\n",
      "19,45\n",
      "19,46\n",
      "19,47\n",
      "19,48\n",
      "19,49\n",
      "19,50\n",
      "19,51\n",
      "19,52\n",
      "19,53\n",
      "19,54\n",
      "19,55\n",
      "19,56\n",
      "19,57\n",
      "19,58\n",
      "19,59\n",
      "19,60\n",
      "19,61\n",
      "19,62\n",
      "19,63\n",
      "19,64\n",
      "19,65\n",
      "19,66\n",
      "19,67\n",
      "19,68\n",
      "19,69\n",
      "19,70\n",
      "19,71\n",
      "19,72\n",
      "19,73\n",
      "19,74\n",
      "19,75\n",
      "19,76\n",
      "19,77\n",
      "19,78\n",
      "19,79\n",
      "20,21\n",
      "20,22\n",
      "20,23\n",
      "20,24\n",
      "20,25\n",
      "20,26\n",
      "20,27\n",
      "20,28\n",
      "20,29\n",
      "20,30\n",
      "20,31\n",
      "20,32\n",
      "20,33\n",
      "20,34\n",
      "20,35\n",
      "20,37\n",
      "20,38\n",
      "20,39\n",
      "20,40\n",
      "20,41\n",
      "20,42\n",
      "20,43\n",
      "20,44\n",
      "20,45\n",
      "20,46\n",
      "20,47\n",
      "20,48\n",
      "20,49\n",
      "20,50\n",
      "20,51\n",
      "20,52\n",
      "20,53\n",
      "20,54\n",
      "20,55\n",
      "20,56\n",
      "20,57\n",
      "20,58\n",
      "20,59\n",
      "20,60\n",
      "20,61\n",
      "20,62\n",
      "20,63\n",
      "20,64\n",
      "20,65\n",
      "20,66\n",
      "20,67\n",
      "20,68\n",
      "20,69\n",
      "20,70\n",
      "20,71\n",
      "20,72\n",
      "20,73\n",
      "20,74\n",
      "20,75\n",
      "20,76\n",
      "20,77\n",
      "20,78\n",
      "20,79\n",
      "21,22\n",
      "21,23\n",
      "21,24\n",
      "21,25\n",
      "21,26\n",
      "21,27\n",
      "21,28\n",
      "21,29\n",
      "21,30\n",
      "21,31\n",
      "21,32\n",
      "21,33\n",
      "21,34\n",
      "21,35\n",
      "21,37\n",
      "21,38\n",
      "21,39\n",
      "21,40\n",
      "21,41\n",
      "21,42\n",
      "21,43\n",
      "21,44\n",
      "21,45\n",
      "21,46\n",
      "21,47\n",
      "21,48\n",
      "21,49\n",
      "21,50\n",
      "21,51\n",
      "21,52\n",
      "21,53\n",
      "21,54\n",
      "21,55\n",
      "21,56\n",
      "21,57\n",
      "21,58\n",
      "21,59\n",
      "21,60\n",
      "21,61\n",
      "21,62\n",
      "21,63\n",
      "21,64\n",
      "21,65\n",
      "21,66\n",
      "21,67\n",
      "21,68\n",
      "21,69\n",
      "21,70\n",
      "21,71\n",
      "21,72\n",
      "21,73\n",
      "21,74\n",
      "21,75\n",
      "21,76\n",
      "21,77\n",
      "21,78\n",
      "21,79\n",
      "22,23\n",
      "22,24\n",
      "22,25\n",
      "22,26\n",
      "22,27\n",
      "22,28\n",
      "22,29\n",
      "22,30\n",
      "22,31\n",
      "22,32\n",
      "22,33\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22,34\n",
      "22,35\n",
      "22,37\n",
      "22,38\n",
      "22,39\n",
      "22,40\n",
      "22,41\n",
      "22,42\n",
      "22,43\n",
      "22,44\n",
      "22,45\n",
      "22,46\n",
      "22,47\n",
      "22,48\n",
      "22,49\n",
      "22,50\n",
      "22,51\n",
      "22,52\n",
      "22,53\n",
      "22,54\n",
      "22,55\n",
      "22,56\n",
      "22,57\n",
      "22,58\n",
      "22,59\n",
      "22,60\n",
      "22,61\n",
      "22,62\n",
      "22,63\n",
      "22,64\n",
      "22,65\n",
      "22,66\n",
      "22,67\n",
      "22,68\n",
      "22,69\n",
      "22,70\n",
      "22,71\n",
      "22,72\n",
      "22,73\n",
      "22,74\n",
      "22,75\n",
      "22,76\n",
      "22,77\n",
      "22,78\n",
      "22,79\n",
      "23,24\n",
      "23,25\n",
      "23,26\n",
      "23,27\n",
      "23,28\n",
      "23,29\n",
      "23,30\n",
      "23,31\n",
      "23,32\n",
      "23,33\n",
      "23,34\n",
      "23,35\n",
      "23,37\n",
      "23,38\n",
      "23,39\n",
      "23,40\n",
      "23,41\n",
      "23,42\n",
      "23,43\n",
      "23,44\n",
      "23,45\n",
      "23,46\n",
      "23,47\n",
      "23,48\n",
      "23,49\n",
      "23,50\n",
      "23,51\n",
      "23,52\n",
      "23,53\n",
      "23,54\n",
      "23,55\n",
      "23,56\n",
      "23,57\n",
      "23,58\n",
      "23,59\n",
      "23,60\n",
      "23,61\n",
      "23,62\n",
      "23,63\n",
      "23,64\n",
      "23,65\n",
      "23,66\n",
      "23,67\n",
      "23,68\n",
      "23,69\n",
      "23,70\n",
      "23,71\n",
      "23,72\n",
      "23,73\n",
      "23,74\n",
      "23,75\n",
      "23,76\n",
      "23,77\n",
      "23,78\n",
      "23,79\n",
      "24,25\n",
      "24,26\n",
      "24,27\n",
      "24,28\n",
      "24,29\n",
      "24,30\n",
      "24,31\n",
      "24,32\n",
      "24,33\n",
      "24,34\n",
      "24,35\n",
      "24,37\n",
      "24,38\n",
      "24,39\n",
      "24,40\n",
      "24,41\n",
      "24,42\n",
      "24,43\n",
      "24,44\n",
      "24,45\n",
      "24,46\n",
      "24,47\n",
      "24,48\n",
      "24,49\n",
      "24,50\n",
      "24,51\n",
      "24,52\n",
      "24,53\n",
      "24,54\n",
      "24,55\n",
      "24,56\n",
      "24,57\n",
      "24,58\n",
      "24,59\n",
      "24,60\n",
      "24,61\n",
      "24,62\n",
      "24,63\n",
      "24,64\n",
      "24,65\n",
      "24,66\n",
      "24,67\n",
      "24,68\n",
      "24,69\n",
      "24,70\n",
      "24,71\n",
      "24,72\n",
      "24,73\n",
      "24,74\n",
      "24,75\n",
      "24,76\n",
      "24,77\n",
      "24,78\n",
      "24,79\n",
      "25,26\n",
      "25,27\n",
      "25,28\n",
      "25,29\n",
      "25,30\n",
      "25,31\n",
      "25,32\n",
      "25,33\n",
      "25,34\n",
      "25,35\n",
      "25,37\n",
      "25,38\n",
      "25,39\n",
      "25,40\n",
      "25,41\n",
      "25,42\n",
      "25,43\n",
      "25,44\n",
      "25,45\n",
      "25,46\n",
      "25,47\n",
      "25,48\n",
      "25,49\n",
      "25,50\n",
      "25,51\n",
      "25,52\n",
      "25,53\n",
      "25,54\n",
      "25,55\n",
      "25,56\n",
      "25,57\n",
      "25,58\n",
      "25,59\n",
      "25,60\n",
      "25,61\n",
      "25,62\n",
      "25,63\n",
      "25,64\n",
      "25,65\n",
      "25,66\n",
      "25,67\n",
      "25,68\n",
      "25,69\n",
      "25,70\n",
      "25,71\n",
      "25,72\n",
      "25,73\n",
      "25,74\n",
      "25,75\n",
      "25,76\n",
      "25,77\n",
      "25,78\n",
      "25,79\n",
      "26,27\n",
      "26,28\n",
      "26,29\n",
      "26,30\n",
      "26,31\n",
      "26,32\n",
      "26,33\n",
      "26,34\n",
      "26,35\n",
      "26,37\n",
      "26,38\n",
      "26,39\n",
      "26,40\n",
      "26,41\n",
      "26,42\n",
      "26,43\n",
      "26,44\n",
      "26,45\n",
      "26,46\n",
      "26,47\n",
      "26,48\n",
      "26,49\n",
      "26,50\n",
      "26,51\n",
      "26,52\n",
      "26,53\n",
      "26,54\n",
      "26,55\n",
      "26,56\n",
      "26,57\n",
      "26,58\n",
      "26,59\n",
      "26,60\n",
      "26,61\n",
      "26,62\n",
      "26,63\n",
      "26,64\n",
      "26,65\n",
      "26,66\n",
      "26,67\n",
      "26,68\n",
      "26,69\n",
      "26,70\n",
      "26,71\n",
      "26,72\n",
      "26,73\n",
      "26,74\n",
      "26,75\n",
      "26,76\n",
      "26,77\n",
      "26,78\n",
      "26,79\n",
      "27,28\n",
      "27,29\n",
      "27,30\n",
      "27,31\n",
      "27,32\n",
      "27,33\n",
      "27,34\n",
      "27,35\n",
      "27,37\n",
      "27,38\n",
      "27,39\n",
      "27,40\n",
      "27,41\n",
      "27,42\n",
      "27,43\n",
      "27,44\n",
      "27,45\n",
      "27,46\n",
      "27,47\n",
      "27,48\n",
      "27,49\n",
      "27,50\n",
      "27,51\n",
      "27,52\n",
      "27,53\n",
      "27,54\n",
      "27,55\n",
      "27,56\n",
      "27,57\n",
      "27,58\n",
      "27,59\n",
      "27,60\n",
      "27,61\n",
      "27,62\n",
      "27,63\n",
      "27,64\n",
      "27,65\n",
      "27,66\n",
      "27,67\n",
      "27,68\n",
      "27,69\n",
      "27,70\n",
      "27,71\n",
      "27,72\n",
      "27,73\n",
      "27,74\n",
      "27,75\n",
      "27,76\n",
      "27,77\n",
      "27,78\n",
      "27,79\n",
      "28,29\n",
      "28,30\n",
      "28,31\n",
      "28,32\n",
      "28,33\n",
      "28,34\n",
      "28,35\n",
      "28,37\n",
      "28,38\n",
      "28,39\n",
      "28,40\n",
      "28,41\n",
      "28,42\n",
      "28,43\n",
      "28,44\n",
      "28,45\n",
      "28,46\n",
      "28,47\n",
      "28,48\n",
      "28,49\n",
      "28,50\n",
      "28,51\n",
      "28,52\n",
      "28,53\n",
      "28,54\n",
      "28,55\n",
      "28,56\n",
      "28,57\n",
      "28,58\n",
      "28,59\n",
      "28,60\n",
      "28,61\n",
      "28,62\n",
      "28,63\n",
      "28,64\n",
      "28,65\n",
      "28,66\n",
      "28,67\n",
      "28,68\n",
      "28,69\n",
      "28,70\n",
      "28,71\n",
      "28,72\n",
      "28,73\n",
      "28,74\n",
      "28,75\n",
      "28,76\n",
      "28,77\n",
      "28,78\n",
      "28,79\n",
      "29,30\n",
      "29,31\n",
      "29,32\n",
      "29,33\n",
      "29,34\n",
      "29,35\n",
      "29,37\n",
      "29,38\n",
      "29,39\n",
      "29,40\n",
      "29,41\n",
      "29,42\n",
      "29,43\n",
      "29,44\n",
      "29,45\n",
      "29,46\n",
      "29,47\n",
      "29,48\n",
      "29,49\n",
      "29,50\n",
      "29,51\n",
      "29,52\n",
      "29,53\n",
      "29,54\n",
      "29,55\n",
      "29,56\n",
      "29,57\n",
      "29,58\n",
      "29,59\n",
      "29,60\n",
      "29,61\n",
      "29,62\n",
      "29,63\n",
      "29,64\n",
      "29,65\n",
      "29,66\n",
      "29,67\n",
      "29,68\n",
      "29,69\n",
      "29,70\n",
      "29,71\n",
      "29,72\n",
      "29,73\n",
      "29,74\n",
      "29,75\n",
      "29,76\n",
      "29,77\n",
      "29,78\n",
      "29,79\n",
      "30,31\n",
      "30,32\n",
      "30,33\n",
      "30,34\n",
      "30,35\n",
      "30,37\n",
      "30,38\n",
      "30,39\n",
      "30,40\n",
      "30,41\n",
      "30,42\n",
      "30,43\n",
      "30,44\n",
      "30,45\n",
      "30,46\n",
      "30,47\n",
      "30,48\n",
      "30,49\n",
      "30,50\n",
      "30,51\n",
      "30,52\n",
      "30,53\n",
      "30,54\n",
      "30,55\n",
      "30,56\n",
      "30,57\n",
      "30,58\n",
      "30,59\n",
      "30,60\n",
      "30,61\n",
      "30,62\n",
      "30,63\n",
      "30,64\n",
      "30,65\n",
      "30,66\n",
      "30,67\n",
      "30,68\n",
      "30,69\n",
      "30,70\n",
      "30,71\n",
      "30,72\n",
      "30,73\n",
      "30,74\n",
      "30,75\n",
      "30,76\n",
      "30,77\n",
      "30,78\n",
      "30,79\n",
      "31,32\n",
      "31,33\n",
      "31,34\n",
      "31,35\n",
      "31,37\n",
      "31,38\n",
      "31,39\n",
      "31,40\n",
      "31,41\n",
      "31,42\n",
      "31,43\n",
      "31,44\n",
      "31,45\n",
      "31,46\n",
      "31,47\n",
      "31,48\n",
      "31,49\n",
      "31,50\n",
      "31,51\n",
      "31,52\n",
      "31,53\n",
      "31,54\n",
      "31,55\n",
      "31,56\n",
      "31,57\n",
      "31,58\n",
      "31,59\n",
      "31,60\n",
      "31,61\n",
      "31,62\n",
      "31,63\n",
      "31,64\n",
      "31,65\n",
      "31,66\n",
      "31,67\n",
      "31,68\n",
      "31,69\n",
      "31,70\n",
      "31,71\n",
      "31,72\n",
      "31,73\n",
      "31,74\n",
      "31,75\n",
      "31,76\n",
      "31,77\n",
      "31,78\n",
      "31,79\n",
      "32,33\n",
      "32,34\n",
      "32,35\n",
      "32,37\n",
      "32,38\n",
      "32,39\n",
      "32,40\n",
      "32,41\n",
      "32,42\n",
      "32,43\n",
      "32,44\n",
      "32,45\n",
      "32,46\n",
      "32,47\n",
      "32,48\n",
      "32,49\n",
      "32,50\n",
      "32,51\n",
      "32,52\n",
      "32,53\n",
      "32,54\n",
      "32,55\n",
      "32,56\n",
      "32,57\n",
      "32,58\n",
      "32,59\n",
      "32,60\n",
      "32,61\n",
      "32,62\n",
      "32,63\n",
      "32,64\n",
      "32,65\n",
      "32,66\n",
      "32,67\n",
      "32,68\n",
      "32,69\n",
      "32,70\n",
      "32,71\n",
      "32,72\n",
      "32,73\n",
      "32,74\n",
      "32,75\n",
      "32,76\n",
      "32,77\n",
      "32,78\n",
      "32,79\n",
      "33,34\n",
      "33,35\n",
      "33,37\n",
      "33,38\n",
      "33,39\n",
      "33,40\n",
      "33,41\n",
      "33,42\n",
      "33,43\n",
      "33,44\n",
      "33,45\n",
      "33,46\n",
      "33,47\n",
      "33,48\n",
      "33,49\n",
      "33,50\n",
      "33,51\n",
      "33,52\n",
      "33,53\n",
      "33,54\n",
      "33,55\n",
      "33,56\n",
      "33,57\n",
      "33,58\n",
      "33,59\n",
      "33,60\n",
      "33,61\n",
      "33,62\n",
      "33,63\n",
      "33,64\n",
      "33,65\n",
      "33,66\n",
      "33,67\n",
      "33,68\n",
      "33,69\n",
      "33,70\n",
      "33,71\n",
      "33,72\n",
      "33,73\n",
      "33,74\n",
      "33,75\n",
      "33,76\n",
      "33,77\n",
      "33,78\n",
      "33,79\n",
      "34,35\n",
      "34,37\n",
      "34,38\n",
      "34,39\n",
      "34,40\n",
      "34,41\n",
      "34,42\n",
      "34,43\n",
      "34,44\n",
      "34,45\n",
      "34,46\n",
      "34,47\n",
      "34,48\n",
      "34,49\n",
      "34,50\n",
      "34,51\n",
      "34,52\n",
      "34,53\n",
      "34,54\n",
      "34,55\n",
      "34,56\n",
      "34,57\n",
      "34,58\n",
      "34,59\n",
      "34,60\n",
      "34,61\n",
      "34,62\n",
      "34,63\n",
      "34,64\n",
      "34,65\n",
      "34,66\n",
      "34,67\n",
      "34,68\n",
      "34,69\n",
      "34,70\n",
      "34,71\n",
      "34,72\n",
      "34,73\n",
      "34,74\n",
      "34,75\n",
      "34,76\n",
      "34,77\n",
      "34,78\n",
      "34,79\n",
      "35,37\n",
      "35,38\n",
      "35,39\n",
      "35,40\n",
      "35,41\n",
      "35,42\n",
      "35,43\n",
      "35,44\n",
      "35,45\n",
      "35,46\n",
      "35,47\n",
      "35,48\n",
      "35,49\n",
      "35,50\n",
      "35,51\n",
      "35,52\n",
      "35,53\n",
      "35,54\n",
      "35,55\n",
      "35,56\n",
      "35,57\n",
      "35,58\n",
      "35,59\n",
      "35,60\n",
      "35,61\n",
      "35,62\n",
      "35,63\n",
      "35,64\n",
      "35,65\n",
      "35,66\n",
      "35,67\n",
      "35,68\n",
      "35,69\n",
      "35,70\n",
      "35,71\n",
      "35,72\n",
      "35,73\n",
      "35,74\n",
      "35,75\n",
      "35,76\n",
      "35,77\n",
      "35,78\n",
      "35,79\n",
      "37,38\n",
      "37,39\n",
      "37,40\n",
      "37,41\n",
      "37,42\n",
      "37,43\n",
      "37,44\n",
      "37,45\n",
      "37,46\n",
      "37,47\n",
      "37,48\n",
      "37,49\n",
      "37,50\n",
      "37,51\n",
      "37,52\n",
      "37,53\n",
      "37,54\n",
      "37,55\n",
      "37,56\n",
      "37,57\n",
      "37,58\n",
      "37,59\n",
      "37,60\n",
      "37,61\n",
      "37,62\n",
      "37,63\n",
      "37,64\n",
      "37,65\n",
      "37,66\n",
      "37,67\n",
      "37,68\n",
      "37,69\n",
      "37,70\n",
      "37,71\n",
      "37,72\n",
      "37,73\n",
      "37,74\n",
      "37,75\n",
      "37,76\n",
      "37,77\n",
      "37,78\n",
      "37,79\n",
      "38,39\n",
      "38,40\n",
      "38,41\n",
      "38,42\n",
      "38,43\n",
      "38,44\n",
      "38,45\n",
      "38,46\n",
      "38,47\n",
      "38,48\n",
      "38,49\n",
      "38,50\n",
      "38,51\n",
      "38,52\n",
      "38,53\n",
      "38,54\n",
      "38,55\n",
      "38,56\n",
      "38,57\n",
      "38,58\n",
      "38,59\n",
      "38,60\n",
      "38,61\n",
      "38,62\n",
      "38,63\n",
      "38,64\n",
      "38,65\n",
      "38,66\n",
      "38,67\n",
      "38,68\n",
      "38,69\n",
      "38,70\n",
      "38,71\n",
      "38,72\n",
      "38,73\n",
      "38,74\n",
      "38,75\n",
      "38,76\n",
      "38,77\n",
      "38,78\n",
      "38,79\n",
      "39,40\n",
      "39,41\n",
      "39,42\n",
      "39,43\n",
      "39,44\n",
      "39,45\n",
      "39,46\n",
      "39,47\n",
      "39,48\n",
      "39,49\n",
      "39,50\n",
      "39,51\n",
      "39,52\n",
      "39,53\n",
      "39,54\n",
      "39,55\n",
      "39,56\n",
      "39,57\n",
      "39,58\n",
      "39,59\n",
      "39,60\n",
      "39,61\n",
      "39,62\n",
      "39,63\n",
      "39,64\n",
      "39,65\n",
      "39,66\n",
      "39,67\n",
      "39,68\n",
      "39,69\n",
      "39,70\n",
      "39,71\n",
      "39,72\n",
      "39,73\n",
      "39,74\n",
      "39,75\n",
      "39,76\n",
      "39,77\n",
      "39,78\n",
      "39,79\n",
      "40,41\n",
      "40,42\n",
      "40,43\n",
      "40,44\n",
      "40,45\n",
      "40,46\n",
      "40,47\n",
      "40,48\n",
      "40,49\n",
      "40,50\n",
      "40,51\n",
      "40,52\n",
      "40,53\n",
      "40,54\n",
      "40,55\n",
      "40,56\n",
      "40,57\n",
      "40,58\n",
      "40,59\n",
      "40,60\n",
      "40,61\n",
      "40,62\n",
      "40,63\n",
      "40,64\n",
      "40,65\n",
      "40,66\n",
      "40,67\n",
      "40,68\n",
      "40,69\n",
      "40,70\n",
      "40,71\n",
      "40,72\n",
      "40,73\n",
      "40,74\n",
      "40,75\n",
      "40,76\n",
      "40,77\n",
      "40,78\n",
      "40,79\n",
      "41,42\n",
      "41,43\n",
      "41,44\n",
      "41,45\n",
      "41,46\n",
      "41,47\n",
      "41,48\n",
      "41,49\n",
      "41,50\n",
      "41,51\n",
      "41,52\n",
      "41,53\n",
      "41,54\n",
      "41,55\n",
      "41,56\n",
      "41,57\n",
      "41,58\n",
      "41,59\n",
      "41,60\n",
      "41,61\n",
      "41,62\n",
      "41,63\n",
      "41,64\n",
      "41,65\n",
      "41,66\n",
      "41,67\n",
      "41,68\n",
      "41,69\n",
      "41,70\n",
      "41,71\n",
      "41,72\n",
      "41,73\n",
      "41,74\n",
      "41,75\n",
      "41,76\n",
      "41,77\n",
      "41,78\n",
      "41,79\n",
      "42,43\n",
      "42,44\n",
      "42,45\n",
      "42,46\n",
      "42,47\n",
      "42,48\n",
      "42,49\n",
      "42,50\n",
      "42,51\n",
      "42,52\n",
      "42,53\n",
      "42,54\n",
      "42,55\n",
      "42,56\n",
      "42,57\n",
      "42,58\n",
      "42,59\n",
      "42,60\n",
      "42,61\n",
      "42,62\n",
      "42,63\n",
      "42,64\n",
      "42,65\n",
      "42,66\n",
      "42,67\n",
      "42,68\n",
      "42,69\n",
      "42,70\n",
      "42,71\n",
      "42,72\n",
      "42,73\n",
      "42,74\n",
      "42,75\n",
      "42,76\n",
      "42,77\n",
      "42,78\n",
      "42,79\n",
      "43,44\n",
      "43,45\n",
      "43,46\n",
      "43,47\n",
      "43,48\n",
      "43,49\n",
      "43,50\n",
      "43,51\n",
      "43,52\n",
      "43,53\n",
      "43,54\n",
      "43,55\n",
      "43,56\n",
      "43,57\n",
      "43,58\n",
      "43,59\n",
      "43,60\n",
      "43,61\n",
      "43,62\n",
      "43,63\n",
      "43,64\n",
      "43,65\n",
      "43,66\n",
      "43,67\n",
      "43,68\n",
      "43,69\n",
      "43,70\n",
      "43,71\n",
      "43,72\n",
      "43,73\n",
      "43,74\n",
      "43,75\n",
      "43,76\n",
      "43,77\n",
      "43,78\n",
      "43,79\n",
      "44,45\n",
      "44,46\n",
      "44,47\n",
      "44,48\n",
      "44,49\n",
      "44,50\n",
      "44,51\n",
      "44,52\n",
      "44,53\n",
      "44,54\n",
      "44,55\n",
      "44,56\n",
      "44,57\n",
      "44,58\n",
      "44,59\n",
      "44,60\n",
      "44,61\n",
      "44,62\n",
      "44,63\n",
      "44,64\n",
      "44,65\n",
      "44,66\n",
      "44,67\n",
      "44,68\n",
      "44,69\n",
      "44,70\n",
      "44,71\n",
      "44,72\n",
      "44,73\n",
      "44,74\n",
      "44,75\n",
      "44,76\n",
      "44,77\n",
      "44,78\n",
      "44,79\n",
      "45,46\n",
      "45,47\n",
      "45,48\n",
      "45,49\n",
      "45,50\n",
      "45,51\n",
      "45,52\n",
      "45,53\n",
      "45,54\n",
      "45,55\n",
      "45,56\n",
      "45,57\n",
      "45,58\n",
      "45,59\n",
      "45,60\n",
      "45,61\n",
      "45,62\n",
      "45,63\n",
      "45,64\n",
      "45,65\n",
      "45,66\n",
      "45,67\n",
      "45,68\n",
      "45,69\n",
      "45,70\n",
      "45,71\n",
      "45,72\n",
      "45,73\n",
      "45,74\n",
      "45,75\n",
      "45,76\n",
      "45,77\n",
      "45,78\n",
      "45,79\n",
      "46,47\n",
      "46,48\n",
      "46,49\n",
      "46,50\n",
      "46,51\n",
      "46,52\n",
      "46,53\n",
      "46,54\n",
      "46,55\n",
      "46,56\n",
      "46,57\n",
      "46,58\n",
      "46,59\n",
      "46,60\n",
      "46,61\n",
      "46,62\n",
      "46,63\n",
      "46,64\n",
      "46,65\n",
      "46,66\n",
      "46,67\n",
      "46,68\n",
      "46,69\n",
      "46,70\n",
      "46,71\n",
      "46,72\n",
      "46,73\n",
      "46,74\n",
      "46,75\n",
      "46,76\n",
      "46,77\n",
      "46,78\n",
      "46,79\n",
      "47,48\n",
      "47,49\n",
      "47,50\n",
      "47,51\n",
      "47,52\n",
      "47,53\n",
      "47,54\n",
      "47,55\n",
      "47,56\n",
      "47,57\n",
      "47,58\n",
      "47,59\n",
      "47,60\n",
      "47,61\n",
      "47,62\n",
      "47,63\n",
      "47,64\n",
      "47,65\n",
      "47,66\n",
      "47,67\n",
      "47,68\n",
      "47,69\n",
      "47,70\n",
      "47,71\n",
      "47,72\n",
      "47,73\n",
      "47,74\n",
      "47,75\n",
      "47,76\n",
      "47,77\n",
      "47,78\n",
      "47,79\n",
      "48,49\n",
      "48,50\n",
      "48,51\n",
      "48,52\n",
      "48,53\n",
      "48,54\n",
      "48,55\n",
      "48,56\n",
      "48,57\n",
      "48,58\n",
      "48,59\n",
      "48,60\n",
      "48,61\n",
      "48,62\n",
      "48,63\n",
      "48,64\n",
      "48,65\n",
      "48,66\n",
      "48,67\n",
      "48,68\n",
      "48,69\n",
      "48,70\n",
      "48,71\n",
      "48,72\n",
      "48,73\n",
      "48,74\n",
      "48,75\n",
      "48,76\n",
      "48,77\n",
      "48,78\n",
      "48,79\n",
      "49,50\n",
      "49,51\n",
      "49,52\n",
      "49,53\n",
      "49,54\n",
      "49,55\n",
      "49,56\n",
      "49,57\n",
      "49,58\n",
      "49,59\n",
      "49,60\n",
      "49,61\n",
      "49,62\n",
      "49,63\n",
      "49,64\n",
      "49,65\n",
      "49,66\n",
      "49,67\n",
      "49,68\n",
      "49,69\n",
      "49,70\n",
      "49,71\n",
      "49,72\n",
      "49,73\n",
      "49,74\n",
      "49,75\n",
      "49,76\n",
      "49,77\n",
      "49,78\n",
      "49,79\n",
      "50,51\n",
      "50,52\n",
      "50,53\n",
      "50,54\n",
      "50,55\n",
      "50,56\n",
      "50,57\n",
      "50,58\n",
      "50,59\n",
      "50,60\n",
      "50,61\n",
      "50,62\n",
      "50,63\n",
      "50,64\n",
      "50,65\n",
      "50,66\n",
      "50,67\n",
      "50,68\n",
      "50,69\n",
      "50,70\n",
      "50,71\n",
      "50,72\n",
      "50,73\n",
      "50,74\n",
      "50,75\n",
      "50,76\n",
      "50,77\n",
      "50,78\n",
      "50,79\n",
      "51,52\n",
      "51,53\n",
      "51,54\n",
      "51,55\n",
      "51,56\n",
      "51,57\n",
      "51,58\n",
      "51,59\n",
      "51,60\n",
      "51,61\n",
      "51,62\n",
      "51,63\n",
      "51,64\n",
      "51,65\n",
      "51,66\n",
      "51,67\n",
      "51,68\n",
      "51,69\n",
      "51,70\n",
      "51,71\n",
      "51,72\n",
      "51,73\n",
      "51,74\n",
      "51,75\n",
      "51,76\n",
      "51,77\n",
      "51,78\n",
      "51,79\n",
      "52,53\n",
      "52,54\n",
      "52,55\n",
      "52,56\n",
      "52,57\n",
      "52,58\n",
      "52,59\n",
      "52,60\n",
      "52,61\n",
      "52,62\n",
      "52,63\n",
      "52,64\n",
      "52,65\n",
      "52,66\n",
      "52,67\n",
      "52,68\n",
      "52,69\n",
      "52,70\n",
      "52,71\n",
      "52,72\n",
      "52,73\n",
      "52,74\n",
      "52,75\n",
      "52,76\n",
      "52,77\n",
      "52,78\n",
      "52,79\n",
      "53,54\n",
      "53,55\n",
      "53,56\n",
      "53,57\n",
      "53,58\n",
      "53,59\n",
      "53,60\n",
      "53,61\n",
      "53,62\n",
      "53,63\n",
      "53,64\n",
      "53,65\n",
      "53,66\n",
      "53,67\n",
      "53,68\n",
      "53,69\n",
      "53,70\n",
      "53,71\n",
      "53,72\n",
      "53,73\n",
      "53,74\n",
      "53,75\n",
      "53,76\n",
      "53,77\n",
      "53,78\n",
      "53,79\n",
      "54,55\n",
      "54,56\n",
      "54,57\n",
      "54,58\n",
      "54,59\n",
      "54,60\n",
      "54,61\n",
      "54,62\n",
      "54,63\n",
      "54,64\n",
      "54,65\n",
      "54,66\n",
      "54,67\n",
      "54,68\n",
      "54,69\n",
      "54,70\n",
      "54,71\n",
      "54,72\n",
      "54,73\n",
      "54,74\n",
      "54,75\n",
      "54,76\n",
      "54,77\n",
      "54,78\n",
      "54,79\n",
      "55,56\n",
      "55,57\n",
      "55,58\n",
      "55,59\n",
      "55,60\n",
      "55,61\n",
      "55,62\n",
      "55,63\n",
      "55,64\n",
      "55,65\n",
      "55,66\n",
      "55,67\n",
      "55,68\n",
      "55,69\n",
      "55,70\n",
      "55,71\n",
      "55,72\n",
      "55,73\n",
      "55,74\n",
      "55,75\n",
      "55,76\n",
      "55,77\n",
      "55,78\n",
      "55,79\n",
      "56,57\n",
      "56,58\n",
      "56,59\n",
      "56,60\n",
      "56,61\n",
      "56,62\n",
      "56,63\n",
      "56,64\n",
      "56,65\n",
      "56,66\n",
      "56,67\n",
      "56,68\n",
      "56,69\n",
      "56,70\n",
      "56,71\n",
      "56,72\n",
      "56,73\n",
      "56,74\n",
      "56,75\n",
      "56,76\n",
      "56,77\n",
      "56,78\n",
      "56,79\n",
      "57,58\n",
      "57,59\n",
      "57,60\n",
      "57,61\n",
      "57,62\n",
      "57,63\n",
      "57,64\n",
      "57,65\n",
      "57,66\n",
      "57,67\n",
      "57,68\n",
      "57,69\n",
      "57,70\n",
      "57,71\n",
      "57,72\n",
      "57,73\n",
      "57,74\n",
      "57,75\n",
      "57,76\n",
      "57,77\n",
      "57,78\n",
      "57,79\n",
      "58,59\n",
      "58,60\n",
      "58,61\n",
      "58,62\n",
      "58,63\n",
      "58,64\n",
      "58,65\n",
      "58,66\n",
      "58,67\n",
      "58,68\n",
      "58,69\n",
      "58,70\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "58,71\n",
      "58,72\n",
      "58,73\n",
      "58,74\n",
      "58,75\n",
      "58,76\n",
      "58,77\n",
      "58,78\n",
      "58,79\n",
      "59,60\n",
      "59,61\n",
      "59,62\n",
      "59,63\n",
      "59,64\n",
      "59,65\n",
      "59,66\n",
      "59,67\n",
      "59,68\n",
      "59,69\n",
      "59,70\n",
      "59,71\n",
      "59,72\n",
      "59,73\n",
      "59,74\n",
      "59,75\n",
      "59,76\n",
      "59,77\n",
      "59,78\n",
      "59,79\n",
      "60,61\n",
      "60,62\n",
      "60,63\n",
      "60,64\n",
      "60,65\n",
      "60,66\n",
      "60,67\n",
      "60,68\n",
      "60,69\n",
      "60,70\n",
      "60,71\n",
      "60,72\n",
      "60,73\n",
      "60,74\n",
      "60,75\n",
      "60,76\n",
      "60,77\n",
      "60,78\n",
      "60,79\n",
      "61,62\n",
      "61,63\n",
      "61,64\n",
      "61,65\n",
      "61,66\n",
      "61,67\n",
      "61,68\n",
      "61,69\n",
      "61,70\n",
      "61,71\n",
      "61,72\n",
      "61,73\n",
      "61,74\n",
      "61,75\n",
      "61,76\n",
      "61,77\n",
      "61,78\n",
      "61,79\n",
      "62,63\n",
      "62,64\n",
      "62,65\n",
      "62,66\n",
      "62,67\n",
      "62,68\n",
      "62,69\n",
      "62,70\n",
      "62,71\n",
      "62,72\n",
      "62,73\n",
      "62,74\n",
      "62,75\n",
      "62,76\n",
      "62,77\n",
      "62,78\n",
      "62,79\n",
      "63,64\n",
      "63,65\n",
      "63,66\n",
      "63,67\n",
      "63,68\n",
      "63,69\n",
      "63,70\n",
      "63,71\n",
      "63,72\n",
      "63,73\n",
      "63,74\n",
      "63,75\n",
      "63,76\n",
      "63,77\n",
      "63,78\n",
      "63,79\n",
      "64,65\n",
      "64,66\n",
      "64,67\n",
      "64,68\n",
      "64,69\n",
      "64,70\n",
      "64,71\n",
      "64,72\n",
      "64,73\n",
      "64,74\n",
      "64,75\n",
      "64,76\n",
      "64,77\n",
      "64,78\n",
      "64,79\n",
      "65,66\n",
      "65,67\n",
      "65,68\n",
      "65,69\n",
      "65,70\n",
      "65,71\n",
      "65,72\n",
      "65,73\n",
      "65,74\n",
      "65,75\n",
      "65,76\n",
      "65,77\n",
      "65,78\n",
      "65,79\n",
      "66,67\n",
      "66,68\n",
      "66,69\n",
      "66,70\n",
      "66,71\n",
      "66,72\n",
      "66,73\n",
      "66,74\n",
      "66,75\n",
      "66,76\n",
      "66,77\n",
      "66,78\n",
      "66,79\n",
      "67,68\n",
      "67,69\n",
      "67,70\n",
      "67,71\n",
      "67,72\n",
      "67,73\n",
      "67,74\n",
      "67,75\n",
      "67,76\n",
      "67,77\n",
      "67,78\n",
      "67,79\n",
      "68,69\n",
      "68,70\n",
      "68,71\n",
      "68,72\n",
      "68,73\n",
      "68,74\n",
      "68,75\n",
      "68,76\n",
      "68,77\n",
      "68,78\n",
      "68,79\n",
      "69,70\n",
      "69,71\n",
      "69,72\n",
      "69,73\n",
      "69,74\n",
      "69,75\n",
      "69,76\n",
      "69,77\n",
      "69,78\n",
      "69,79\n",
      "70,71\n",
      "70,72\n",
      "70,73\n",
      "70,74\n",
      "70,75\n",
      "70,76\n",
      "70,77\n",
      "70,78\n",
      "70,79\n",
      "71,72\n",
      "71,73\n",
      "71,74\n",
      "71,75\n",
      "71,76\n",
      "71,77\n",
      "71,78\n",
      "71,79\n",
      "72,73\n",
      "72,74\n",
      "72,75\n",
      "72,76\n",
      "72,77\n",
      "72,78\n",
      "72,79\n",
      "73,74\n",
      "73,75\n",
      "73,76\n",
      "73,77\n",
      "73,78\n",
      "73,79\n",
      "74,75\n",
      "74,76\n",
      "74,77\n",
      "74,78\n",
      "74,79\n",
      "75,76\n",
      "75,77\n",
      "75,78\n",
      "75,79\n",
      "76,77\n",
      "76,78\n",
      "76,79\n",
      "77,78\n",
      "77,79\n",
      "78,79\n"
     ]
    }
   ],
   "source": [
    "# ovo\n",
    "all_rfs = {} \n",
    "all_predicts = [] \n",
    "sids = sorted(all_choose.keys()) \n",
    "for _i in range(len(sids)): \n",
    "    for _j in range(_i+1, len(sids)): \n",
    "        s1 = sids[_i] \n",
    "        s2 = sids[_j] \n",
    "        _indexs = list(set(all_choose[s1]).union(set(all_choose[s2]))) \n",
    "        _train_bool_index = (_train_b_y[:,s1] == 1) | (_train_b_y[:,s2]==1) \n",
    "        ptrain_x = train_wifi_all_x[_train_bool_index][:,_indexs] \n",
    "        ptrain_y = train_y[_train_bool_index] \n",
    "        ptrain_x = np.concatenate([ptrain_x, train_lonlats[_train_bool_index]],axis=1)\n",
    "        rf2 = RandomForestClassifier(n_jobs=-1,n_estimators=122,class_weight=\"balanced\")\n",
    "        rf2.fit(ptrain_x,ptrain_y)\n",
    "        _key = str(s1) + \",\" + str(s2)\n",
    "        print _key\n",
    "        all_rfs[_key] = rf2\n",
    "\n",
    "        pvalid_x = valid_wifi_all_x[:,_indexs]\n",
    "        pvalid_x = np.concatenate([pvalid_x,valid_lonlats],axis=1)\n",
    "        _p = rf2.predict(pvalid_x)\n",
    "        all_predicts.append(_p)\n",
    "all_predict = np.vstack(all_predicts).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "ap = np.concatenate([all_predict,last_p.reshape(-1,1)],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "lp = np.asarray(map(lambda x:np.argmax(np.bincount(x)),ap))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9426008968609866"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc(lp,valid_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0,1\n",
      "0,2\n",
      "0,3\n",
      "0,4\n",
      "0,5\n",
      "0,6\n",
      "0,7\n",
      "0,8\n",
      "0,9\n",
      "0,10\n",
      "0,11\n",
      "0,12\n",
      "0,13\n",
      "0,14\n",
      "0,15\n",
      "0,16\n",
      "0,17\n",
      "0,18\n",
      "0,19\n",
      "0,20\n",
      "0,21\n",
      "0,22\n",
      "0,23\n",
      "0,24\n",
      "0,25\n",
      "0,26\n",
      "0,27\n",
      "0,28\n",
      "0,29\n",
      "0,30\n",
      "0,31\n",
      "0,32\n",
      "0,33\n",
      "0,34\n",
      "0,35\n",
      "0,37\n",
      "0,38\n",
      "0,39\n",
      "0,40\n",
      "0,41\n",
      "0,42\n",
      "0,43\n",
      "0,44\n",
      "0,45\n",
      "0,46\n",
      "0,47\n",
      "0,48\n",
      "0,49\n",
      "0,50\n",
      "0,51\n",
      "0,52\n",
      "0,53\n",
      "0,54\n",
      "0,55\n",
      "0,56\n",
      "0,57\n",
      "0,58\n",
      "0,59\n",
      "0,60\n",
      "0,61\n",
      "0,62\n",
      "0,63\n",
      "0,64\n",
      "0,65\n",
      "0,66\n",
      "0,67\n",
      "0,68\n",
      "0,69\n",
      "0,70\n",
      "0,71\n",
      "0,72\n",
      "0,73\n",
      "0,74\n",
      "0,75\n",
      "0,76\n",
      "0,77\n",
      "0,78\n",
      "0,79\n",
      "1,2\n",
      "1,3\n",
      "1,4\n",
      "1,5\n",
      "1,6\n",
      "1,7\n",
      "1,8\n",
      "1,9\n",
      "1,10\n",
      "1,11\n",
      "1,12\n",
      "1,13\n",
      "1,14\n",
      "1,15\n",
      "1,16\n",
      "1,17\n",
      "1,18\n",
      "1,19\n",
      "1,20\n",
      "1,21\n",
      "1,22\n",
      "1,23\n",
      "1,24\n",
      "1,25\n",
      "1,26\n",
      "1,27\n",
      "1,28\n",
      "1,29\n",
      "1,30\n",
      "1,31\n",
      "1,32\n",
      "1,33\n",
      "1,34\n",
      "1,35\n",
      "1,37\n",
      "1,38\n",
      "1,39\n",
      "1,40\n",
      "1,41\n",
      "1,42\n",
      "1,43\n",
      "1,44\n",
      "1,45\n",
      "1,46\n",
      "1,47\n",
      "1,48\n",
      "1,49\n",
      "1,50\n",
      "1,51\n",
      "1,52\n",
      "1,53\n",
      "1,54\n",
      "1,55\n",
      "1,56\n",
      "1,57\n",
      "1,58\n",
      "1,59\n",
      "1,60\n",
      "1,61\n",
      "1,62\n",
      "1,63\n",
      "1,64\n",
      "1,65\n",
      "1,66\n",
      "1,67\n",
      "1,68\n",
      "1,69\n",
      "1,70\n",
      "1,71\n",
      "1,72\n",
      "1,73\n",
      "1,74\n",
      "1,75\n",
      "1,76\n",
      "1,77\n",
      "1,78\n",
      "1,79\n",
      "2,3\n",
      "2,4\n",
      "2,5\n",
      "2,6\n",
      "2,7\n",
      "2,8\n",
      "2,9\n",
      "2,10\n",
      "2,11\n",
      "2,12\n",
      "2,13\n",
      "2,14\n",
      "2,15\n",
      "2,16\n",
      "2,17\n",
      "2,18\n",
      "2,19\n",
      "2,20\n",
      "2,21\n",
      "2,22\n",
      "2,23\n",
      "2,24\n",
      "2,25\n",
      "2,26\n",
      "2,27\n",
      "2,28\n",
      "2,29\n",
      "2,30\n",
      "2,31\n",
      "2,32\n",
      "2,33\n",
      "2,34\n",
      "2,35\n",
      "2,37\n",
      "2,38\n",
      "2,39\n",
      "2,40\n",
      "2,41\n",
      "2,42\n",
      "2,43\n",
      "2,44\n",
      "2,45\n",
      "2,46\n",
      "2,47\n",
      "2,48\n",
      "2,49\n",
      "2,50\n",
      "2,51\n",
      "2,52\n",
      "2,53\n",
      "2,54\n",
      "2,55\n",
      "2,56\n",
      "2,57\n",
      "2,58\n",
      "2,59\n",
      "2,60\n",
      "2,61\n",
      "2,62\n",
      "2,63\n",
      "2,64\n",
      "2,65\n",
      "2,66\n",
      "2,67\n",
      "2,68\n",
      "2,69\n",
      "2,70\n",
      "2,71\n",
      "2,72\n",
      "2,73\n",
      "2,74\n",
      "2,75\n",
      "2,76\n",
      "2,77\n",
      "2,78\n",
      "2,79\n",
      "3,4\n",
      "3,5\n",
      "3,6\n",
      "3,7\n",
      "3,8\n",
      "3,9\n",
      "3,10\n",
      "3,11\n",
      "3,12\n",
      "3,13\n",
      "3,14\n",
      "3,15\n",
      "3,16\n",
      "3,17\n",
      "3,18\n",
      "3,19\n",
      "3,20\n",
      "3,21\n",
      "3,22\n",
      "3,23\n",
      "3,24\n",
      "3,25\n",
      "3,26\n",
      "3,27\n",
      "3,28\n",
      "3,29\n",
      "3,30\n",
      "3,31\n",
      "3,32\n",
      "3,33\n",
      "3,34\n",
      "3,35\n",
      "3,37\n",
      "3,38\n",
      "3,39\n",
      "3,40\n",
      "3,41\n",
      "3,42\n",
      "3,43\n",
      "3,44\n",
      "3,45\n",
      "3,46\n",
      "3,47\n",
      "3,48\n",
      "3,49\n",
      "3,50\n",
      "3,51\n",
      "3,52\n",
      "3,53\n",
      "3,54\n",
      "3,55\n",
      "3,56\n",
      "3,57\n",
      "3,58\n",
      "3,59\n",
      "3,60\n",
      "3,61\n",
      "3,62\n",
      "3,63\n",
      "3,64\n",
      "3,65\n",
      "3,66\n",
      "3,67\n",
      "3,68\n",
      "3,69\n",
      "3,70\n",
      "3,71\n",
      "3,72\n",
      "3,73\n",
      "3,74\n",
      "3,75\n",
      "3,76\n",
      "3,77\n",
      "3,78\n",
      "3,79\n",
      "4,5\n",
      "4,6\n",
      "4,7\n",
      "4,8\n",
      "4,9\n",
      "4,10\n",
      "4,11\n",
      "4,12\n",
      "4,13\n",
      "4,14\n",
      "4,15\n",
      "4,16\n",
      "4,17\n",
      "4,18\n",
      "4,19\n",
      "4,20\n",
      "4,21\n",
      "4,22\n",
      "4,23\n",
      "4,24\n",
      "4,25\n",
      "4,26\n",
      "4,27\n",
      "4,28\n",
      "4,29\n",
      "4,30\n",
      "4,31\n",
      "4,32\n",
      "4,33\n",
      "4,34\n",
      "4,35\n",
      "4,37\n",
      "4,38\n",
      "4,39\n",
      "4,40\n",
      "4,41\n",
      "4,42\n",
      "4,43\n",
      "4,44\n",
      "4,45\n",
      "4,46\n",
      "4,47\n",
      "4,48\n",
      "4,49\n",
      "4,50\n",
      "4,51\n",
      "4,52\n",
      "4,53\n",
      "4,54\n",
      "4,55\n",
      "4,56\n",
      "4,57\n",
      "4,58\n",
      "4,59\n",
      "4,60\n",
      "4,61\n",
      "4,62\n",
      "4,63\n",
      "4,64\n",
      "4,65\n",
      "4,66\n",
      "4,67\n",
      "4,68\n",
      "4,69\n",
      "4,70\n",
      "4,71\n",
      "4,72\n",
      "4,73\n",
      "4,74\n",
      "4,75\n",
      "4,76\n",
      "4,77\n",
      "4,78\n",
      "4,79\n",
      "5,6\n",
      "5,7\n",
      "5,8\n",
      "5,9\n",
      "5,10\n",
      "5,11\n",
      "5,12\n",
      "5,13\n",
      "5,14\n",
      "5,15\n",
      "5,16\n",
      "5,17\n",
      "5,18\n",
      "5,19\n",
      "5,20\n",
      "5,21\n",
      "5,22\n",
      "5,23\n",
      "5,24\n",
      "5,25\n",
      "5,26\n",
      "5,27\n",
      "5,28\n",
      "5,29\n",
      "5,30\n",
      "5,31\n",
      "5,32\n",
      "5,33\n",
      "5,34\n",
      "5,35\n",
      "5,37\n",
      "5,38\n",
      "5,39\n",
      "5,40\n",
      "5,41\n",
      "5,42\n",
      "5,43\n",
      "5,44\n",
      "5,45\n",
      "5,46\n",
      "5,47\n",
      "5,48\n",
      "5,49\n",
      "5,50\n",
      "5,51\n",
      "5,52\n",
      "5,53\n",
      "5,54\n",
      "5,55\n",
      "5,56\n",
      "5,57\n",
      "5,58\n",
      "5,59\n",
      "5,60\n",
      "5,61\n",
      "5,62\n",
      "5,63\n",
      "5,64\n",
      "5,65\n",
      "5,66\n",
      "5,67\n",
      "5,68\n",
      "5,69\n",
      "5,70\n",
      "5,71\n",
      "5,72\n",
      "5,73\n",
      "5,74\n",
      "5,75\n",
      "5,76\n",
      "5,77\n",
      "5,78\n",
      "5,79\n",
      "6,7\n",
      "6,8\n",
      "6,9\n",
      "6,10\n",
      "6,11\n",
      "6,12\n",
      "6,13\n",
      "6,14\n",
      "6,15\n",
      "6,16\n",
      "6,17\n",
      "6,18\n",
      "6,19\n",
      "6,20\n",
      "6,21\n",
      "6,22\n",
      "6,23\n",
      "6,24\n",
      "6,25\n",
      "6,26\n",
      "6,27\n",
      "6,28\n",
      "6,29\n",
      "6,30\n",
      "6,31\n",
      "6,32\n",
      "6,33\n",
      "6,34\n",
      "6,35\n",
      "6,37\n",
      "6,38\n",
      "6,39\n",
      "6,40\n",
      "6,41\n",
      "6,42\n",
      "6,43\n",
      "6,44\n",
      "6,45\n",
      "6,46\n",
      "6,47\n",
      "6,48\n",
      "6,49\n",
      "6,50\n",
      "6,51\n",
      "6,52\n",
      "6,53\n",
      "6,54\n",
      "6,55\n",
      "6,56\n",
      "6,57\n",
      "6,58\n",
      "6,59\n",
      "6,60\n",
      "6,61\n",
      "6,62\n",
      "6,63\n",
      "6,64\n",
      "6,65\n",
      "6,66\n",
      "6,67\n",
      "6,68\n",
      "6,69\n",
      "6,70\n",
      "6,71\n",
      "6,72\n",
      "6,73\n",
      "6,74\n",
      "6,75\n",
      "6,76\n",
      "6,77\n",
      "6,78\n",
      "6,79\n",
      "7,8\n",
      "7,9\n",
      "7,10\n",
      "7,11\n",
      "7,12\n",
      "7,13\n",
      "7,14\n",
      "7,15\n",
      "7,16\n",
      "7,17\n",
      "7,18\n",
      "7,19\n",
      "7,20\n",
      "7,21\n",
      "7,22\n",
      "7,23\n",
      "7,24\n",
      "7,25\n",
      "7,26\n",
      "7,27\n",
      "7,28\n",
      "7,29\n",
      "7,30\n",
      "7,31\n",
      "7,32\n",
      "7,33\n",
      "7,34\n",
      "7,35\n",
      "7,37\n",
      "7,38\n",
      "7,39\n",
      "7,40\n",
      "7,41\n",
      "7,42\n",
      "7,43\n",
      "7,44\n",
      "7,45\n",
      "7,46\n",
      "7,47\n",
      "7,48\n",
      "7,49\n",
      "7,50\n",
      "7,51\n",
      "7,52\n",
      "7,53\n",
      "7,54\n",
      "7,55\n",
      "7,56\n",
      "7,57\n",
      "7,58\n",
      "7,59\n",
      "7,60\n",
      "7,61\n",
      "7,62\n",
      "7,63\n",
      "7,64\n",
      "7,65\n",
      "7,66\n",
      "7,67\n",
      "7,68\n",
      "7,69\n",
      "7,70\n",
      "7,71\n",
      "7,72\n",
      "7,73\n",
      "7,74\n",
      "7,75\n",
      "7,76\n",
      "7,77\n",
      "7,78\n",
      "7,79\n",
      "8,9\n",
      "8,10\n",
      "8,11\n",
      "8,12\n",
      "8,13\n",
      "8,14\n",
      "8,15\n",
      "8,16\n",
      "8,17\n",
      "8,18\n",
      "8,19\n",
      "8,20\n",
      "8,21\n",
      "8,22\n",
      "8,23\n",
      "8,24\n",
      "8,25\n",
      "8,26\n",
      "8,27\n",
      "8,28\n",
      "8,29\n",
      "8,30\n",
      "8,31\n",
      "8,32\n",
      "8,33\n",
      "8,34\n",
      "8,35\n",
      "8,37\n",
      "8,38\n",
      "8,39\n",
      "8,40\n",
      "8,41\n",
      "8,42\n",
      "8,43\n",
      "8,44\n",
      "8,45\n",
      "8,46\n",
      "8,47\n",
      "8,48\n",
      "8,49\n",
      "8,50\n",
      "8,51\n",
      "8,52\n",
      "8,53\n",
      "8,54\n",
      "8,55\n",
      "8,56\n",
      "8,57\n",
      "8,58\n",
      "8,59\n",
      "8,60\n",
      "8,61\n",
      "8,62\n",
      "8,63\n",
      "8,64\n",
      "8,65\n",
      "8,66\n",
      "8,67\n",
      "8,68\n",
      "8,69\n",
      "8,70\n",
      "8,71\n",
      "8,72\n",
      "8,73\n",
      "8,74\n",
      "8,75\n",
      "8,76\n",
      "8,77\n",
      "8,78\n",
      "8,79\n",
      "9,10\n",
      "9,11\n",
      "9,12\n",
      "9,13\n",
      "9,14\n",
      "9,15\n",
      "9,16\n",
      "9,17\n",
      "9,18\n",
      "9,19\n",
      "9,20\n",
      "9,21\n",
      "9,22\n",
      "9,23\n",
      "9,24\n",
      "9,25\n",
      "9,26\n",
      "9,27\n",
      "9,28\n",
      "9,29\n",
      "9,30\n",
      "9,31\n",
      "9,32\n",
      "9,33\n",
      "9,34\n",
      "9,35\n",
      "9,37\n",
      "9,38\n",
      "9,39\n",
      "9,40\n",
      "9,41\n",
      "9,42\n",
      "9,43\n",
      "9,44\n",
      "9,45\n",
      "9,46\n",
      "9,47\n",
      "9,48\n",
      "9,49\n",
      "9,50\n",
      "9,51\n",
      "9,52\n",
      "9,53\n",
      "9,54\n",
      "9,55\n",
      "9,56\n",
      "9,57\n",
      "9,58\n",
      "9,59\n",
      "9,60\n",
      "9,61\n",
      "9,62\n",
      "9,63\n",
      "9,64\n",
      "9,65\n",
      "9,66\n",
      "9,67\n",
      "9,68\n",
      "9,69\n",
      "9,70\n",
      "9,71\n",
      "9,72\n",
      "9,73\n",
      "9,74\n",
      "9,75\n",
      "9,76\n",
      "9,77\n",
      "9,78\n",
      "9,79\n",
      "10,11\n",
      "10,12\n",
      "10,13\n",
      "10,14\n",
      "10,15\n",
      "10,16\n",
      "10,17\n",
      "10,18\n",
      "10,19\n",
      "10,20\n",
      "10,21\n",
      "10,22\n",
      "10,23\n",
      "10,24\n",
      "10,25\n",
      "10,26\n",
      "10,27\n",
      "10,28\n",
      "10,29\n",
      "10,30\n",
      "10,31\n",
      "10,32\n",
      "10,33\n",
      "10,34\n",
      "10,35\n",
      "10,37\n",
      "10,38\n",
      "10,39\n",
      "10,40\n",
      "10,41\n",
      "10,42\n",
      "10,43\n",
      "10,44\n",
      "10,45\n",
      "10,46\n",
      "10,47\n",
      "10,48\n",
      "10,49\n",
      "10,50\n",
      "10,51\n",
      "10,52\n",
      "10,53\n",
      "10,54\n",
      "10,55\n",
      "10,56\n",
      "10,57\n",
      "10,58\n",
      "10,59\n",
      "10,60\n",
      "10,61\n",
      "10,62\n",
      "10,63\n",
      "10,64\n",
      "10,65\n",
      "10,66\n",
      "10,67\n",
      "10,68\n",
      "10,69\n",
      "10,70\n",
      "10,71\n",
      "10,72\n",
      "10,73\n",
      "10,74\n",
      "10,75\n",
      "10,76\n",
      "10,77\n",
      "10,78\n",
      "10,79\n",
      "11,12\n",
      "11,13\n",
      "11,14\n",
      "11,15\n",
      "11,16\n",
      "11,17\n",
      "11,18\n",
      "11,19\n",
      "11,20\n",
      "11,21\n",
      "11,22\n",
      "11,23\n",
      "11,24\n",
      "11,25\n",
      "11,26\n",
      "11,27\n",
      "11,28\n",
      "11,29\n",
      "11,30\n",
      "11,31\n",
      "11,32\n",
      "11,33\n",
      "11,34\n",
      "11,35\n",
      "11,37\n",
      "11,38\n",
      "11,39\n",
      "11,40\n",
      "11,41\n",
      "11,42\n",
      "11,43\n",
      "11,44\n",
      "11,45\n",
      "11,46\n",
      "11,47\n",
      "11,48\n",
      "11,49\n",
      "11,50\n",
      "11,51\n",
      "11,52\n",
      "11,53\n",
      "11,54\n",
      "11,55\n",
      "11,56\n",
      "11,57\n",
      "11,58\n",
      "11,59\n",
      "11,60\n",
      "11,61\n",
      "11,62\n",
      "11,63\n",
      "11,64\n",
      "11,65\n",
      "11,66\n",
      "11,67\n",
      "11,68\n",
      "11,69\n",
      "11,70\n",
      "11,71\n",
      "11,72\n",
      "11,73\n",
      "11,74\n",
      "11,75\n",
      "11,76\n",
      "11,77\n",
      "11,78\n",
      "11,79\n",
      "12,13\n",
      "12,14\n",
      "12,15\n",
      "12,16\n",
      "12,17\n",
      "12,18\n",
      "12,19\n",
      "12,20\n",
      "12,21\n",
      "12,22\n",
      "12,23\n",
      "12,24\n",
      "12,25\n",
      "12,26\n",
      "12,27\n",
      "12,28\n",
      "12,29\n",
      "12,30\n",
      "12,31\n",
      "12,32\n",
      "12,33\n",
      "12,34\n",
      "12,35\n",
      "12,37\n",
      "12,38\n",
      "12,39\n",
      "12,40\n",
      "12,41\n",
      "12,42\n",
      "12,43\n",
      "12,44\n",
      "12,45\n",
      "12,46\n",
      "12,47\n",
      "12,48\n",
      "12,49\n",
      "12,50\n",
      "12,51\n",
      "12,52\n",
      "12,53\n",
      "12,54\n",
      "12,55\n",
      "12,56\n",
      "12,57\n",
      "12,58\n",
      "12,59\n",
      "12,60\n",
      "12,61\n",
      "12,62\n",
      "12,63\n",
      "12,64\n",
      "12,65\n",
      "12,66\n",
      "12,67\n",
      "12,68\n",
      "12,69\n",
      "12,70\n",
      "12,71\n",
      "12,72\n",
      "12,73\n",
      "12,74\n",
      "12,75\n",
      "12,76\n",
      "12,77\n",
      "12,78\n",
      "12,79\n",
      "13,14\n",
      "13,15\n",
      "13,16\n",
      "13,17\n",
      "13,18\n",
      "13,19\n",
      "13,20\n",
      "13,21\n",
      "13,22\n",
      "13,23\n",
      "13,24\n",
      "13,25\n",
      "13,26\n",
      "13,27\n",
      "13,28\n",
      "13,29\n",
      "13,30\n",
      "13,31\n",
      "13,32\n",
      "13,33\n",
      "13,34\n",
      "13,35\n",
      "13,37\n",
      "13,38\n",
      "13,39\n",
      "13,40\n",
      "13,41\n",
      "13,42\n",
      "13,43\n",
      "13,44\n",
      "13,45\n",
      "13,46\n",
      "13,47\n",
      "13,48\n",
      "13,49\n",
      "13,50\n",
      "13,51\n",
      "13,52\n",
      "13,53\n",
      "13,54\n",
      "13,55\n",
      "13,56\n",
      "13,57\n",
      "13,58\n",
      "13,59\n",
      "13,60\n",
      "13,61\n",
      "13,62\n",
      "13,63\n",
      "13,64\n",
      "13,65\n",
      "13,66\n",
      "13,67\n",
      "13,68\n",
      "13,69\n",
      "13,70\n",
      "13,71\n",
      "13,72\n",
      "13,73\n",
      "13,74\n",
      "13,75\n",
      "13,76\n",
      "13,77\n",
      "13,78\n",
      "13,79\n",
      "14,15\n",
      "14,16\n",
      "14,17\n",
      "14,18\n",
      "14,19\n",
      "14,20\n",
      "14,21\n",
      "14,22\n",
      "14,23\n",
      "14,24\n",
      "14,25\n",
      "14,26\n",
      "14,27\n",
      "14,28\n",
      "14,29\n",
      "14,30\n",
      "14,31\n",
      "14,32\n",
      "14,33\n",
      "14,34\n",
      "14,35\n",
      "14,37\n",
      "14,38\n",
      "14,39\n",
      "14,40\n",
      "14,41\n",
      "14,42\n",
      "14,43\n",
      "14,44\n",
      "14,45\n",
      "14,46\n",
      "14,47\n",
      "14,48\n",
      "14,49\n",
      "14,50\n",
      "14,51\n",
      "14,52\n",
      "14,53\n",
      "14,54\n",
      "14,55\n",
      "14,56\n",
      "14,57\n",
      "14,58\n",
      "14,59\n",
      "14,60\n",
      "14,61\n",
      "14,62\n",
      "14,63\n",
      "14,64\n",
      "14,65\n",
      "14,66\n",
      "14,67\n",
      "14,68\n",
      "14,69\n",
      "14,70\n",
      "14,71\n",
      "14,72\n",
      "14,73\n",
      "14,74\n",
      "14,75\n",
      "14,76\n",
      "14,77\n",
      "14,78\n",
      "14,79\n",
      "15,16\n",
      "15,17\n",
      "15,18\n",
      "15,19\n",
      "15,20\n",
      "15,21\n",
      "15,22\n",
      "15,23\n",
      "15,24\n",
      "15,25\n",
      "15,26\n",
      "15,27\n",
      "15,28\n",
      "15,29\n",
      "15,30\n",
      "15,31\n",
      "15,32\n",
      "15,33\n",
      "15,34\n",
      "15,35\n",
      "15,37\n",
      "15,38\n",
      "15,39\n",
      "15,40\n",
      "15,41\n",
      "15,42\n",
      "15,43\n",
      "15,44\n",
      "15,45\n",
      "15,46\n",
      "15,47\n",
      "15,48\n",
      "15,49\n",
      "15,50\n",
      "15,51\n",
      "15,52\n",
      "15,53\n",
      "15,54\n",
      "15,55\n",
      "15,56\n",
      "15,57\n",
      "15,58\n",
      "15,59\n",
      "15,60\n",
      "15,61\n",
      "15,62\n",
      "15,63\n",
      "15,64\n",
      "15,65\n",
      "15,66\n",
      "15,67\n",
      "15,68\n",
      "15,69\n",
      "15,70\n",
      "15,71\n",
      "15,72\n",
      "15,73\n",
      "15,74\n",
      "15,75\n",
      "15,76\n",
      "15,77\n",
      "15,78\n",
      "15,79\n",
      "16,17\n",
      "16,18\n",
      "16,19\n",
      "16,20\n",
      "16,21\n",
      "16,22\n",
      "16,23\n",
      "16,24\n",
      "16,25\n",
      "16,26\n",
      "16,27\n",
      "16,28\n",
      "16,29\n",
      "16,30\n",
      "16,31\n",
      "16,32\n",
      "16,33\n",
      "16,34\n",
      "16,35\n",
      "16,37\n",
      "16,38\n",
      "16,39\n",
      "16,40\n",
      "16,41\n",
      "16,42\n",
      "16,43\n",
      "16,44\n",
      "16,45\n",
      "16,46\n",
      "16,47\n",
      "16,48\n",
      "16,49\n",
      "16,50\n",
      "16,51\n",
      "16,52\n",
      "16,53\n",
      "16,54\n",
      "16,55\n",
      "16,56\n",
      "16,57\n",
      "16,58\n",
      "16,59\n",
      "16,60\n",
      "16,61\n",
      "16,62\n",
      "16,63\n",
      "16,64\n",
      "16,65\n",
      "16,66\n",
      "16,67\n",
      "16,68\n",
      "16,69\n",
      "16,70\n",
      "16,71\n",
      "16,72\n",
      "16,73\n",
      "16,74\n",
      "16,75\n",
      "16,76\n",
      "16,77\n",
      "16,78\n",
      "16,79\n",
      "17,18\n",
      "17,19\n",
      "17,20\n",
      "17,21\n",
      "17,22\n",
      "17,23\n",
      "17,24\n",
      "17,25\n",
      "17,26\n",
      "17,27\n",
      "17,28\n",
      "17,29\n",
      "17,30\n",
      "17,31\n",
      "17,32\n",
      "17,33\n",
      "17,34\n",
      "17,35\n",
      "17,37\n",
      "17,38\n",
      "17,39\n",
      "17,40\n",
      "17,41\n",
      "17,42\n",
      "17,43\n",
      "17,44\n",
      "17,45\n",
      "17,46\n",
      "17,47\n",
      "17,48\n",
      "17,49\n",
      "17,50\n",
      "17,51\n",
      "17,52\n",
      "17,53\n",
      "17,54\n",
      "17,55\n",
      "17,56\n",
      "17,57\n",
      "17,58\n",
      "17,59\n",
      "17,60\n",
      "17,61\n",
      "17,62\n",
      "17,63\n",
      "17,64\n",
      "17,65\n",
      "17,66\n",
      "17,67\n",
      "17,68\n",
      "17,69\n",
      "17,70\n",
      "17,71\n",
      "17,72\n",
      "17,73\n",
      "17,74\n",
      "17,75\n",
      "17,76\n",
      "17,77\n",
      "17,78\n",
      "17,79\n",
      "18,19\n",
      "18,20\n",
      "18,21\n",
      "18,22\n",
      "18,23\n",
      "18,24\n",
      "18,25\n",
      "18,26\n",
      "18,27\n",
      "18,28\n",
      "18,29\n",
      "18,30\n",
      "18,31\n",
      "18,32\n",
      "18,33\n",
      "18,34\n",
      "18,35\n",
      "18,37\n",
      "18,38\n",
      "18,39\n",
      "18,40\n",
      "18,41\n",
      "18,42\n",
      "18,43\n",
      "18,44\n",
      "18,45\n",
      "18,46\n",
      "18,47\n",
      "18,48\n",
      "18,49\n",
      "18,50\n",
      "18,51\n",
      "18,52\n",
      "18,53\n",
      "18,54\n",
      "18,55\n",
      "18,56\n",
      "18,57\n",
      "18,58\n",
      "18,59\n",
      "18,60\n",
      "18,61\n",
      "18,62\n",
      "18,63\n",
      "18,64\n",
      "18,65\n",
      "18,66\n",
      "18,67\n",
      "18,68\n",
      "18,69\n",
      "18,70\n",
      "18,71\n",
      "18,72\n",
      "18,73\n",
      "18,74\n",
      "18,75\n",
      "18,76\n",
      "18,77\n",
      "18,78\n",
      "18,79\n",
      "19,20\n",
      "19,21\n",
      "19,22\n",
      "19,23\n",
      "19,24\n",
      "19,25\n",
      "19,26\n",
      "19,27\n",
      "19,28\n",
      "19,29\n",
      "19,30\n",
      "19,31\n",
      "19,32\n",
      "19,33\n",
      "19,34\n",
      "19,35\n",
      "19,37\n",
      "19,38\n",
      "19,39\n",
      "19,40\n",
      "19,41\n",
      "19,42\n",
      "19,43\n",
      "19,44\n",
      "19,45\n",
      "19,46\n",
      "19,47\n",
      "19,48\n",
      "19,49\n",
      "19,50\n",
      "19,51\n",
      "19,52\n",
      "19,53\n",
      "19,54\n",
      "19,55\n",
      "19,56\n",
      "19,57\n",
      "19,58\n",
      "19,59\n",
      "19,60\n",
      "19,61\n",
      "19,62\n",
      "19,63\n",
      "19,64\n",
      "19,65\n",
      "19,66\n",
      "19,67\n",
      "19,68\n",
      "19,69\n",
      "19,70\n",
      "19,71\n",
      "19,72\n",
      "19,73\n",
      "19,74\n",
      "19,75\n",
      "19,76\n",
      "19,77\n",
      "19,78\n",
      "19,79\n",
      "20,21\n",
      "20,22\n",
      "20,23\n",
      "20,24\n",
      "20,25\n",
      "20,26\n",
      "20,27\n",
      "20,28\n",
      "20,29\n",
      "20,30\n",
      "20,31\n",
      "20,32\n",
      "20,33\n",
      "20,34\n",
      "20,35\n",
      "20,37\n",
      "20,38\n",
      "20,39\n",
      "20,40\n",
      "20,41\n",
      "20,42\n",
      "20,43\n",
      "20,44\n",
      "20,45\n",
      "20,46\n",
      "20,47\n",
      "20,48\n",
      "20,49\n",
      "20,50\n",
      "20,51\n",
      "20,52\n",
      "20,53\n",
      "20,54\n",
      "20,55\n",
      "20,56\n",
      "20,57\n",
      "20,58\n",
      "20,59\n",
      "20,60\n",
      "20,61\n",
      "20,62\n",
      "20,63\n",
      "20,64\n",
      "20,65\n",
      "20,66\n",
      "20,67\n",
      "20,68\n",
      "20,69\n",
      "20,70\n",
      "20,71\n",
      "20,72\n",
      "20,73\n",
      "20,74\n",
      "20,75\n",
      "20,76\n",
      "20,77\n",
      "20,78\n",
      "20,79\n",
      "21,22\n",
      "21,23\n",
      "21,24\n",
      "21,25\n",
      "21,26\n",
      "21,27\n",
      "21,28\n",
      "21,29\n",
      "21,30\n",
      "21,31\n",
      "21,32\n",
      "21,33\n",
      "21,34\n",
      "21,35\n",
      "21,37\n",
      "21,38\n",
      "21,39\n",
      "21,40\n",
      "21,41\n",
      "21,42\n",
      "21,43\n",
      "21,44\n",
      "21,45\n",
      "21,46\n",
      "21,47\n",
      "21,48\n",
      "21,49\n",
      "21,50\n",
      "21,51\n",
      "21,52\n",
      "21,53\n",
      "21,54\n",
      "21,55\n",
      "21,56\n",
      "21,57\n",
      "21,58\n",
      "21,59\n",
      "21,60\n",
      "21,61\n",
      "21,62\n",
      "21,63\n",
      "21,64\n",
      "21,65\n",
      "21,66\n",
      "21,67\n",
      "21,68\n",
      "21,69\n",
      "21,70\n",
      "21,71\n",
      "21,72\n",
      "21,73\n",
      "21,74\n",
      "21,75\n",
      "21,76\n",
      "21,77\n",
      "21,78\n",
      "21,79\n",
      "22,23\n",
      "22,24\n",
      "22,25\n",
      "22,26\n",
      "22,27\n",
      "22,28\n",
      "22,29\n",
      "22,30\n",
      "22,31\n",
      "22,32\n",
      "22,33\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22,34\n",
      "22,35\n",
      "22,37\n",
      "22,38\n",
      "22,39\n",
      "22,40\n",
      "22,41\n",
      "22,42\n",
      "22,43\n",
      "22,44\n",
      "22,45\n",
      "22,46\n",
      "22,47\n",
      "22,48\n",
      "22,49\n",
      "22,50\n",
      "22,51\n",
      "22,52\n",
      "22,53\n",
      "22,54\n",
      "22,55\n",
      "22,56\n",
      "22,57\n",
      "22,58\n",
      "22,59\n",
      "22,60\n",
      "22,61\n",
      "22,62\n",
      "22,63\n",
      "22,64\n",
      "22,65\n",
      "22,66\n",
      "22,67\n",
      "22,68\n",
      "22,69\n",
      "22,70\n",
      "22,71\n",
      "22,72\n",
      "22,73\n",
      "22,74\n",
      "22,75\n",
      "22,76\n",
      "22,77\n",
      "22,78\n",
      "22,79\n",
      "23,24\n",
      "23,25\n",
      "23,26\n",
      "23,27\n",
      "23,28\n",
      "23,29\n",
      "23,30\n",
      "23,31\n",
      "23,32\n",
      "23,33\n",
      "23,34\n",
      "23,35\n",
      "23,37\n",
      "23,38\n",
      "23,39\n",
      "23,40\n",
      "23,41\n",
      "23,42\n",
      "23,43\n",
      "23,44\n",
      "23,45\n",
      "23,46\n",
      "23,47\n",
      "23,48\n",
      "23,49\n",
      "23,50\n",
      "23,51\n",
      "23,52\n",
      "23,53\n",
      "23,54\n",
      "23,55\n",
      "23,56\n",
      "23,57\n",
      "23,58\n",
      "23,59\n",
      "23,60\n",
      "23,61\n",
      "23,62\n",
      "23,63\n",
      "23,64\n",
      "23,65\n",
      "23,66\n",
      "23,67\n",
      "23,68\n",
      "23,69\n",
      "23,70\n",
      "23,71\n",
      "23,72\n",
      "23,73\n",
      "23,74\n",
      "23,75\n",
      "23,76\n",
      "23,77\n",
      "23,78\n",
      "23,79\n",
      "24,25\n",
      "24,26\n",
      "24,27\n",
      "24,28\n",
      "24,29\n",
      "24,30\n",
      "24,31\n",
      "24,32\n",
      "24,33\n",
      "24,34\n",
      "24,35\n",
      "24,37\n",
      "24,38\n",
      "24,39\n",
      "24,40\n",
      "24,41\n",
      "24,42\n",
      "24,43\n",
      "24,44\n",
      "24,45\n",
      "24,46\n",
      "24,47\n",
      "24,48\n",
      "24,49\n",
      "24,50\n",
      "24,51\n",
      "24,52\n",
      "24,53\n",
      "24,54\n",
      "24,55\n",
      "24,56\n",
      "24,57\n",
      "24,58\n",
      "24,59\n",
      "24,60\n",
      "24,61\n",
      "24,62\n",
      "24,63\n",
      "24,64\n",
      "24,65\n",
      "24,66\n",
      "24,67\n",
      "24,68\n",
      "24,69\n",
      "24,70\n",
      "24,71\n",
      "24,72\n",
      "24,73\n",
      "24,74\n",
      "24,75\n",
      "24,76\n",
      "24,77\n",
      "24,78\n",
      "24,79\n",
      "25,26\n",
      "25,27\n",
      "25,28\n",
      "25,29\n",
      "25,30\n",
      "25,31\n",
      "25,32\n",
      "25,33\n",
      "25,34\n",
      "25,35\n",
      "25,37\n",
      "25,38\n",
      "25,39\n",
      "25,40\n",
      "25,41\n",
      "25,42\n",
      "25,43\n",
      "25,44\n",
      "25,45\n",
      "25,46\n",
      "25,47\n",
      "25,48\n",
      "25,49\n",
      "25,50\n",
      "25,51\n",
      "25,52\n",
      "25,53\n",
      "25,54\n",
      "25,55\n",
      "25,56\n",
      "25,57\n",
      "25,58\n",
      "25,59\n",
      "25,60\n",
      "25,61\n",
      "25,62\n",
      "25,63\n",
      "25,64\n",
      "25,65\n",
      "25,66\n",
      "25,67\n",
      "25,68\n",
      "25,69\n",
      "25,70\n",
      "25,71\n",
      "25,72\n",
      "25,73\n",
      "25,74\n",
      "25,75\n",
      "25,76\n",
      "25,77\n",
      "25,78\n",
      "25,79\n",
      "26,27\n",
      "26,28\n",
      "26,29\n",
      "26,30\n",
      "26,31\n",
      "26,32\n",
      "26,33\n",
      "26,34\n",
      "26,35\n",
      "26,37\n",
      "26,38\n",
      "26,39\n",
      "26,40\n",
      "26,41\n",
      "26,42\n",
      "26,43\n",
      "26,44\n",
      "26,45\n",
      "26,46\n",
      "26,47\n",
      "26,48\n",
      "26,49\n",
      "26,50\n",
      "26,51\n",
      "26,52\n",
      "26,53\n",
      "26,54\n",
      "26,55\n",
      "26,56\n",
      "26,57\n",
      "26,58\n",
      "26,59\n",
      "26,60\n",
      "26,61\n",
      "26,62\n",
      "26,63\n",
      "26,64\n",
      "26,65\n",
      "26,66\n",
      "26,67\n",
      "26,68\n",
      "26,69\n",
      "26,70\n",
      "26,71\n",
      "26,72\n",
      "26,73\n",
      "26,74\n",
      "26,75\n",
      "26,76\n",
      "26,77\n",
      "26,78\n",
      "26,79\n",
      "27,28\n",
      "27,29\n",
      "27,30\n",
      "27,31\n",
      "27,32\n",
      "27,33\n",
      "27,34\n",
      "27,35\n",
      "27,37\n",
      "27,38\n",
      "27,39\n",
      "27,40\n",
      "27,41\n",
      "27,42\n",
      "27,43\n",
      "27,44\n",
      "27,45\n",
      "27,46\n",
      "27,47\n",
      "27,48\n",
      "27,49\n",
      "27,50\n",
      "27,51\n",
      "27,52\n",
      "27,53\n",
      "27,54\n",
      "27,55\n",
      "27,56\n",
      "27,57\n",
      "27,58\n",
      "27,59\n",
      "27,60\n",
      "27,61\n",
      "27,62\n",
      "27,63\n",
      "27,64\n",
      "27,65\n",
      "27,66\n",
      "27,67\n",
      "27,68\n",
      "27,69\n",
      "27,70\n",
      "27,71\n",
      "27,72\n",
      "27,73\n",
      "27,74\n",
      "27,75\n",
      "27,76\n",
      "27,77\n",
      "27,78\n",
      "27,79\n",
      "28,29\n",
      "28,30\n",
      "28,31\n",
      "28,32\n",
      "28,33\n",
      "28,34\n",
      "28,35\n",
      "28,37\n",
      "28,38\n",
      "28,39\n",
      "28,40\n",
      "28,41\n",
      "28,42\n",
      "28,43\n",
      "28,44\n",
      "28,45\n",
      "28,46\n",
      "28,47\n",
      "28,48\n",
      "28,49\n",
      "28,50\n",
      "28,51\n",
      "28,52\n",
      "28,53\n",
      "28,54\n",
      "28,55\n",
      "28,56\n",
      "28,57\n",
      "28,58\n",
      "28,59\n",
      "28,60\n",
      "28,61\n",
      "28,62\n",
      "28,63\n",
      "28,64\n",
      "28,65\n",
      "28,66\n",
      "28,67\n",
      "28,68\n",
      "28,69\n",
      "28,70\n",
      "28,71\n",
      "28,72\n",
      "28,73\n",
      "28,74\n",
      "28,75\n",
      "28,76\n",
      "28,77\n",
      "28,78\n",
      "28,79\n",
      "29,30\n",
      "29,31\n",
      "29,32\n",
      "29,33\n",
      "29,34\n",
      "29,35\n",
      "29,37\n",
      "29,38\n",
      "29,39\n",
      "29,40\n",
      "29,41\n",
      "29,42\n",
      "29,43\n",
      "29,44\n",
      "29,45\n",
      "29,46\n",
      "29,47\n",
      "29,48\n",
      "29,49\n",
      "29,50\n",
      "29,51\n",
      "29,52\n",
      "29,53\n",
      "29,54\n",
      "29,55\n",
      "29,56\n",
      "29,57\n",
      "29,58\n",
      "29,59\n",
      "29,60\n",
      "29,61\n",
      "29,62\n",
      "29,63\n",
      "29,64\n",
      "29,65\n",
      "29,66\n",
      "29,67\n",
      "29,68\n",
      "29,69\n",
      "29,70\n",
      "29,71\n",
      "29,72\n",
      "29,73\n",
      "29,74\n",
      "29,75\n",
      "29,76\n",
      "29,77\n",
      "29,78\n",
      "29,79\n",
      "30,31\n",
      "30,32\n",
      "30,33\n",
      "30,34\n",
      "30,35\n",
      "30,37\n",
      "30,38\n",
      "30,39\n",
      "30,40\n",
      "30,41\n",
      "30,42\n",
      "30,43\n",
      "30,44\n",
      "30,45\n",
      "30,46\n",
      "30,47\n",
      "30,48\n",
      "30,49\n",
      "30,50\n",
      "30,51\n",
      "30,52\n",
      "30,53\n",
      "30,54\n",
      "30,55\n",
      "30,56\n",
      "30,57\n",
      "30,58\n",
      "30,59\n",
      "30,60\n",
      "30,61\n",
      "30,62\n",
      "30,63\n",
      "30,64\n",
      "30,65\n",
      "30,66\n",
      "30,67\n",
      "30,68\n",
      "30,69\n",
      "30,70\n",
      "30,71\n",
      "30,72\n",
      "30,73\n",
      "30,74\n",
      "30,75\n",
      "30,76\n",
      "30,77\n",
      "30,78\n",
      "30,79\n",
      "31,32\n",
      "31,33\n",
      "31,34\n",
      "31,35\n",
      "31,37\n",
      "31,38\n",
      "31,39\n",
      "31,40\n",
      "31,41\n",
      "31,42\n",
      "31,43\n",
      "31,44\n",
      "31,45\n",
      "31,46\n",
      "31,47\n",
      "31,48\n",
      "31,49\n",
      "31,50\n",
      "31,51\n",
      "31,52\n",
      "31,53\n",
      "31,54\n",
      "31,55\n",
      "31,56\n",
      "31,57\n",
      "31,58\n",
      "31,59\n",
      "31,60\n",
      "31,61\n",
      "31,62\n",
      "31,63\n",
      "31,64\n",
      "31,65\n",
      "31,66\n",
      "31,67\n",
      "31,68\n",
      "31,69\n",
      "31,70\n",
      "31,71\n",
      "31,72\n",
      "31,73\n",
      "31,74\n",
      "31,75\n",
      "31,76\n",
      "31,77\n",
      "31,78\n",
      "31,79\n",
      "32,33\n",
      "32,34\n",
      "32,35\n",
      "32,37\n",
      "32,38\n",
      "32,39\n",
      "32,40\n",
      "32,41\n",
      "32,42\n",
      "32,43\n",
      "32,44\n",
      "32,45\n",
      "32,46\n",
      "32,47\n",
      "32,48\n",
      "32,49\n",
      "32,50\n",
      "32,51\n",
      "32,52\n",
      "32,53\n",
      "32,54\n",
      "32,55\n",
      "32,56\n",
      "32,57\n",
      "32,58\n",
      "32,59\n",
      "32,60\n",
      "32,61\n",
      "32,62\n",
      "32,63\n",
      "32,64\n",
      "32,65\n",
      "32,66\n",
      "32,67\n",
      "32,68\n",
      "32,69\n",
      "32,70\n",
      "32,71\n",
      "32,72\n",
      "32,73\n",
      "32,74\n",
      "32,75\n",
      "32,76\n",
      "32,77\n",
      "32,78\n",
      "32,79\n",
      "33,34\n",
      "33,35\n",
      "33,37\n",
      "33,38\n",
      "33,39\n",
      "33,40\n",
      "33,41\n",
      "33,42\n",
      "33,43\n",
      "33,44\n",
      "33,45\n",
      "33,46\n",
      "33,47\n",
      "33,48\n",
      "33,49\n",
      "33,50\n",
      "33,51\n",
      "33,52\n",
      "33,53\n",
      "33,54\n",
      "33,55\n",
      "33,56\n",
      "33,57\n",
      "33,58\n",
      "33,59\n",
      "33,60\n",
      "33,61\n",
      "33,62\n",
      "33,63\n",
      "33,64\n",
      "33,65\n",
      "33,66\n",
      "33,67\n",
      "33,68\n",
      "33,69\n",
      "33,70\n",
      "33,71\n",
      "33,72\n",
      "33,73\n",
      "33,74\n",
      "33,75\n",
      "33,76\n",
      "33,77\n",
      "33,78\n",
      "33,79\n",
      "34,35\n",
      "34,37\n",
      "34,38\n",
      "34,39\n",
      "34,40\n",
      "34,41\n",
      "34,42\n",
      "34,43\n",
      "34,44\n",
      "34,45\n",
      "34,46\n",
      "34,47\n",
      "34,48\n",
      "34,49\n",
      "34,50\n",
      "34,51\n",
      "34,52\n",
      "34,53\n",
      "34,54\n",
      "34,55\n",
      "34,56\n",
      "34,57\n",
      "34,58\n",
      "34,59\n",
      "34,60\n",
      "34,61\n",
      "34,62\n",
      "34,63\n",
      "34,64\n",
      "34,65\n",
      "34,66\n",
      "34,67\n",
      "34,68\n",
      "34,69\n",
      "34,70\n",
      "34,71\n",
      "34,72\n",
      "34,73\n",
      "34,74\n",
      "34,75\n",
      "34,76\n",
      "34,77\n",
      "34,78\n",
      "34,79\n",
      "35,37\n",
      "35,38\n",
      "35,39\n",
      "35,40\n",
      "35,41\n",
      "35,42\n",
      "35,43\n",
      "35,44\n",
      "35,45\n",
      "35,46\n",
      "35,47\n",
      "35,48\n",
      "35,49\n",
      "35,50\n",
      "35,51\n",
      "35,52\n",
      "35,53\n",
      "35,54\n",
      "35,55\n",
      "35,56\n",
      "35,57\n",
      "35,58\n",
      "35,59\n",
      "35,60\n",
      "35,61\n",
      "35,62\n",
      "35,63\n",
      "35,64\n",
      "35,65\n",
      "35,66\n",
      "35,67\n",
      "35,68\n",
      "35,69\n",
      "35,70\n",
      "35,71\n",
      "35,72\n",
      "35,73\n",
      "35,74\n",
      "35,75\n",
      "35,76\n",
      "35,77\n",
      "35,78\n",
      "35,79\n",
      "37,38\n",
      "37,39\n",
      "37,40\n",
      "37,41\n",
      "37,42\n",
      "37,43\n",
      "37,44\n",
      "37,45\n",
      "37,46\n",
      "37,47\n",
      "37,48\n",
      "37,49\n",
      "37,50\n",
      "37,51\n",
      "37,52\n",
      "37,53\n",
      "37,54\n",
      "37,55\n",
      "37,56\n",
      "37,57\n",
      "37,58\n",
      "37,59\n",
      "37,60\n",
      "37,61\n",
      "37,62\n",
      "37,63\n",
      "37,64\n",
      "37,65\n",
      "37,66\n",
      "37,67\n",
      "37,68\n",
      "37,69\n",
      "37,70\n",
      "37,71\n",
      "37,72\n",
      "37,73\n",
      "37,74\n",
      "37,75\n",
      "37,76\n",
      "37,77\n",
      "37,78\n",
      "37,79\n",
      "38,39\n",
      "38,40\n",
      "38,41\n",
      "38,42\n",
      "38,43\n",
      "38,44\n",
      "38,45\n",
      "38,46\n",
      "38,47\n",
      "38,48\n",
      "38,49\n",
      "38,50\n",
      "38,51\n",
      "38,52\n",
      "38,53\n",
      "38,54\n",
      "38,55\n",
      "38,56\n",
      "38,57\n",
      "38,58\n",
      "38,59\n",
      "38,60\n",
      "38,61\n",
      "38,62\n",
      "38,63\n",
      "38,64\n",
      "38,65\n",
      "38,66\n",
      "38,67\n",
      "38,68\n",
      "38,69\n",
      "38,70\n",
      "38,71\n",
      "38,72\n",
      "38,73\n",
      "38,74\n",
      "38,75\n",
      "38,76\n",
      "38,77\n",
      "38,78\n",
      "38,79\n",
      "39,40\n",
      "39,41\n",
      "39,42\n",
      "39,43\n",
      "39,44\n",
      "39,45\n",
      "39,46\n",
      "39,47\n",
      "39,48\n",
      "39,49\n",
      "39,50\n",
      "39,51\n",
      "39,52\n",
      "39,53\n",
      "39,54\n",
      "39,55\n",
      "39,56\n",
      "39,57\n",
      "39,58\n",
      "39,59\n",
      "39,60\n",
      "39,61\n",
      "39,62\n",
      "39,63\n",
      "39,64\n",
      "39,65\n",
      "39,66\n",
      "39,67\n",
      "39,68\n",
      "39,69\n",
      "39,70\n",
      "39,71\n",
      "39,72\n",
      "39,73\n",
      "39,74\n",
      "39,75\n",
      "39,76\n",
      "39,77\n",
      "39,78\n",
      "39,79\n",
      "40,41\n",
      "40,42\n",
      "40,43\n",
      "40,44\n",
      "40,45\n",
      "40,46\n",
      "40,47\n",
      "40,48\n",
      "40,49\n",
      "40,50\n",
      "40,51\n",
      "40,52\n",
      "40,53\n",
      "40,54\n",
      "40,55\n",
      "40,56\n",
      "40,57\n",
      "40,58\n",
      "40,59\n",
      "40,60\n",
      "40,61\n",
      "40,62\n",
      "40,63\n",
      "40,64\n",
      "40,65\n",
      "40,66\n",
      "40,67\n",
      "40,68\n",
      "40,69\n",
      "40,70\n",
      "40,71\n",
      "40,72\n",
      "40,73\n",
      "40,74\n",
      "40,75\n",
      "40,76\n",
      "40,77\n",
      "40,78\n",
      "40,79\n",
      "41,42\n",
      "41,43\n",
      "41,44\n",
      "41,45\n",
      "41,46\n",
      "41,47\n",
      "41,48\n",
      "41,49\n",
      "41,50\n",
      "41,51\n",
      "41,52\n",
      "41,53\n",
      "41,54\n",
      "41,55\n",
      "41,56\n",
      "41,57\n",
      "41,58\n",
      "41,59\n",
      "41,60\n",
      "41,61\n",
      "41,62\n",
      "41,63\n",
      "41,64\n",
      "41,65\n",
      "41,66\n",
      "41,67\n",
      "41,68\n",
      "41,69\n",
      "41,70\n",
      "41,71\n",
      "41,72\n",
      "41,73\n",
      "41,74\n",
      "41,75\n",
      "41,76\n",
      "41,77\n",
      "41,78\n",
      "41,79\n",
      "42,43\n",
      "42,44\n",
      "42,45\n",
      "42,46\n",
      "42,47\n",
      "42,48\n",
      "42,49\n",
      "42,50\n",
      "42,51\n",
      "42,52\n",
      "42,53\n",
      "42,54\n",
      "42,55\n",
      "42,56\n",
      "42,57\n",
      "42,58\n",
      "42,59\n",
      "42,60\n",
      "42,61\n",
      "42,62\n",
      "42,63\n",
      "42,64\n",
      "42,65\n",
      "42,66\n",
      "42,67\n",
      "42,68\n",
      "42,69\n",
      "42,70\n",
      "42,71\n",
      "42,72\n",
      "42,73\n",
      "42,74\n",
      "42,75\n",
      "42,76\n",
      "42,77\n",
      "42,78\n",
      "42,79\n",
      "43,44\n",
      "43,45\n",
      "43,46\n",
      "43,47\n",
      "43,48\n",
      "43,49\n",
      "43,50\n",
      "43,51\n",
      "43,52\n",
      "43,53\n",
      "43,54\n",
      "43,55\n",
      "43,56\n",
      "43,57\n",
      "43,58\n",
      "43,59\n",
      "43,60\n",
      "43,61\n",
      "43,62\n",
      "43,63\n",
      "43,64\n",
      "43,65\n",
      "43,66\n",
      "43,67\n",
      "43,68\n",
      "43,69\n",
      "43,70\n",
      "43,71\n",
      "43,72\n",
      "43,73\n",
      "43,74\n",
      "43,75\n",
      "43,76\n",
      "43,77\n",
      "43,78\n",
      "43,79\n",
      "44,45\n",
      "44,46\n",
      "44,47\n",
      "44,48\n",
      "44,49\n",
      "44,50\n",
      "44,51\n",
      "44,52\n",
      "44,53\n",
      "44,54\n",
      "44,55\n",
      "44,56\n",
      "44,57\n",
      "44,58\n",
      "44,59\n",
      "44,60\n",
      "44,61\n",
      "44,62\n",
      "44,63\n",
      "44,64\n",
      "44,65\n",
      "44,66\n",
      "44,67\n",
      "44,68\n",
      "44,69\n",
      "44,70\n",
      "44,71\n",
      "44,72\n",
      "44,73\n",
      "44,74\n",
      "44,75\n",
      "44,76\n",
      "44,77\n",
      "44,78\n",
      "44,79\n",
      "45,46\n",
      "45,47\n",
      "45,48\n",
      "45,49\n",
      "45,50\n",
      "45,51\n",
      "45,52\n",
      "45,53\n",
      "45,54\n",
      "45,55\n",
      "45,56\n",
      "45,57\n",
      "45,58\n",
      "45,59\n",
      "45,60\n",
      "45,61\n",
      "45,62\n",
      "45,63\n",
      "45,64\n",
      "45,65\n",
      "45,66\n",
      "45,67\n",
      "45,68\n",
      "45,69\n",
      "45,70\n",
      "45,71\n",
      "45,72\n",
      "45,73\n",
      "45,74\n",
      "45,75\n",
      "45,76\n",
      "45,77\n",
      "45,78\n",
      "45,79\n",
      "46,47\n",
      "46,48\n",
      "46,49\n",
      "46,50\n",
      "46,51\n",
      "46,52\n",
      "46,53\n",
      "46,54\n",
      "46,55\n",
      "46,56\n",
      "46,57\n",
      "46,58\n",
      "46,59\n",
      "46,60\n",
      "46,61\n",
      "46,62\n",
      "46,63\n",
      "46,64\n",
      "46,65\n",
      "46,66\n",
      "46,67\n",
      "46,68\n",
      "46,69\n",
      "46,70\n",
      "46,71\n",
      "46,72\n",
      "46,73\n",
      "46,74\n",
      "46,75\n",
      "46,76\n",
      "46,77\n",
      "46,78\n",
      "46,79\n",
      "47,48\n",
      "47,49\n",
      "47,50\n",
      "47,51\n",
      "47,52\n",
      "47,53\n",
      "47,54\n",
      "47,55\n",
      "47,56\n",
      "47,57\n",
      "47,58\n",
      "47,59\n",
      "47,60\n",
      "47,61\n",
      "47,62\n",
      "47,63\n",
      "47,64\n",
      "47,65\n",
      "47,66\n",
      "47,67\n",
      "47,68\n",
      "47,69\n",
      "47,70\n",
      "47,71\n",
      "47,72\n",
      "47,73\n",
      "47,74\n",
      "47,75\n",
      "47,76\n",
      "47,77\n",
      "47,78\n",
      "47,79\n",
      "48,49\n",
      "48,50\n",
      "48,51\n",
      "48,52\n",
      "48,53\n",
      "48,54\n",
      "48,55\n",
      "48,56\n",
      "48,57\n",
      "48,58\n",
      "48,59\n",
      "48,60\n",
      "48,61\n",
      "48,62\n",
      "48,63\n",
      "48,64\n",
      "48,65\n",
      "48,66\n",
      "48,67\n",
      "48,68\n",
      "48,69\n",
      "48,70\n",
      "48,71\n",
      "48,72\n",
      "48,73\n",
      "48,74\n",
      "48,75\n",
      "48,76\n",
      "48,77\n",
      "48,78\n",
      "48,79\n",
      "49,50\n",
      "49,51\n",
      "49,52\n",
      "49,53\n",
      "49,54\n",
      "49,55\n",
      "49,56\n",
      "49,57\n",
      "49,58\n",
      "49,59\n",
      "49,60\n",
      "49,61\n",
      "49,62\n",
      "49,63\n",
      "49,64\n",
      "49,65\n",
      "49,66\n",
      "49,67\n",
      "49,68\n",
      "49,69\n",
      "49,70\n",
      "49,71\n",
      "49,72\n",
      "49,73\n",
      "49,74\n",
      "49,75\n",
      "49,76\n",
      "49,77\n",
      "49,78\n",
      "49,79\n",
      "50,51\n",
      "50,52\n",
      "50,53\n",
      "50,54\n",
      "50,55\n",
      "50,56\n",
      "50,57\n",
      "50,58\n",
      "50,59\n",
      "50,60\n",
      "50,61\n",
      "50,62\n",
      "50,63\n",
      "50,64\n",
      "50,65\n",
      "50,66\n",
      "50,67\n",
      "50,68\n",
      "50,69\n",
      "50,70\n",
      "50,71\n",
      "50,72\n",
      "50,73\n",
      "50,74\n",
      "50,75\n",
      "50,76\n",
      "50,77\n",
      "50,78\n",
      "50,79\n",
      "51,52\n",
      "51,53\n",
      "51,54\n",
      "51,55\n",
      "51,56\n",
      "51,57\n",
      "51,58\n",
      "51,59\n",
      "51,60\n",
      "51,61\n",
      "51,62\n",
      "51,63\n",
      "51,64\n",
      "51,65\n",
      "51,66\n",
      "51,67\n",
      "51,68\n",
      "51,69\n",
      "51,70\n",
      "51,71\n",
      "51,72\n",
      "51,73\n",
      "51,74\n",
      "51,75\n",
      "51,76\n",
      "51,77\n",
      "51,78\n",
      "51,79\n",
      "52,53\n",
      "52,54\n",
      "52,55\n",
      "52,56\n",
      "52,57\n",
      "52,58\n",
      "52,59\n",
      "52,60\n",
      "52,61\n",
      "52,62\n",
      "52,63\n",
      "52,64\n",
      "52,65\n",
      "52,66\n",
      "52,67\n",
      "52,68\n",
      "52,69\n",
      "52,70\n",
      "52,71\n",
      "52,72\n",
      "52,73\n",
      "52,74\n",
      "52,75\n",
      "52,76\n",
      "52,77\n",
      "52,78\n",
      "52,79\n",
      "53,54\n",
      "53,55\n",
      "53,56\n",
      "53,57\n",
      "53,58\n",
      "53,59\n",
      "53,60\n",
      "53,61\n",
      "53,62\n",
      "53,63\n",
      "53,64\n",
      "53,65\n",
      "53,66\n",
      "53,67\n",
      "53,68\n",
      "53,69\n",
      "53,70\n",
      "53,71\n",
      "53,72\n",
      "53,73\n",
      "53,74\n",
      "53,75\n",
      "53,76\n",
      "53,77\n",
      "53,78\n",
      "53,79\n",
      "54,55\n",
      "54,56\n",
      "54,57\n",
      "54,58\n",
      "54,59\n",
      "54,60\n",
      "54,61\n",
      "54,62\n",
      "54,63\n",
      "54,64\n",
      "54,65\n",
      "54,66\n",
      "54,67\n",
      "54,68\n",
      "54,69\n",
      "54,70\n",
      "54,71\n",
      "54,72\n",
      "54,73\n",
      "54,74\n",
      "54,75\n",
      "54,76\n",
      "54,77\n",
      "54,78\n",
      "54,79\n",
      "55,56\n",
      "55,57\n",
      "55,58\n",
      "55,59\n",
      "55,60\n",
      "55,61\n",
      "55,62\n",
      "55,63\n",
      "55,64\n",
      "55,65\n",
      "55,66\n",
      "55,67\n",
      "55,68\n",
      "55,69\n",
      "55,70\n",
      "55,71\n",
      "55,72\n",
      "55,73\n",
      "55,74\n",
      "55,75\n",
      "55,76\n",
      "55,77\n",
      "55,78\n",
      "55,79\n",
      "56,57\n",
      "56,58\n",
      "56,59\n",
      "56,60\n",
      "56,61\n",
      "56,62\n",
      "56,63\n",
      "56,64\n",
      "56,65\n",
      "56,66\n",
      "56,67\n",
      "56,68\n",
      "56,69\n",
      "56,70\n",
      "56,71\n",
      "56,72\n",
      "56,73\n",
      "56,74\n",
      "56,75\n",
      "56,76\n",
      "56,77\n",
      "56,78\n",
      "56,79\n",
      "57,58\n",
      "57,59\n",
      "57,60\n",
      "57,61\n",
      "57,62\n",
      "57,63\n",
      "57,64\n",
      "57,65\n",
      "57,66\n",
      "57,67\n",
      "57,68\n",
      "57,69\n",
      "57,70\n",
      "57,71\n",
      "57,72\n",
      "57,73\n",
      "57,74\n",
      "57,75\n",
      "57,76\n",
      "57,77\n",
      "57,78\n",
      "57,79\n",
      "58,59\n",
      "58,60\n",
      "58,61\n",
      "58,62\n",
      "58,63\n",
      "58,64\n",
      "58,65\n",
      "58,66\n",
      "58,67\n",
      "58,68\n",
      "58,69\n",
      "58,70\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "58,71\n",
      "58,72\n",
      "58,73\n",
      "58,74\n",
      "58,75\n",
      "58,76\n",
      "58,77\n",
      "58,78\n",
      "58,79\n",
      "59,60\n",
      "59,61\n",
      "59,62\n",
      "59,63\n",
      "59,64\n",
      "59,65\n",
      "59,66\n",
      "59,67\n",
      "59,68\n",
      "59,69\n",
      "59,70\n",
      "59,71\n",
      "59,72\n",
      "59,73\n",
      "59,74\n",
      "59,75\n",
      "59,76\n",
      "59,77\n",
      "59,78\n",
      "59,79\n",
      "60,61\n",
      "60,62\n",
      "60,63\n",
      "60,64\n",
      "60,65\n",
      "60,66\n",
      "60,67\n",
      "60,68\n",
      "60,69\n",
      "60,70\n",
      "60,71\n",
      "60,72\n",
      "60,73\n",
      "60,74\n",
      "60,75\n",
      "60,76\n",
      "60,77\n",
      "60,78\n",
      "60,79\n",
      "61,62\n",
      "61,63\n",
      "61,64\n",
      "61,65\n",
      "61,66\n",
      "61,67\n",
      "61,68\n",
      "61,69\n",
      "61,70\n",
      "61,71\n",
      "61,72\n",
      "61,73\n",
      "61,74\n",
      "61,75\n",
      "61,76\n",
      "61,77\n",
      "61,78\n",
      "61,79\n",
      "62,63\n",
      "62,64\n",
      "62,65\n",
      "62,66\n",
      "62,67\n",
      "62,68\n",
      "62,69\n",
      "62,70\n",
      "62,71\n",
      "62,72\n",
      "62,73\n",
      "62,74\n",
      "62,75\n",
      "62,76\n",
      "62,77\n",
      "62,78\n",
      "62,79\n",
      "63,64\n",
      "63,65\n",
      "63,66\n",
      "63,67\n",
      "63,68\n",
      "63,69\n",
      "63,70\n",
      "63,71\n",
      "63,72\n",
      "63,73\n",
      "63,74\n",
      "63,75\n",
      "63,76\n",
      "63,77\n",
      "63,78\n",
      "63,79\n",
      "64,65\n",
      "64,66\n",
      "64,67\n",
      "64,68\n",
      "64,69\n",
      "64,70\n",
      "64,71\n",
      "64,72\n",
      "64,73\n",
      "64,74\n",
      "64,75\n",
      "64,76\n",
      "64,77\n",
      "64,78\n",
      "64,79\n",
      "65,66\n",
      "65,67\n",
      "65,68\n",
      "65,69\n",
      "65,70\n",
      "65,71\n",
      "65,72\n",
      "65,73\n",
      "65,74\n",
      "65,75\n",
      "65,76\n",
      "65,77\n",
      "65,78\n",
      "65,79\n",
      "66,67\n",
      "66,68\n",
      "66,69\n",
      "66,70\n",
      "66,71\n",
      "66,72\n",
      "66,73\n",
      "66,74\n",
      "66,75\n",
      "66,76\n",
      "66,77\n",
      "66,78\n",
      "66,79\n",
      "67,68\n",
      "67,69\n",
      "67,70\n",
      "67,71\n",
      "67,72\n",
      "67,73\n",
      "67,74\n",
      "67,75\n",
      "67,76\n",
      "67,77\n",
      "67,78\n",
      "67,79\n",
      "68,69\n",
      "68,70\n",
      "68,71\n",
      "68,72\n",
      "68,73\n",
      "68,74\n",
      "68,75\n",
      "68,76\n",
      "68,77\n",
      "68,78\n",
      "68,79\n",
      "69,70\n",
      "69,71\n",
      "69,72\n",
      "69,73\n",
      "69,74\n",
      "69,75\n",
      "69,76\n",
      "69,77\n",
      "69,78\n",
      "69,79\n",
      "70,71\n",
      "70,72\n",
      "70,73\n",
      "70,74\n",
      "70,75\n",
      "70,76\n",
      "70,77\n",
      "70,78\n",
      "70,79\n",
      "71,72\n",
      "71,73\n",
      "71,74\n",
      "71,75\n",
      "71,76\n",
      "71,77\n",
      "71,78\n",
      "71,79\n",
      "72,73\n",
      "72,74\n",
      "72,75\n",
      "72,76\n",
      "72,77\n",
      "72,78\n",
      "72,79\n",
      "73,74\n",
      "73,75\n",
      "73,76\n",
      "73,77\n",
      "73,78\n",
      "73,79\n",
      "74,75\n",
      "74,76\n",
      "74,77\n",
      "74,78\n",
      "74,79\n",
      "75,76\n",
      "75,77\n",
      "75,78\n",
      "75,79\n",
      "76,77\n",
      "76,78\n",
      "76,79\n",
      "77,78\n",
      "77,79\n",
      "78,79\n"
     ]
    }
   ],
   "source": [
    "# ovo2\n",
    "all_rfs = {} \n",
    "all_predicts = [] \n",
    "sids = sorted(all_choose.keys()) \n",
    "for _i in range(len(sids)): \n",
    "    for _j in range(_i+1, len(sids)): \n",
    "        s1 = sids[_i] \n",
    "        s2 = sids[_j] \n",
    "        _indexs = list(set(all_choose[s1]).union(set(all_choose[s2]))) \n",
    "        _train_bool_index = (_train_b_y[:,s1] != 1) & (_train_b_y[:,s2]!=1) \n",
    "        ptrain_x = train_wifi_all_x[:,_indexs] \n",
    "        ptrain_y = train_y.copy()\n",
    "        ptrain_y[_train_bool_index] = -1 \n",
    "        ptrain_x = np.concatenate([ptrain_x, train_lonlats],axis=1)\n",
    "        rf2 = RandomForestClassifier(n_jobs=-1,n_estimators=122,class_weight=\"balanced\")\n",
    "        rf2.fit(ptrain_x,ptrain_y)\n",
    "        _key = str(s1) + \",\" + str(s2)\n",
    "        print _key\n",
    "        all_rfs[_key] = rf2\n",
    "\n",
    "        pvalid_x = valid_wifi_all_x[:,_indexs]\n",
    "        pvalid_x = np.concatenate([pvalid_x,valid_lonlats],axis=1)\n",
    "        _p = rf2.predict(pvalid_x)\n",
    "        all_predicts.append(_p)\n",
    "all_predict = np.vstack(all_predicts).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 0\n",
    "scores = np.zeros((len(valid_y),_train_b_y.shape[1]))\n",
    "base_score = 0.5\n",
    "for _i in range(len(sids)): \n",
    "    for _j in range(_i+1, len(sids)): \n",
    "        s1 = sids[_i] \n",
    "        s2 = sids[_j] \n",
    "        _p = all_predict[:,k]\n",
    "        for _k,_v in enumerate(_p):\n",
    "            if _v == _train_b_y.shape[1]:\n",
    "                scores[_k,:] += base_score\n",
    "                scores[_k,s1] -= base_score\n",
    "                scores[_k,s2] -= base_score\n",
    "            else:\n",
    "                scores[_k,_v]+=1\n",
    "        k += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9181614349775785"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc(np.argmax(scores,axis=1),valid_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_predict[all_predict==-1] =_train_b_y.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def amax(x):\n",
    "    l = np.bincount(x)\n",
    "    l[-1] = 0\n",
    "    return np.argmax(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9367713004484305"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc(np.asarray(map(lambda x:amax(x), all_predict)),valid_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0,1\n",
      "[1 0 0 ..., 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "sids = sorted(all_choose.keys())\n",
    "for _i in range(len(sids)):\n",
    "    for _j in range(_i+1, len(sids)):\n",
    "        s1 = sids[_i]\n",
    "        s2 = sids[_j]\n",
    "        _indexs = list(set(all_choose[s1]).union(set(all_choose[s2])))\n",
    "        _key = str(s1) + \",\" + str(s2)\n",
    "        print _key\n",
    "        \n",
    "        \n",
    "        break\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4460,)"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as ses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEKCAYAAAAfGVI8AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0xa437e950>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ses.countplot(x=\"dayofyear\",data=valid[last_p != valid_y])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABLoAAArOCAYAAACdFDTqAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0xa4a09150>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "f,ax = plt.subplots(1,1,figsize=(20,50))\n",
    "ses.countplot(y=\"shop_id\",data=valid[last_p != valid_y],ax=ax)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_times = train[[\"weekday\",\"hour\",\"is_weekend\"]].values\n",
    "valid_times = valid[[\"weekday\",\"hour\",\"is_weekend\"]].values\n",
    "train_connect_wifi = (train.basic_wifi_info.map(lambda x: len(x[1])).values > 0).astype(int).reshape(-1,1)\n",
    "valid_connect_wifi = (valid.basic_wifi_info.map(lambda x: len(x[1])).values > 0).astype(int).reshape(-1,1)\n",
    "train_search_wifi_size = train.basic_wifi_info.map(lambda x: x[0]).values.reshape(-1,1)\n",
    "valid_search_wifi_size = valid.basic_wifi_info.map(lambda x: x[0]).values.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9, 2, 7, 3, 13, 4]\n",
      "[2, 3, 6, 7, 71, 9]\n"
     ]
    }
   ],
   "source": [
    "print all_choose[55]\n",
    "print all_choose[52]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([223, 222, 231, 235, 229, 236, 221, 225, 226, 216, 232, 228, 234,\n",
       "       213, 220, 215, 233, 224, 214, 227, 218, 217, 219, 230])"
      ]
     },
     "execution_count": 362,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 560,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_search(estimetor, trainx,trainy,fix_trainx,validx,validy,fix_validx):\n",
    "    choose = []\n",
    "    best_acc=0\n",
    "    cc = 0\n",
    "    while cc != -1:\n",
    "        cc = -1\n",
    "        for _i in range(trainx.shape[1]):\n",
    "            if _i not in choose:\n",
    "                curr = choose + [_i]\n",
    "                ptrainx = np.concatenate([trainx[:,curr], fix_trainx],axis=1)\n",
    "                pvalidx = np.concatenate([validx[:,curr], fix_validx],axis=1)\n",
    "                m.fit(ptrainx,trainy)\n",
    "                _acc = acc(m.predict(pvalidx),validy)\n",
    "                if _acc > best_acc:\n",
    "                    best_acc = _acc\n",
    "                    cc = _i\n",
    "        if cc != -1:\n",
    "            print \"choose\", cc\n",
    "            print \"best acc\", best_acc\n",
    "            choose.append(cc)\n",
    "        if best_acc == 1:\n",
    "            break\n",
    "    return choose\n",
    "                \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 565,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s1 ['s_490980']\n",
      "s2 ['s_509630']\n",
      "\n",
      "s1 train shape (78, 25)\n",
      "s2 train shape (180, 25)\n",
      "s1 valid shape (17, 26)\n",
      "s2 valid shape (44, 26)\n",
      "wifi indexs [67, 72, 140, 301, 239, 80, 59, 156, 109]\n",
      "wifi size: 9\n",
      "train acc 1.0\n",
      "(61,)\n",
      "valid acc 1.0\n",
      "feature importance [(0.24324205081054456, 109), (0.18236084974348118, 67), (0.17181308268530457, 156), (0.1584750358011954, 72), (0.11842862518398974, 'lon'), (0.052231985759119144, 'lat'), (0.026494825868536478, 140), (0.018105457971091168, 301), (0.013134065260949778, 239), (0.012579927355214926, 80), (0.0031340935605730872, 59)]\n",
      "choose 0\n",
      "best acc 1.0\n",
      "[109, 67, 156, 72, 140]\n",
      "train acc 1.0\n",
      "(61,)\n",
      "valid acc 1.0\n"
     ]
    }
   ],
   "source": [
    "s1 = 44\n",
    "print \"s1\", le.inverse_transform([s1])\n",
    "s2 = 56\n",
    "print \"s2\", le.inverse_transform([s2])\n",
    "print\n",
    "s1_train = train[_train_b_y[:,s1] == 1]\n",
    "# s1_train = s1_train[s1_train.dayofyear >= 231]\n",
    "s2_train = train[_train_b_y[:,s2] == 1]\n",
    "s1_valid = valid[_valid_b_y[:,s1] == 1]\n",
    "s2_valid = valid[_valid_b_y[:,s2] == 1]\n",
    "print \"s1 train shape\",s1_train.shape\n",
    "print \"s2 train shape\",s2_train.shape\n",
    "print \"s1 valid shape\",s1_valid.shape\n",
    "print \"s2 valid shape\",s2_valid.shape\n",
    "\n",
    "s1_wifi_all_x = train_wifi_all_x[_train_b_y[:,s1] == 1]\n",
    "s2_wifi_all_x = train_wifi_all_x[_train_b_y[:,s2] == 1]\n",
    "s1_indexs = choose_strong_wifi_index(-115,6,s1_wifi_all_x)\n",
    "s2_indexs = choose_strong_wifi_index(-115,6,s2_wifi_all_x)\n",
    "_indexs = list(set(s1_indexs).union(set(s2_indexs)))\n",
    "# _indexs = list(set(all_choose[s1]).union(set(all_choose[s2])))\n",
    "# _indexs = [10,8,74]\n",
    "print \"wifi indexs\",_indexs\n",
    "print \"wifi size:\",len(_indexs)\n",
    "_train_bool_index = (_train_b_y[:,s1] == 1) | (_train_b_y[:,s2]==1)\n",
    "_valid_bool_index = (_valid_b_y[:,s1] == 1) | (_valid_b_y[:,s2]==1)\n",
    "ptrain_x = train_wifi_all_x[_train_bool_index][:,_indexs]\n",
    "ptrain_y = train_y[_train_bool_index]\n",
    "pvalid_x = valid_wifi_all_x[_valid_bool_index][:,_indexs]\n",
    "pvalid_y = valid_y[_valid_bool_index]\n",
    "pvalid = valid[_valid_bool_index]\n",
    "\n",
    "ptrain_x = np.concatenate([ptrain_x, \n",
    "                           train_lonlats[_train_bool_index],\n",
    "                          ],axis=1)\n",
    "\n",
    "pvalid_x = np.concatenate([pvalid_x,\n",
    "                           valid_lonlats[_valid_bool_index],\n",
    "                          ], axis=1)\n",
    "\n",
    "_indexs.append(\"lon\")\n",
    "_indexs.append(\"lat\")\n",
    "m = RandomForestClassifier(n_jobs=-1,\n",
    "                           n_estimators=188,\n",
    "#                            min_samples_leaf=2\n",
    "#                            min_samples_split=5,\n",
    "                           class_weight=\"balanced\",\n",
    "#                            random_state=2017\n",
    "                          )\n",
    "\n",
    "\n",
    "m.fit(ptrain_x,ptrain_y)\n",
    "p1 = m.predict(ptrain_x)\n",
    "print \"train acc\", acc(p1,ptrain_y)\n",
    "p2 = m.predict(pvalid_x)\n",
    "print p2.shape\n",
    "print \"valid acc\", acc(p2,pvalid_y)\n",
    "\n",
    "if hasattr(m,\"feature_importances_\"):\n",
    "    fi = zip(m.feature_importances_, _indexs)\n",
    "    fi = sorted(fi,key=lambda x:-x[0])\n",
    "    print \"feature importance\",fi[:30]\n",
    "    find_indexs = []\n",
    "    for _f in fi[:30]:\n",
    "        if isinstance(_f[1],int):\n",
    "            find_indexs.append(_f[1])\n",
    "    choose = forward_search(m,\n",
    "                   train_wifi_all_x[_train_bool_index][:,find_indexs],\n",
    "                   train_y[_train_bool_index],\n",
    "                   train_lonlats[_train_bool_index],\n",
    "                   valid_wifi_all_x[_valid_bool_index][:,find_indexs],\n",
    "                   valid_y[_valid_bool_index],\n",
    "                   valid_lonlats[_valid_bool_index]\n",
    "                  )\n",
    "    choose = [find_indexs[_c] for _c in choose]\n",
    "    min_wifi_index_num = 5\n",
    "    while len(choose) < min_wifi_index_num:\n",
    "        for _index in find_indexs:\n",
    "            if _index not in choose:\n",
    "                choose.append(_index)\n",
    "            if len(choose) >= min_wifi_index_num:\n",
    "                break\n",
    "    print choose\n",
    "    \n",
    "ptrain_x = np.concatenate([train_wifi_all_x[_train_bool_index][:,choose], \n",
    "                           train_lonlats[_train_bool_index],\n",
    "                          ],axis=1)\n",
    "pvalid_x = np.concatenate([valid_wifi_all_x[_valid_bool_index][:,choose], \n",
    "                           valid_lonlats[_valid_bool_index],\n",
    "                          ],axis=1)\n",
    "\n",
    "m.fit(ptrain_x,ptrain_y)\n",
    "p1 = m.predict(ptrain_x)\n",
    "print \"train acc\", acc(p1,ptrain_y)\n",
    "p2 = m.predict(pvalid_x)\n",
    "print p2.shape\n",
    "print \"valid acc\", acc(p2,pvalid_y)\n",
    "# pvalid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>shop_id</th>\n",
       "      <th>time_stamp</th>\n",
       "      <th>longitude</th>\n",
       "      <th>latitude</th>\n",
       "      <th>wifi_infos</th>\n",
       "      <th>category_id</th>\n",
       "      <th>shop_longitude</th>\n",
       "      <th>shop_latitude</th>\n",
       "      <th>price</th>\n",
       "      <th>...</th>\n",
       "      <th>basic_wifi_info</th>\n",
       "      <th>wifi_size</th>\n",
       "      <th>use_wifi_size</th>\n",
       "      <th>no_use_wifi_size</th>\n",
       "      <th>use_wifi_freq</th>\n",
       "      <th>no_use_wifi_freq</th>\n",
       "      <th>i_loc</th>\n",
       "      <th>dayofyear</th>\n",
       "      <th>minute</th>\n",
       "      <th>hour_minute</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2374</th>\n",
       "      <td>u_383311</td>\n",
       "      <td>s_3726779</td>\n",
       "      <td>2017-08-11 18:00</td>\n",
       "      <td>117.256029</td>\n",
       "      <td>36.899741</td>\n",
       "      <td>b_52168829|-78|false;b_26467949|-73|false;b_17079182|-74|false;b_40209888|-54|false;b_44415092|-78|false;b_51726487|-75|false;b_52048604|-76|false;b_44200910|-81|false;b_39574079|-79|false;b_51048975|-77|false</td>\n",
       "      <td>c_28</td>\n",
       "      <td>117.25607</td>\n",
       "      <td>36.899736</td>\n",
       "      <td>40</td>\n",
       "      <td>...</td>\n",
       "      <td>(10, [], [(b_52168829, -78), (b_26467949, -73), (b_17079182, -74), (b_40209888, -54), (b_44415092, -78), (b_51726487, -75), (b_52048604, -76), (b_44200910, -81), (b_39574079, -79), (b_51048975, -77)])</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>14</td>\n",
       "      <td>223</td>\n",
       "      <td>0</td>\n",
       "      <td>1080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3217</th>\n",
       "      <td>u_523659</td>\n",
       "      <td>s_3726779</td>\n",
       "      <td>2017-08-10 20:10</td>\n",
       "      <td>117.256025</td>\n",
       "      <td>36.899719</td>\n",
       "      <td>b_40362685|-86|false;b_17079184|-87|false;b_51726487|-86|false;b_48793119|-90|false;b_40209888|-68|false;b_15638003|-77|false;b_49278725|-92|false;b_15638004|-87|false;b_52048604|-87|false;b_51271474|-83|false</td>\n",
       "      <td>c_28</td>\n",
       "      <td>117.25607</td>\n",
       "      <td>36.899736</td>\n",
       "      <td>40</td>\n",
       "      <td>...</td>\n",
       "      <td>(10, [], [(b_40362685, -86), (b_17079184, -87), (b_51726487, -86), (b_48793119, -90), (b_40209888, -68), (b_15638003, -77), (b_49278725, -92), (b_15638004, -87), (b_52048604, -87), (b_51271474, -83)])</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>27</td>\n",
       "      <td>222</td>\n",
       "      <td>10</td>\n",
       "      <td>1210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5512</th>\n",
       "      <td>u_900254</td>\n",
       "      <td>s_3726779</td>\n",
       "      <td>2017-08-19 20:40</td>\n",
       "      <td>117.256053</td>\n",
       "      <td>36.899719</td>\n",
       "      <td>b_51048975|-75|false;b_12705141|-75|false;b_49278725|-76|false;b_51726487|-68|false;b_40209888|-65|false;b_22472878|-74|false;b_52025240|-81|false;b_40362685|-79|false;b_53340470|-66|false;b_44415092|-75|false</td>\n",
       "      <td>c_28</td>\n",
       "      <td>117.25607</td>\n",
       "      <td>36.899736</td>\n",
       "      <td>40</td>\n",
       "      <td>...</td>\n",
       "      <td>(10, [], [(b_51048975, -75), (b_12705141, -75), (b_49278725, -76), (b_51726487, -68), (b_40209888, -65), (b_22472878, -74), (b_52025240, -81), (b_40362685, -79), (b_53340470, -66), (b_44415092, -75)])</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>81</td>\n",
       "      <td>231</td>\n",
       "      <td>40</td>\n",
       "      <td>1240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5603</th>\n",
       "      <td>u_918067</td>\n",
       "      <td>s_3726779</td>\n",
       "      <td>2017-08-23 18:40</td>\n",
       "      <td>117.256061</td>\n",
       "      <td>36.899698</td>\n",
       "      <td>b_38366281|-78|false;b_51726487|-73|false;b_17079184|-78|false;b_40059953|-80|false;b_30559295|-77|false;b_55016265|-76|false;b_40209888|-52|false;b_17079183|-78|false;b_15638003|-79|false;b_44415092|-77|false</td>\n",
       "      <td>c_28</td>\n",
       "      <td>117.25607</td>\n",
       "      <td>36.899736</td>\n",
       "      <td>40</td>\n",
       "      <td>...</td>\n",
       "      <td>(10, [], [(b_38366281, -78), (b_51726487, -73), (b_17079184, -78), (b_40059953, -80), (b_30559295, -77), (b_55016265, -76), (b_40209888, -52), (b_17079183, -78), (b_15638003, -79), (b_44415092, -77)])</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>83</td>\n",
       "      <td>235</td>\n",
       "      <td>40</td>\n",
       "      <td>1120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6358</th>\n",
       "      <td>u_1045586</td>\n",
       "      <td>s_3726779</td>\n",
       "      <td>2017-08-17 20:30</td>\n",
       "      <td>117.256027</td>\n",
       "      <td>36.899709</td>\n",
       "      <td>b_15638003|-83|false;b_48793119|-81|false;b_44200910|-86|false;b_49278725|-83|false;b_55289296|-84|false;b_39574079|-86|false;b_51726487|-76|false;b_28660854|-86|false;b_22472878|-76|false;b_40209888|-59|false</td>\n",
       "      <td>c_28</td>\n",
       "      <td>117.25607</td>\n",
       "      <td>36.899736</td>\n",
       "      <td>40</td>\n",
       "      <td>...</td>\n",
       "      <td>(10, [], [(b_15638003, -83), (b_48793119, -81), (b_44200910, -86), (b_49278725, -83), (b_55289296, -84), (b_39574079, -86), (b_51726487, -76), (b_28660854, -86), (b_22472878, -76), (b_40209888, -59)])</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>92</td>\n",
       "      <td>229</td>\n",
       "      <td>30</td>\n",
       "      <td>1230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7692</th>\n",
       "      <td>u_1263622</td>\n",
       "      <td>s_3726779</td>\n",
       "      <td>2017-08-10 20:20</td>\n",
       "      <td>117.256092</td>\n",
       "      <td>36.899723</td>\n",
       "      <td>b_49278725|-76|false;b_48793119|-78|false;b_40209888|-57|false;b_26467980|-71|false;b_51271474|-69|false;b_12705141|-76|false;b_30379445|-80|false;b_52048604|-67|false;b_17079182|-84|false;b_53667045|-87|false</td>\n",
       "      <td>c_28</td>\n",
       "      <td>117.25607</td>\n",
       "      <td>36.899736</td>\n",
       "      <td>40</td>\n",
       "      <td>...</td>\n",
       "      <td>(10, [], [(b_49278725, -76), (b_48793119, -78), (b_40209888, -57), (b_26467980, -71), (b_51271474, -69), (b_12705141, -76), (b_30379445, -80), (b_52048604, -67), (b_17079182, -84), (b_53667045, -87)])</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>98</td>\n",
       "      <td>222</td>\n",
       "      <td>20</td>\n",
       "      <td>1220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7919</th>\n",
       "      <td>u_1297990</td>\n",
       "      <td>s_3726779</td>\n",
       "      <td>2017-08-24 21:10</td>\n",
       "      <td>117.256005</td>\n",
       "      <td>36.899658</td>\n",
       "      <td>b_40059953|-86|false;b_26467980|-80|false;b_40209888|-61|false;b_51726487|-72|false;b_52048604|-79|false;b_52048607|-84|false;b_39574079|-80|false;b_40362685|-78|false</td>\n",
       "      <td>c_28</td>\n",
       "      <td>117.25607</td>\n",
       "      <td>36.899736</td>\n",
       "      <td>40</td>\n",
       "      <td>...</td>\n",
       "      <td>(8, [], [(b_40059953, -86), (b_26467980, -80), (b_40209888, -61), (b_51726487, -72), (b_52048604, -79), (b_52048607, -84), (b_39574079, -80), (b_40362685, -78)])</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>101</td>\n",
       "      <td>236</td>\n",
       "      <td>10</td>\n",
       "      <td>1270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15241</th>\n",
       "      <td>u_2603632</td>\n",
       "      <td>s_3726779</td>\n",
       "      <td>2017-08-09 18:10</td>\n",
       "      <td>117.255977</td>\n",
       "      <td>36.899716</td>\n",
       "      <td>b_51726487|-65|false;b_44322551|-80|false;b_26467949|-77|false;b_30118357|-86|false;b_44434668|-83|false;b_52048604|-77|false;b_52048607|-78|false;b_51048975|-84|false;b_40362685|-84|false;b_40209888|-61|false</td>\n",
       "      <td>c_28</td>\n",
       "      <td>117.25607</td>\n",
       "      <td>36.899736</td>\n",
       "      <td>40</td>\n",
       "      <td>...</td>\n",
       "      <td>(10, [], [(b_51726487, -65), (b_44322551, -80), (b_26467949, -77), (b_30118357, -86), (b_44434668, -83), (b_52048604, -77), (b_52048607, -78), (b_51048975, -84), (b_40362685, -84), (b_40209888, -61)])</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>176</td>\n",
       "      <td>221</td>\n",
       "      <td>10</td>\n",
       "      <td>1090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15242</th>\n",
       "      <td>u_2603632</td>\n",
       "      <td>s_3726779</td>\n",
       "      <td>2017-08-17 11:50</td>\n",
       "      <td>117.256020</td>\n",
       "      <td>36.899713</td>\n",
       "      <td>b_51726487|-68|false;b_40209888|-60|false;b_52048607|-69|false;b_14580929|-83|false;b_17079184|-81|false;b_30379445|-78|false;b_44415092|-75|false;b_49278725|-80|false;b_51048975|-74|false;b_22472878|-77|false</td>\n",
       "      <td>c_28</td>\n",
       "      <td>117.25607</td>\n",
       "      <td>36.899736</td>\n",
       "      <td>40</td>\n",
       "      <td>...</td>\n",
       "      <td>(10, [], [(b_51726487, -68), (b_40209888, -60), (b_52048607, -69), (b_14580929, -83), (b_17079184, -81), (b_30379445, -78), (b_44415092, -75), (b_49278725, -80), (b_51048975, -74), (b_22472878, -77)])</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>177</td>\n",
       "      <td>229</td>\n",
       "      <td>50</td>\n",
       "      <td>710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16201</th>\n",
       "      <td>u_2798769</td>\n",
       "      <td>s_3726779</td>\n",
       "      <td>2017-08-13 13:50</td>\n",
       "      <td>117.256050</td>\n",
       "      <td>36.899714</td>\n",
       "      <td>b_26467980|-70|false;b_30379445|-88|false;b_48793119|-93|false;b_51726487|-80|false;b_52048607|-85|false;b_39574079|-83|false;b_44415092|-73|false;b_49278725|-90|false;b_40209888|-72|false;b_12705141|-87|false</td>\n",
       "      <td>c_28</td>\n",
       "      <td>117.25607</td>\n",
       "      <td>36.899736</td>\n",
       "      <td>40</td>\n",
       "      <td>...</td>\n",
       "      <td>(10, [], [(b_26467980, -70), (b_30379445, -88), (b_48793119, -93), (b_51726487, -80), (b_52048607, -85), (b_39574079, -83), (b_44415092, -73), (b_49278725, -90), (b_40209888, -72), (b_12705141, -87)])</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>183</td>\n",
       "      <td>225</td>\n",
       "      <td>50</td>\n",
       "      <td>830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17102</th>\n",
       "      <td>u_2959695</td>\n",
       "      <td>s_3726779</td>\n",
       "      <td>2017-08-17 20:40</td>\n",
       "      <td>117.256042</td>\n",
       "      <td>36.899703</td>\n",
       "      <td>b_51726487|-73|false;b_3046873|-70|false;b_17079184|-81|false;b_40209888|-49|false;b_52048604|-64|false;b_17079183|-82|false;b_55016265|-80|false;b_48249102|-77|false;b_44415092|-76|false;b_30559295|-76|false</td>\n",
       "      <td>c_28</td>\n",
       "      <td>117.25607</td>\n",
       "      <td>36.899736</td>\n",
       "      <td>40</td>\n",
       "      <td>...</td>\n",
       "      <td>(10, [], [(b_51726487, -73), (b_3046873, -70), (b_17079184, -81), (b_40209888, -49), (b_52048604, -64), (b_17079183, -82), (b_55016265, -80), (b_48249102, -77), (b_44415092, -76), (b_30559295, -76)])</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>197</td>\n",
       "      <td>229</td>\n",
       "      <td>40</td>\n",
       "      <td>1240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17267</th>\n",
       "      <td>u_2991298</td>\n",
       "      <td>s_3726779</td>\n",
       "      <td>2017-08-14 13:20</td>\n",
       "      <td>117.256038</td>\n",
       "      <td>36.899714</td>\n",
       "      <td>b_39574079|-77|false;b_40362685|-77|false;b_51145806|-81|false;b_52048607|-71|false;b_40209888|-56|false;b_51726487|-71|false;b_31112294|-81|false;b_52025241|-80|false;b_26613990|-85|false;b_22472878|-71|false</td>\n",
       "      <td>c_28</td>\n",
       "      <td>117.25607</td>\n",
       "      <td>36.899736</td>\n",
       "      <td>40</td>\n",
       "      <td>...</td>\n",
       "      <td>(10, [], [(b_39574079, -77), (b_40362685, -77), (b_51145806, -81), (b_52048607, -71), (b_40209888, -56), (b_51726487, -71), (b_31112294, -81), (b_52025241, -80), (b_26613990, -85), (b_22472878, -71)])</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>200</td>\n",
       "      <td>226</td>\n",
       "      <td>20</td>\n",
       "      <td>800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19018</th>\n",
       "      <td>u_3347503</td>\n",
       "      <td>s_3726779</td>\n",
       "      <td>2017-08-24 19:40</td>\n",
       "      <td>117.256037</td>\n",
       "      <td>36.899731</td>\n",
       "      <td>b_51726487|-77|false;b_48793119|-79|false;b_40209888|-61|false;b_40059953|-83|false;b_17079183|-86|false;b_44434668|-86|false;b_52048604|-88|false;b_44415092|-78|false;b_49278725|-78|false;b_52048607|-85|false</td>\n",
       "      <td>c_28</td>\n",
       "      <td>117.25607</td>\n",
       "      <td>36.899736</td>\n",
       "      <td>40</td>\n",
       "      <td>...</td>\n",
       "      <td>(10, [], [(b_51726487, -77), (b_48793119, -79), (b_40209888, -61), (b_40059953, -83), (b_17079183, -86), (b_44434668, -86), (b_52048604, -88), (b_44415092, -78), (b_49278725, -78), (b_52048607, -85)])</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>214</td>\n",
       "      <td>236</td>\n",
       "      <td>40</td>\n",
       "      <td>1180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22419</th>\n",
       "      <td>u_4005712</td>\n",
       "      <td>s_3726779</td>\n",
       "      <td>2017-08-04 20:10</td>\n",
       "      <td>117.256024</td>\n",
       "      <td>36.899730</td>\n",
       "      <td>b_20874926|-78|false;b_52048605|-83|false;b_52048607|-77|false;b_40209888|-60|false;b_51726487|-79|false;b_3114529|-76|false;b_28660854|-76|false;b_26467949|-82|false;b_51048975|-74|false;b_52048604|-78|false</td>\n",
       "      <td>c_28</td>\n",
       "      <td>117.25607</td>\n",
       "      <td>36.899736</td>\n",
       "      <td>40</td>\n",
       "      <td>...</td>\n",
       "      <td>(10, [], [(b_20874926, -78), (b_52048605, -83), (b_52048607, -77), (b_40209888, -60), (b_51726487, -79), (b_3114529, -76), (b_28660854, -76), (b_26467949, -82), (b_51048975, -74), (b_52048604, -78)])</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>253</td>\n",
       "      <td>216</td>\n",
       "      <td>10</td>\n",
       "      <td>1210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24196</th>\n",
       "      <td>u_4404917</td>\n",
       "      <td>s_3726779</td>\n",
       "      <td>2017-08-17 20:10</td>\n",
       "      <td>117.255965</td>\n",
       "      <td>36.899722</td>\n",
       "      <td>b_53667045|-89|false;b_40209888|-57|false;b_44200910|-86|false;b_49148817|-89|false;b_30379445|-79|false;b_44415092|-81|false;b_51726487|-76|false;b_15638004|-90|false;b_15638003|-82|false;b_22472878|-81|false</td>\n",
       "      <td>c_28</td>\n",
       "      <td>117.25607</td>\n",
       "      <td>36.899736</td>\n",
       "      <td>40</td>\n",
       "      <td>...</td>\n",
       "      <td>(10, [], [(b_53667045, -89), (b_40209888, -57), (b_44200910, -86), (b_49148817, -89), (b_30379445, -79), (b_44415092, -81), (b_51726487, -76), (b_15638004, -90), (b_15638003, -82), (b_22472878, -81)])</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>288</td>\n",
       "      <td>229</td>\n",
       "      <td>10</td>\n",
       "      <td>1210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45444</th>\n",
       "      <td>u_10665203</td>\n",
       "      <td>s_3726779</td>\n",
       "      <td>2017-08-19 15:20</td>\n",
       "      <td>117.256011</td>\n",
       "      <td>36.899715</td>\n",
       "      <td>b_51726487|-80|false;b_28660854|-78|false;b_31112294|-85|false;b_51048975|-71|false;b_40209888|-61|false;b_52048604|-68|false;b_44415092|-79|false;b_22472878|-74|false;b_52048607|-79|false;b_40362685|-71|false</td>\n",
       "      <td>c_28</td>\n",
       "      <td>117.25607</td>\n",
       "      <td>36.899736</td>\n",
       "      <td>40</td>\n",
       "      <td>...</td>\n",
       "      <td>(10, [], [(b_51726487, -80), (b_28660854, -78), (b_31112294, -85), (b_51048975, -71), (b_40209888, -61), (b_52048604, -68), (b_44415092, -79), (b_22472878, -74), (b_52048607, -79), (b_40362685, -71)])</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>679</td>\n",
       "      <td>231</td>\n",
       "      <td>20</td>\n",
       "      <td>920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46295</th>\n",
       "      <td>u_10842859</td>\n",
       "      <td>s_3726779</td>\n",
       "      <td>2017-08-20 12:00</td>\n",
       "      <td>117.256064</td>\n",
       "      <td>36.899701</td>\n",
       "      <td>b_39574079|-73|false;b_12705141|-77|false;b_55289296|-81|false;b_52048604|-70|false;b_49278725|-76|false;b_48793119|-78|false;b_51726487|-76|false;b_9025795|-85|false;b_40209888|-55|false;b_52048607|-77|false</td>\n",
       "      <td>c_28</td>\n",
       "      <td>117.25607</td>\n",
       "      <td>36.899736</td>\n",
       "      <td>40</td>\n",
       "      <td>...</td>\n",
       "      <td>(10, [], [(b_39574079, -73), (b_12705141, -77), (b_55289296, -81), (b_52048604, -70), (b_49278725, -76), (b_48793119, -78), (b_51726487, -76), (b_9025795, -85), (b_40209888, -55), (b_52048607, -77)])</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>690</td>\n",
       "      <td>232</td>\n",
       "      <td>0</td>\n",
       "      <td>720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48004</th>\n",
       "      <td>u_11193921</td>\n",
       "      <td>s_3726779</td>\n",
       "      <td>2017-08-16 19:20</td>\n",
       "      <td>117.256072</td>\n",
       "      <td>36.899742</td>\n",
       "      <td>b_48793119|-79|false;b_22472878|-72|false;b_40059953|-80|false;b_40209888|-59|false;b_52048604|-75|false;b_44415092|-79|false;b_51726487|-76|false;b_49278725|-79|false;b_55289296|-81|false;b_26467980|-79|false</td>\n",
       "      <td>c_28</td>\n",
       "      <td>117.25607</td>\n",
       "      <td>36.899736</td>\n",
       "      <td>40</td>\n",
       "      <td>...</td>\n",
       "      <td>(10, [], [(b_48793119, -79), (b_22472878, -72), (b_40059953, -80), (b_40209888, -59), (b_52048604, -75), (b_44415092, -79), (b_51726487, -76), (b_49278725, -79), (b_55289296, -81), (b_26467980, -79)])</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>731</td>\n",
       "      <td>228</td>\n",
       "      <td>20</td>\n",
       "      <td>1160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56501</th>\n",
       "      <td>u_13109059</td>\n",
       "      <td>s_3726779</td>\n",
       "      <td>2017-08-24 20:20</td>\n",
       "      <td>117.256044</td>\n",
       "      <td>36.899707</td>\n",
       "      <td>b_39574079|-90|false;b_55289296|-79|false;b_40209888|-66|false;b_51726487|-87|false;b_30379445|-81|false;b_22472878|-85|false;b_49278725|-91|false;b_52048604|-87|false;b_48793119|-89|false;b_49148816|-87|false</td>\n",
       "      <td>c_28</td>\n",
       "      <td>117.25607</td>\n",
       "      <td>36.899736</td>\n",
       "      <td>40</td>\n",
       "      <td>...</td>\n",
       "      <td>(10, [], [(b_39574079, -90), (b_55289296, -79), (b_40209888, -66), (b_51726487, -87), (b_30379445, -81), (b_22472878, -85), (b_49278725, -91), (b_52048604, -87), (b_48793119, -89), (b_49148816, -87)])</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>841</td>\n",
       "      <td>236</td>\n",
       "      <td>20</td>\n",
       "      <td>1220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57408</th>\n",
       "      <td>u_13371404</td>\n",
       "      <td>s_3726779</td>\n",
       "      <td>2017-08-22 21:00</td>\n",
       "      <td>117.256056</td>\n",
       "      <td>36.899699</td>\n",
       "      <td>b_22472878|-81|false;b_40059952|-78|false;b_44415092|-81|false;b_55289296|-91|false;b_40059953|-77|false;b_52048604|-73|false;b_26467980|-79|false;b_40209888|-73|false;b_30379445|-88|false;b_51726487|-80|false</td>\n",
       "      <td>c_28</td>\n",
       "      <td>117.25607</td>\n",
       "      <td>36.899736</td>\n",
       "      <td>40</td>\n",
       "      <td>...</td>\n",
       "      <td>(10, [], [(b_22472878, -81), (b_40059952, -78), (b_44415092, -81), (b_55289296, -91), (b_40059953, -77), (b_52048604, -73), (b_26467980, -79), (b_40209888, -73), (b_30379445, -88), (b_51726487, -80)])</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>848</td>\n",
       "      <td>234</td>\n",
       "      <td>0</td>\n",
       "      <td>1260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69517</th>\n",
       "      <td>u_17056510</td>\n",
       "      <td>s_3726779</td>\n",
       "      <td>2017-08-13 18:10</td>\n",
       "      <td>117.256048</td>\n",
       "      <td>36.899721</td>\n",
       "      <td>b_51726487|-64|false;b_44415092|-73|false;b_26467980|-74|false;b_30379445|-64|false;b_52048604|-63|false;b_40209888|-48|false;b_55289296|-63|false;b_22472878|-66|false;b_12705141|-71|false;b_49148817|-88|false</td>\n",
       "      <td>c_28</td>\n",
       "      <td>117.25607</td>\n",
       "      <td>36.899736</td>\n",
       "      <td>40</td>\n",
       "      <td>...</td>\n",
       "      <td>(10, [], [(b_51726487, -64), (b_44415092, -73), (b_26467980, -74), (b_30379445, -64), (b_52048604, -63), (b_40209888, -48), (b_55289296, -63), (b_22472878, -66), (b_12705141, -71), (b_49148817, -88)])</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1017</td>\n",
       "      <td>225</td>\n",
       "      <td>10</td>\n",
       "      <td>1090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73730</th>\n",
       "      <td>u_18366431</td>\n",
       "      <td>s_3726779</td>\n",
       "      <td>2017-08-14 12:10</td>\n",
       "      <td>117.256055</td>\n",
       "      <td>36.899719</td>\n",
       "      <td>b_22472878|-76|false;b_39574079|-78|false;b_12705141|-69|false;b_30379445|-76|false;b_52025241|-82|false;b_14580929|-76|false;b_49148817|-84|false;b_51726487|-79|false;b_55289296|-76|false;b_40209888|-58|false</td>\n",
       "      <td>c_28</td>\n",
       "      <td>117.25607</td>\n",
       "      <td>36.899736</td>\n",
       "      <td>40</td>\n",
       "      <td>...</td>\n",
       "      <td>(10, [], [(b_22472878, -76), (b_39574079, -78), (b_12705141, -69), (b_30379445, -76), (b_52025241, -82), (b_14580929, -76), (b_49148817, -84), (b_51726487, -79), (b_55289296, -76), (b_40209888, -58)])</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1087</td>\n",
       "      <td>226</td>\n",
       "      <td>10</td>\n",
       "      <td>730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73966</th>\n",
       "      <td>u_18428121</td>\n",
       "      <td>s_3726779</td>\n",
       "      <td>2017-08-09 12:30</td>\n",
       "      <td>117.256044</td>\n",
       "      <td>36.899713</td>\n",
       "      <td>b_49148817|-86|false;b_55289296|-83|false;b_30379445|-82|false;b_40209888|-58|false;b_44434668|-82|false;b_44200910|-80|false;b_38366281|-71|false;b_17079182|-88|false;b_52048604|-68|false;b_51726487|-74|false</td>\n",
       "      <td>c_28</td>\n",
       "      <td>117.25607</td>\n",
       "      <td>36.899736</td>\n",
       "      <td>40</td>\n",
       "      <td>...</td>\n",
       "      <td>(10, [], [(b_49148817, -86), (b_55289296, -83), (b_30379445, -82), (b_40209888, -58), (b_44434668, -82), (b_44200910, -80), (b_38366281, -71), (b_17079182, -88), (b_52048604, -68), (b_51726487, -74)])</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1092</td>\n",
       "      <td>221</td>\n",
       "      <td>30</td>\n",
       "      <td>750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74309</th>\n",
       "      <td>u_18492188</td>\n",
       "      <td>s_3726779</td>\n",
       "      <td>2017-08-01 19:00</td>\n",
       "      <td>117.255977</td>\n",
       "      <td>36.899728</td>\n",
       "      <td>b_15638003|-88|false;b_26468126|-91|false;b_40209888|-65|false;b_34893889|-82|false;b_51048975|-81|false;b_44434668|-84|false;b_48793119|-82|false</td>\n",
       "      <td>c_28</td>\n",
       "      <td>117.25607</td>\n",
       "      <td>36.899736</td>\n",
       "      <td>40</td>\n",
       "      <td>...</td>\n",
       "      <td>(7, [], [(b_15638003, -88), (b_26468126, -91), (b_40209888, -65), (b_34893889, -82), (b_51048975, -81), (b_44434668, -84), (b_48793119, -82)])</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1098</td>\n",
       "      <td>213</td>\n",
       "      <td>0</td>\n",
       "      <td>1140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83993</th>\n",
       "      <td>u_20963816</td>\n",
       "      <td>s_3726779</td>\n",
       "      <td>2017-08-20 18:30</td>\n",
       "      <td>117.256016</td>\n",
       "      <td>36.899759</td>\n",
       "      <td>b_12705141|-84|false;b_40059953|-86|false;b_48793119|-91|false;b_26467980|-86|false;b_52025240|-87|false;b_40209888|-63|false;b_44415092|-88|false;b_51726487|-74|false;b_40059952|-85|false;b_48249102|-92|false</td>\n",
       "      <td>c_28</td>\n",
       "      <td>117.25607</td>\n",
       "      <td>36.899736</td>\n",
       "      <td>40</td>\n",
       "      <td>...</td>\n",
       "      <td>(10, [], [(b_12705141, -84), (b_40059953, -86), (b_48793119, -91), (b_26467980, -86), (b_52025240, -87), (b_40209888, -63), (b_44415092, -88), (b_51726487, -74), (b_40059952, -85), (b_48249102, -92)])</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1229</td>\n",
       "      <td>232</td>\n",
       "      <td>30</td>\n",
       "      <td>1110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88117</th>\n",
       "      <td>u_22269170</td>\n",
       "      <td>s_3726779</td>\n",
       "      <td>2017-08-20 21:00</td>\n",
       "      <td>117.256073</td>\n",
       "      <td>36.899744</td>\n",
       "      <td>b_52048604|-60|false;b_52025240|-76|false;b_44200910|-72|false;b_40059953|-76|false;b_40209888|-58|false;b_40059952|-70|false;b_17079183|-67|false;b_38300990|-74|false;b_52048607|-75|false;b_51726487|-66|false</td>\n",
       "      <td>c_28</td>\n",
       "      <td>117.25607</td>\n",
       "      <td>36.899736</td>\n",
       "      <td>40</td>\n",
       "      <td>...</td>\n",
       "      <td>(10, [], [(b_52048604, -60), (b_52025240, -76), (b_44200910, -72), (b_40059953, -76), (b_40209888, -58), (b_40059952, -70), (b_17079183, -67), (b_38300990, -74), (b_52048607, -75), (b_51726487, -66)])</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1286</td>\n",
       "      <td>232</td>\n",
       "      <td>0</td>\n",
       "      <td>1260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92536</th>\n",
       "      <td>u_23639090</td>\n",
       "      <td>s_3726779</td>\n",
       "      <td>2017-08-08 20:20</td>\n",
       "      <td>117.256001</td>\n",
       "      <td>36.899724</td>\n",
       "      <td>b_55016265|-88|false;b_40362685|-83|false;b_30379445|-88|false;b_40209888|-67|false;b_51726487|-86|false;b_51271474|-86|false;b_52048607|-85|false;b_48249102|-86|false;b_49278725|-84|false;b_48793119|-82|false</td>\n",
       "      <td>c_28</td>\n",
       "      <td>117.25607</td>\n",
       "      <td>36.899736</td>\n",
       "      <td>40</td>\n",
       "      <td>...</td>\n",
       "      <td>(10, [], [(b_55016265, -88), (b_40362685, -83), (b_30379445, -88), (b_40209888, -67), (b_51726487, -86), (b_51271474, -86), (b_52048607, -85), (b_48249102, -86), (b_49278725, -84), (b_48793119, -82)])</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1341</td>\n",
       "      <td>220</td>\n",
       "      <td>20</td>\n",
       "      <td>1220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94712</th>\n",
       "      <td>u_24267847</td>\n",
       "      <td>s_3726779</td>\n",
       "      <td>2017-08-24 22:10</td>\n",
       "      <td>117.256054</td>\n",
       "      <td>36.899686</td>\n",
       "      <td>b_23895926|-86|false;b_55289296|-65|false;b_51726487|-72|false;b_52048604|-74|false;b_52048607|-72|false;b_44415092|-74|false;b_40059952|-87|false;b_19770055|-88|false;b_30379445|-69|false;b_40209888|-60|false</td>\n",
       "      <td>c_28</td>\n",
       "      <td>117.25607</td>\n",
       "      <td>36.899736</td>\n",
       "      <td>40</td>\n",
       "      <td>...</td>\n",
       "      <td>(10, [], [(b_23895926, -86), (b_55289296, -65), (b_51726487, -72), (b_52048604, -74), (b_52048607, -72), (b_44415092, -74), (b_40059952, -87), (b_19770055, -88), (b_30379445, -69), (b_40209888, -60)])</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1370</td>\n",
       "      <td>236</td>\n",
       "      <td>10</td>\n",
       "      <td>1330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99169</th>\n",
       "      <td>u_25320118</td>\n",
       "      <td>s_3726779</td>\n",
       "      <td>2017-08-10 10:50</td>\n",
       "      <td>117.255980</td>\n",
       "      <td>36.899720</td>\n",
       "      <td>b_26467981|-92|false;b_55312507|-80|false;b_52048599|-91|false;b_48793119|-84|false;b_40209888|-68|false;b_51726487|-79|false;b_40362685|-79|false;b_51048975|-77|false;b_50640451|-86|false;b_53667047|-90|false</td>\n",
       "      <td>c_28</td>\n",
       "      <td>117.25607</td>\n",
       "      <td>36.899736</td>\n",
       "      <td>40</td>\n",
       "      <td>...</td>\n",
       "      <td>(10, [], [(b_26467981, -92), (b_55312507, -80), (b_52048599, -91), (b_48793119, -84), (b_40209888, -68), (b_51726487, -79), (b_40362685, -79), (b_51048975, -77), (b_50640451, -86), (b_53667047, -90)])</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1405</td>\n",
       "      <td>222</td>\n",
       "      <td>50</td>\n",
       "      <td>650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99597</th>\n",
       "      <td>u_25413152</td>\n",
       "      <td>s_3726779</td>\n",
       "      <td>2017-08-20 13:30</td>\n",
       "      <td>117.256024</td>\n",
       "      <td>36.899741</td>\n",
       "      <td>b_40209888|-63|false;b_52048607|-77|false;b_44415092|-81|false;b_48793119|-86|false;b_51726487|-81|false;b_3918679|-81|false;b_49278725|-86|false;b_48249102|-84|false;b_40059952|-84|false;b_40059953|-78|false</td>\n",
       "      <td>c_28</td>\n",
       "      <td>117.25607</td>\n",
       "      <td>36.899736</td>\n",
       "      <td>40</td>\n",
       "      <td>...</td>\n",
       "      <td>(10, [], [(b_40209888, -63), (b_52048607, -77), (b_44415092, -81), (b_48793119, -86), (b_51726487, -81), (b_3918679, -81), (b_49278725, -86), (b_48249102, -84), (b_40059952, -84), (b_40059953, -78)])</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1415</td>\n",
       "      <td>232</td>\n",
       "      <td>30</td>\n",
       "      <td>810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1058701</th>\n",
       "      <td>u_40262017</td>\n",
       "      <td>s_3726779</td>\n",
       "      <td>2017-08-05 13:00</td>\n",
       "      <td>117.256021</td>\n",
       "      <td>36.899724</td>\n",
       "      <td>b_22472878|-83|false;b_17079183|-86|false;b_40209888|-60|false;b_51726487|-75|false;b_52048607|-79|false;b_40362685|-84|false;b_52025241|-89|false;b_52048604|-80|false;b_17079184|-85|false;b_51048975|-87|false</td>\n",
       "      <td>c_28</td>\n",
       "      <td>117.25607</td>\n",
       "      <td>36.899736</td>\n",
       "      <td>40</td>\n",
       "      <td>...</td>\n",
       "      <td>(10, [], [(b_22472878, -83), (b_17079183, -86), (b_40209888, -60), (b_51726487, -75), (b_52048607, -79), (b_40362685, -84), (b_52025241, -89), (b_52048604, -80), (b_17079184, -85), (b_51048975, -87)])</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>15336</td>\n",
       "      <td>217</td>\n",
       "      <td>0</td>\n",
       "      <td>780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1069387</th>\n",
       "      <td>u_43850085</td>\n",
       "      <td>s_3726779</td>\n",
       "      <td>2017-08-15 17:30</td>\n",
       "      <td>117.256071</td>\n",
       "      <td>36.899687</td>\n",
       "      <td>b_22472878|-71|false;b_52048604|-74|false;b_51726487|-77|false;b_26467980|-81|false;b_40209888|-63|false;b_55289296|-85|false;b_44415092|-83|false;b_30379445|-88|false;b_40059952|-74|false;b_52048607|-79|false</td>\n",
       "      <td>c_28</td>\n",
       "      <td>117.25607</td>\n",
       "      <td>36.899736</td>\n",
       "      <td>40</td>\n",
       "      <td>...</td>\n",
       "      <td>(10, [], [(b_22472878, -71), (b_52048604, -74), (b_51726487, -77), (b_26467980, -81), (b_40209888, -63), (b_55289296, -85), (b_44415092, -83), (b_30379445, -88), (b_40059952, -74), (b_52048607, -79)])</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>15499</td>\n",
       "      <td>227</td>\n",
       "      <td>30</td>\n",
       "      <td>1050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1070075</th>\n",
       "      <td>u_44095801</td>\n",
       "      <td>s_3726779</td>\n",
       "      <td>2017-08-12 10:30</td>\n",
       "      <td>117.256036</td>\n",
       "      <td>36.899722</td>\n",
       "      <td>b_55289296|-75|false;b_44415092|-64|false;b_40209888|-58|false;b_40362685|-81|false;b_51726487|-69|false;b_30379445|-73|false;b_26467981|-85|false;b_39574079|-72|false;b_22472878|-74|false;b_12705141|-74|false</td>\n",
       "      <td>c_28</td>\n",
       "      <td>117.25607</td>\n",
       "      <td>36.899736</td>\n",
       "      <td>40</td>\n",
       "      <td>...</td>\n",
       "      <td>(10, [], [(b_55289296, -75), (b_44415092, -64), (b_40209888, -58), (b_40362685, -81), (b_51726487, -69), (b_30379445, -73), (b_26467981, -85), (b_39574079, -72), (b_22472878, -74), (b_12705141, -74)])</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>15503</td>\n",
       "      <td>224</td>\n",
       "      <td>30</td>\n",
       "      <td>630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1075865</th>\n",
       "      <td>u_45568463</td>\n",
       "      <td>s_3726779</td>\n",
       "      <td>2017-08-11 21:20</td>\n",
       "      <td>117.256026</td>\n",
       "      <td>36.899722</td>\n",
       "      <td>b_33962055|-85|false;b_52048604|-74|false;b_28660854|-76|false;b_55289296|-78|false;b_51726487|-75|false;b_30118357|-86|false;b_44415092|-72|false;b_14580929|-72|false;b_40209888|-60|false;b_30379445|-78|false</td>\n",
       "      <td>c_28</td>\n",
       "      <td>117.25607</td>\n",
       "      <td>36.899736</td>\n",
       "      <td>40</td>\n",
       "      <td>...</td>\n",
       "      <td>(10, [], [(b_33962055, -85), (b_52048604, -74), (b_28660854, -76), (b_55289296, -78), (b_51726487, -75), (b_30118357, -86), (b_44415092, -72), (b_14580929, -72), (b_40209888, -60), (b_30379445, -78)])</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>15574</td>\n",
       "      <td>223</td>\n",
       "      <td>20</td>\n",
       "      <td>1280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1076618</th>\n",
       "      <td>u_45787430</td>\n",
       "      <td>s_3726779</td>\n",
       "      <td>2017-08-11 19:00</td>\n",
       "      <td>117.255956</td>\n",
       "      <td>36.899728</td>\n",
       "      <td>b_15638003|-84|false;b_42415389|-81|false;b_12705141|-80|false;b_49278725|-86|false;b_52048604|-84|false;b_44415092|-88|false;b_51726487|-77|false;b_40209888|-56|false;b_53667047|-88|false;b_53667045|-89|false</td>\n",
       "      <td>c_28</td>\n",
       "      <td>117.25607</td>\n",
       "      <td>36.899736</td>\n",
       "      <td>40</td>\n",
       "      <td>...</td>\n",
       "      <td>(10, [], [(b_15638003, -84), (b_42415389, -81), (b_12705141, -80), (b_49278725, -86), (b_52048604, -84), (b_44415092, -88), (b_51726487, -77), (b_40209888, -56), (b_53667047, -88), (b_53667045, -89)])</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>15586</td>\n",
       "      <td>223</td>\n",
       "      <td>0</td>\n",
       "      <td>1140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1082274</th>\n",
       "      <td>u_47288216</td>\n",
       "      <td>s_3726779</td>\n",
       "      <td>2017-08-04 16:10</td>\n",
       "      <td>117.256001</td>\n",
       "      <td>36.899742</td>\n",
       "      <td>b_44434668|-91|false;b_48793119|-84|false;b_52048604|-84|false;b_40209888|-62|false</td>\n",
       "      <td>c_28</td>\n",
       "      <td>117.25607</td>\n",
       "      <td>36.899736</td>\n",
       "      <td>40</td>\n",
       "      <td>...</td>\n",
       "      <td>(4, [], [(b_44434668, -91), (b_48793119, -84), (b_52048604, -84), (b_40209888, -62)])</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>15674</td>\n",
       "      <td>216</td>\n",
       "      <td>10</td>\n",
       "      <td>970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1083580</th>\n",
       "      <td>u_47645769</td>\n",
       "      <td>s_3726779</td>\n",
       "      <td>2017-08-16 20:30</td>\n",
       "      <td>117.256056</td>\n",
       "      <td>36.899712</td>\n",
       "      <td>b_51696066|-82|false;b_51726487|-72|false;b_52048607|-88|false;b_52048604|-76|false;b_40209888|-64|false;b_22472878|-77|false;b_48793119|-79|false;b_49278725|-79|false;b_40059953|-75|false;b_26467949|-82|false</td>\n",
       "      <td>c_28</td>\n",
       "      <td>117.25607</td>\n",
       "      <td>36.899736</td>\n",
       "      <td>40</td>\n",
       "      <td>...</td>\n",
       "      <td>(10, [], [(b_51696066, -82), (b_51726487, -72), (b_52048607, -88), (b_52048604, -76), (b_40209888, -64), (b_22472878, -77), (b_48793119, -79), (b_49278725, -79), (b_40059953, -75), (b_26467949, -82)])</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>15688</td>\n",
       "      <td>228</td>\n",
       "      <td>30</td>\n",
       "      <td>1230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1083967</th>\n",
       "      <td>u_47756473</td>\n",
       "      <td>s_3726779</td>\n",
       "      <td>2017-08-12 21:20</td>\n",
       "      <td>117.255982</td>\n",
       "      <td>36.899734</td>\n",
       "      <td>b_26467980|-82|false;b_30379445|-81|false;b_52048607|-73|false;b_22472878|-83|false;b_44959978|-58|false;b_50640451|-80|false;b_40362685|-72|false;b_51726487|-78|false;b_40209888|-55|false;b_44415092|-75|false</td>\n",
       "      <td>c_28</td>\n",
       "      <td>117.25607</td>\n",
       "      <td>36.899736</td>\n",
       "      <td>40</td>\n",
       "      <td>...</td>\n",
       "      <td>(10, [], [(b_26467980, -82), (b_30379445, -81), (b_52048607, -73), (b_22472878, -83), (b_44959978, -58), (b_50640451, -80), (b_40362685, -72), (b_51726487, -78), (b_40209888, -55), (b_44415092, -75)])</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>15691</td>\n",
       "      <td>224</td>\n",
       "      <td>20</td>\n",
       "      <td>1280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1086135</th>\n",
       "      <td>u_48308349</td>\n",
       "      <td>s_3726779</td>\n",
       "      <td>2017-08-17 18:10</td>\n",
       "      <td>117.256032</td>\n",
       "      <td>36.899757</td>\n",
       "      <td>b_51726487|-70|false;b_40209888|-52|false;b_44200910|-77|false;b_44415092|-72|false;b_22472878|-69|false;b_40059953|-77|false;b_40059952|-70|false;b_26467980|-78|false;b_55016265|-72|false;b_28660854|-77|false</td>\n",
       "      <td>c_28</td>\n",
       "      <td>117.25607</td>\n",
       "      <td>36.899736</td>\n",
       "      <td>40</td>\n",
       "      <td>...</td>\n",
       "      <td>(10, [], [(b_51726487, -70), (b_40209888, -52), (b_44200910, -77), (b_44415092, -72), (b_22472878, -69), (b_40059953, -77), (b_40059952, -70), (b_26467980, -78), (b_55016265, -72), (b_28660854, -77)])</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>15732</td>\n",
       "      <td>229</td>\n",
       "      <td>10</td>\n",
       "      <td>1090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1089420</th>\n",
       "      <td>u_49215617</td>\n",
       "      <td>s_3726779</td>\n",
       "      <td>2017-08-14 13:10</td>\n",
       "      <td>117.256060</td>\n",
       "      <td>36.899721</td>\n",
       "      <td>b_49148817|-92|false;b_51726487|-73|false;b_55289296|-83|false;b_52048604|-66|false;b_40059953|-88|false;b_44415092|-85|false;b_53667045|-93|false;b_22472878|-73|false;b_40209888|-58|false;b_49278725|-84|false</td>\n",
       "      <td>c_28</td>\n",
       "      <td>117.25607</td>\n",
       "      <td>36.899736</td>\n",
       "      <td>40</td>\n",
       "      <td>...</td>\n",
       "      <td>(10, [], [(b_49148817, -92), (b_51726487, -73), (b_55289296, -83), (b_52048604, -66), (b_40059953, -88), (b_44415092, -85), (b_53667045, -93), (b_22472878, -73), (b_40209888, -58), (b_49278725, -84)])</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>15791</td>\n",
       "      <td>226</td>\n",
       "      <td>10</td>\n",
       "      <td>790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1092357</th>\n",
       "      <td>u_50180800</td>\n",
       "      <td>s_3726779</td>\n",
       "      <td>2017-08-24 19:20</td>\n",
       "      <td>117.256034</td>\n",
       "      <td>36.899707</td>\n",
       "      <td>b_28660854|-79|false;b_22472878|-77|false;b_12705141|-80|false;b_44874709|-78|false;b_52048604|-77|false;b_49278725|-77|false;b_51726487|-77|false;b_48793119|-80|false;b_55289296|-80|false;b_40209888|-51|false</td>\n",
       "      <td>c_28</td>\n",
       "      <td>117.25607</td>\n",
       "      <td>36.899736</td>\n",
       "      <td>40</td>\n",
       "      <td>...</td>\n",
       "      <td>(10, [], [(b_28660854, -79), (b_22472878, -77), (b_12705141, -80), (b_44874709, -78), (b_52048604, -77), (b_49278725, -77), (b_51726487, -77), (b_48793119, -80), (b_55289296, -80), (b_40209888, -51)])</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>15825</td>\n",
       "      <td>236</td>\n",
       "      <td>20</td>\n",
       "      <td>1160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1094189</th>\n",
       "      <td>u_50710973</td>\n",
       "      <td>s_3726779</td>\n",
       "      <td>2017-08-04 15:40</td>\n",
       "      <td>117.256025</td>\n",
       "      <td>36.899704</td>\n",
       "      <td>b_53667045|-87|false;b_22472878|-74|false;b_49278725|-78|false;b_52048607|-71|false;b_51271474|-68|false;b_26467949|-77|false;b_53667051|-88|false;b_48793119|-78|false;b_52048604|-71|false;b_40209888|-55|false</td>\n",
       "      <td>c_28</td>\n",
       "      <td>117.25607</td>\n",
       "      <td>36.899736</td>\n",
       "      <td>40</td>\n",
       "      <td>...</td>\n",
       "      <td>(10, [], [(b_53667045, -87), (b_22472878, -74), (b_49278725, -78), (b_52048607, -71), (b_51271474, -68), (b_26467949, -77), (b_53667051, -88), (b_48793119, -78), (b_52048604, -71), (b_40209888, -55)])</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>15855</td>\n",
       "      <td>216</td>\n",
       "      <td>40</td>\n",
       "      <td>940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1101291</th>\n",
       "      <td>u_53098611</td>\n",
       "      <td>s_3726779</td>\n",
       "      <td>2017-08-24 21:00</td>\n",
       "      <td>117.256059</td>\n",
       "      <td>36.899743</td>\n",
       "      <td>b_51726487|-82|false;b_40209888|-50|false;b_22472878|-74|false;b_52048604|-72|false;b_40059952|-77|false;b_23895926|-86|false;b_30379445|-75|false;b_44200910|-72|false;b_55289296|-74|false;b_44415092|-66|false</td>\n",
       "      <td>c_28</td>\n",
       "      <td>117.25607</td>\n",
       "      <td>36.899736</td>\n",
       "      <td>40</td>\n",
       "      <td>...</td>\n",
       "      <td>(10, [], [(b_51726487, -82), (b_40209888, -50), (b_22472878, -74), (b_52048604, -72), (b_40059952, -77), (b_23895926, -86), (b_30379445, -75), (b_44200910, -72), (b_55289296, -74), (b_44415092, -66)])</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>15928</td>\n",
       "      <td>236</td>\n",
       "      <td>0</td>\n",
       "      <td>1260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1104122</th>\n",
       "      <td>u_53858511</td>\n",
       "      <td>s_3726779</td>\n",
       "      <td>2017-08-20 22:10</td>\n",
       "      <td>117.256050</td>\n",
       "      <td>36.899722</td>\n",
       "      <td>b_52048607|-81|false;b_40209888|-57|false;b_40059952|-78|false;b_44317488|-88|false;b_44415092|-80|false;b_26467980|-79|false;b_51726487|-81|false;b_40059953|-81|false;b_31112294|-90|false;b_40362685|-77|false</td>\n",
       "      <td>c_28</td>\n",
       "      <td>117.25607</td>\n",
       "      <td>36.899736</td>\n",
       "      <td>40</td>\n",
       "      <td>...</td>\n",
       "      <td>(10, [], [(b_52048607, -81), (b_40209888, -57), (b_40059952, -78), (b_44317488, -88), (b_44415092, -80), (b_26467980, -79), (b_51726487, -81), (b_40059953, -81), (b_31112294, -90), (b_40362685, -77)])</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>15986</td>\n",
       "      <td>232</td>\n",
       "      <td>10</td>\n",
       "      <td>1330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1106217</th>\n",
       "      <td>u_54459522</td>\n",
       "      <td>s_3726779</td>\n",
       "      <td>2017-08-15 21:20</td>\n",
       "      <td>117.255984</td>\n",
       "      <td>36.899730</td>\n",
       "      <td>b_22472878|-67|false;b_28660854|-77|false;b_40209888|-47|true;b_51726487|-72|false;b_53667050|-66|false;b_26613990|-84|false;b_44415092|-67|false;b_14580929|-75|false;b_40362685|-80|false;b_16409372|-71|false</td>\n",
       "      <td>c_28</td>\n",
       "      <td>117.25607</td>\n",
       "      <td>36.899736</td>\n",
       "      <td>40</td>\n",
       "      <td>...</td>\n",
       "      <td>(10, [(b_40209888, -47)], [(b_22472878, -67), (b_28660854, -77), (b_51726487, -72), (b_53667050, -66), (b_26613990, -84), (b_44415092, -67), (b_14580929, -75), (b_40362685, -80), (b_16409372, -71)])</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.9</td>\n",
       "      <td>16019</td>\n",
       "      <td>227</td>\n",
       "      <td>20</td>\n",
       "      <td>1280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1107285</th>\n",
       "      <td>u_54788026</td>\n",
       "      <td>s_3726779</td>\n",
       "      <td>2017-08-18 17:40</td>\n",
       "      <td>117.256082</td>\n",
       "      <td>36.899738</td>\n",
       "      <td>b_40209888|-54|false;b_44415092|-73|false;b_22472878|-67|false;b_30379445|-73|false;b_49278725|-75|false;b_52048604|-67|false;b_40059953|-73|false;b_40059952|-76|false;b_51726487|-76|false;b_3933545|-43|false</td>\n",
       "      <td>c_28</td>\n",
       "      <td>117.25607</td>\n",
       "      <td>36.899736</td>\n",
       "      <td>40</td>\n",
       "      <td>...</td>\n",
       "      <td>(10, [], [(b_40209888, -54), (b_44415092, -73), (b_22472878, -67), (b_30379445, -73), (b_49278725, -75), (b_52048604, -67), (b_40059953, -73), (b_40059952, -76), (b_51726487, -76), (b_3933545, -43)])</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>16039</td>\n",
       "      <td>230</td>\n",
       "      <td>40</td>\n",
       "      <td>1060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1107760</th>\n",
       "      <td>u_54933112</td>\n",
       "      <td>s_3726779</td>\n",
       "      <td>2017-08-24 11:00</td>\n",
       "      <td>117.256054</td>\n",
       "      <td>36.899701</td>\n",
       "      <td>b_30379445|-75|false;b_51048975|-75|false;b_26467980|-79|false;b_40059952|-82|false;b_52048607|-78|false;b_40209888|-62|false;b_12705141|-76|false;b_22472878|-77|false;b_52048604|-74|false;b_55289296|-79|false</td>\n",
       "      <td>c_28</td>\n",
       "      <td>117.25607</td>\n",
       "      <td>36.899736</td>\n",
       "      <td>40</td>\n",
       "      <td>...</td>\n",
       "      <td>(10, [], [(b_30379445, -75), (b_51048975, -75), (b_26467980, -79), (b_40059952, -82), (b_52048607, -78), (b_40209888, -62), (b_12705141, -76), (b_22472878, -77), (b_52048604, -74), (b_55289296, -79)])</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>16051</td>\n",
       "      <td>236</td>\n",
       "      <td>0</td>\n",
       "      <td>660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1107808</th>\n",
       "      <td>u_54949084</td>\n",
       "      <td>s_3726779</td>\n",
       "      <td>2017-08-05 18:10</td>\n",
       "      <td>117.258360</td>\n",
       "      <td>36.898480</td>\n",
       "      <td>b_21514052|-67|false;b_51726487|-81|false;b_14753472|-60|false;b_26467980|-77|false;b_52048607|-68|false;b_52048604|-73|false;b_15638003|-71|false;b_40209888|-53|false;b_40362685|-78|false;b_51048975|-67|false</td>\n",
       "      <td>c_28</td>\n",
       "      <td>117.25607</td>\n",
       "      <td>36.899736</td>\n",
       "      <td>40</td>\n",
       "      <td>...</td>\n",
       "      <td>(10, [], [(b_21514052, -67), (b_51726487, -81), (b_14753472, -60), (b_26467980, -77), (b_52048607, -68), (b_52048604, -73), (b_15638003, -71), (b_40209888, -53), (b_40362685, -78), (b_51048975, -67)])</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>16052</td>\n",
       "      <td>217</td>\n",
       "      <td>10</td>\n",
       "      <td>1090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1110020</th>\n",
       "      <td>u_55514576</td>\n",
       "      <td>s_3726779</td>\n",
       "      <td>2017-08-10 11:40</td>\n",
       "      <td>117.256047</td>\n",
       "      <td>36.899709</td>\n",
       "      <td>b_31112294|-87|false;b_22472878|-84|false;b_28660854|-85|false;b_12705141|-74|false;b_38366281|-88|false;b_26467980|-75|false;b_52048604|-78|false;b_40209888|-69|false;b_51271474|-76|false;b_30559295|-81|false</td>\n",
       "      <td>c_28</td>\n",
       "      <td>117.25607</td>\n",
       "      <td>36.899736</td>\n",
       "      <td>40</td>\n",
       "      <td>...</td>\n",
       "      <td>(10, [], [(b_31112294, -87), (b_22472878, -84), (b_28660854, -85), (b_12705141, -74), (b_38366281, -88), (b_26467980, -75), (b_52048604, -78), (b_40209888, -69), (b_51271474, -76), (b_30559295, -81)])</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>16099</td>\n",
       "      <td>222</td>\n",
       "      <td>40</td>\n",
       "      <td>700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1110021</th>\n",
       "      <td>u_55514576</td>\n",
       "      <td>s_3726779</td>\n",
       "      <td>2017-08-15 13:10</td>\n",
       "      <td>117.256031</td>\n",
       "      <td>36.899726</td>\n",
       "      <td>b_55289296|-78|false;b_51726487|-74|false;b_40059952|-79|false;b_53667045|-83|false;b_22472878|-75|false;b_40059953|-75|false;b_30379445|-78|false;b_44415092|-79|false;b_15638003|-75|false;b_40209888|-58|false</td>\n",
       "      <td>c_28</td>\n",
       "      <td>117.25607</td>\n",
       "      <td>36.899736</td>\n",
       "      <td>40</td>\n",
       "      <td>...</td>\n",
       "      <td>(10, [], [(b_55289296, -78), (b_51726487, -74), (b_40059952, -79), (b_53667045, -83), (b_22472878, -75), (b_40059953, -75), (b_30379445, -78), (b_44415092, -79), (b_15638003, -75), (b_40209888, -58)])</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>16100</td>\n",
       "      <td>227</td>\n",
       "      <td>10</td>\n",
       "      <td>790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1110029</th>\n",
       "      <td>u_55517851</td>\n",
       "      <td>s_3726779</td>\n",
       "      <td>2017-08-11 11:20</td>\n",
       "      <td>117.255965</td>\n",
       "      <td>36.899742</td>\n",
       "      <td>b_44434668|-74|false;b_53667047|-84|false;b_20874926|-74|false;b_52048607|-75|false;b_51726487|-73|false;b_28660854|-76|false;b_53667045|-89|false;b_40209888|-54|false;b_53667051|-86|false;b_40362685|-77|false</td>\n",
       "      <td>c_28</td>\n",
       "      <td>117.25607</td>\n",
       "      <td>36.899736</td>\n",
       "      <td>40</td>\n",
       "      <td>...</td>\n",
       "      <td>(10, [], [(b_44434668, -74), (b_53667047, -84), (b_20874926, -74), (b_52048607, -75), (b_51726487, -73), (b_28660854, -76), (b_53667045, -89), (b_40209888, -54), (b_53667051, -86), (b_40362685, -77)])</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>16104</td>\n",
       "      <td>223</td>\n",
       "      <td>20</td>\n",
       "      <td>680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1117927</th>\n",
       "      <td>u_57819177</td>\n",
       "      <td>s_3726779</td>\n",
       "      <td>2017-08-07 13:50</td>\n",
       "      <td>117.256054</td>\n",
       "      <td>36.899729</td>\n",
       "      <td>b_48793119|-76|false;b_39574079|-76|false;b_53667045|-92|false;b_51271474|-66|false;b_12705141|-67|false;b_22472878|-69|false;b_40362685|-73|false;b_34039495|-77|false;b_51726487|-71|false;b_40209888|-55|false</td>\n",
       "      <td>c_28</td>\n",
       "      <td>117.25607</td>\n",
       "      <td>36.899736</td>\n",
       "      <td>40</td>\n",
       "      <td>...</td>\n",
       "      <td>(10, [], [(b_48793119, -76), (b_39574079, -76), (b_53667045, -92), (b_51271474, -66), (b_12705141, -67), (b_22472878, -69), (b_40362685, -73), (b_34039495, -77), (b_51726487, -71), (b_40209888, -55)])</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>16186</td>\n",
       "      <td>219</td>\n",
       "      <td>50</td>\n",
       "      <td>830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1118268</th>\n",
       "      <td>u_57906745</td>\n",
       "      <td>s_3726779</td>\n",
       "      <td>2017-08-18 12:00</td>\n",
       "      <td>117.256063</td>\n",
       "      <td>36.899732</td>\n",
       "      <td>b_52048604|-69|false;b_26467980|-76|false;b_26467981|-88|false;b_38366281|-80|false;b_40209888|-58|false;b_44415092|-70|false;b_51048975|-79|false;b_39574079|-81|false;b_12705141|-75|false;b_51726487|-84|false</td>\n",
       "      <td>c_28</td>\n",
       "      <td>117.25607</td>\n",
       "      <td>36.899736</td>\n",
       "      <td>40</td>\n",
       "      <td>...</td>\n",
       "      <td>(10, [], [(b_52048604, -69), (b_26467980, -76), (b_26467981, -88), (b_38366281, -80), (b_40209888, -58), (b_44415092, -70), (b_51048975, -79), (b_39574079, -81), (b_12705141, -75), (b_51726487, -84)])</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>16188</td>\n",
       "      <td>230</td>\n",
       "      <td>0</td>\n",
       "      <td>720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1119906</th>\n",
       "      <td>u_58504303</td>\n",
       "      <td>s_3726779</td>\n",
       "      <td>2017-08-07 19:40</td>\n",
       "      <td>117.255992</td>\n",
       "      <td>36.899717</td>\n",
       "      <td>b_15638003|-88|false;b_51726487|-82|false;b_22472878|-87|false;b_48793119|-89|false;b_44200910|-84|false;b_26467980|-88|false;b_52048604|-81|false;b_49278725|-89|false;b_51048975|-73|false;b_40209888|-64|false</td>\n",
       "      <td>c_28</td>\n",
       "      <td>117.25607</td>\n",
       "      <td>36.899736</td>\n",
       "      <td>40</td>\n",
       "      <td>...</td>\n",
       "      <td>(10, [], [(b_15638003, -88), (b_51726487, -82), (b_22472878, -87), (b_48793119, -89), (b_44200910, -84), (b_26467980, -88), (b_52048604, -81), (b_49278725, -89), (b_51048975, -73), (b_40209888, -64)])</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>16207</td>\n",
       "      <td>219</td>\n",
       "      <td>40</td>\n",
       "      <td>1180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1120462</th>\n",
       "      <td>u_58661155</td>\n",
       "      <td>s_3726779</td>\n",
       "      <td>2017-08-03 12:10</td>\n",
       "      <td>117.256026</td>\n",
       "      <td>36.899728</td>\n",
       "      <td>b_51048975|-76|false;b_44434668|-77|false;b_44499866|-73|false;b_17079184|-74|false;b_40209888|-58|false;b_51726487|-71|false;b_52048607|-72|false;b_55289296|-85|false;b_22472878|-76|false;b_3417117|-78|false</td>\n",
       "      <td>c_28</td>\n",
       "      <td>117.25607</td>\n",
       "      <td>36.899736</td>\n",
       "      <td>40</td>\n",
       "      <td>...</td>\n",
       "      <td>(10, [], [(b_51048975, -76), (b_44434668, -77), (b_44499866, -73), (b_17079184, -74), (b_40209888, -58), (b_51726487, -71), (b_52048607, -72), (b_55289296, -85), (b_22472878, -76), (b_3417117, -78)])</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>16219</td>\n",
       "      <td>215</td>\n",
       "      <td>10</td>\n",
       "      <td>730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1120895</th>\n",
       "      <td>u_58767256</td>\n",
       "      <td>s_3726779</td>\n",
       "      <td>2017-08-13 19:50</td>\n",
       "      <td>117.256033</td>\n",
       "      <td>36.899723</td>\n",
       "      <td>b_52048607|-79|false;b_51726487|-78|false;b_22472878|-74|false;b_52048604|-69|false;b_54149190|-85|false;b_31112294|-78|false;b_40209888|-48|false;b_55016265|-75|false;b_26467980|-73|false;b_44415092|-73|false</td>\n",
       "      <td>c_28</td>\n",
       "      <td>117.25607</td>\n",
       "      <td>36.899736</td>\n",
       "      <td>40</td>\n",
       "      <td>...</td>\n",
       "      <td>(10, [], [(b_52048607, -79), (b_51726487, -78), (b_22472878, -74), (b_52048604, -69), (b_54149190, -85), (b_31112294, -78), (b_40209888, -48), (b_55016265, -75), (b_26467980, -73), (b_44415092, -73)])</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>16224</td>\n",
       "      <td>225</td>\n",
       "      <td>50</td>\n",
       "      <td>1190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1124211</th>\n",
       "      <td>u_59751224</td>\n",
       "      <td>s_3726779</td>\n",
       "      <td>2017-08-16 15:50</td>\n",
       "      <td>117.256055</td>\n",
       "      <td>36.899730</td>\n",
       "      <td>b_49278725|-76|false;b_22472878|-71|false;b_55312507|-77|false;b_12705141|-70|false;b_40059953|-78|false;b_53667045|-89|false;b_40209888|-61|false;b_38366281|-77|false;b_40059952|-75|false;b_40362685|-74|false</td>\n",
       "      <td>c_28</td>\n",
       "      <td>117.25607</td>\n",
       "      <td>36.899736</td>\n",
       "      <td>40</td>\n",
       "      <td>...</td>\n",
       "      <td>(10, [], [(b_49278725, -76), (b_22472878, -71), (b_55312507, -77), (b_12705141, -70), (b_40059953, -78), (b_53667045, -89), (b_40209888, -61), (b_38366281, -77), (b_40059952, -75), (b_40362685, -74)])</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>16262</td>\n",
       "      <td>228</td>\n",
       "      <td>50</td>\n",
       "      <td>950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1124212</th>\n",
       "      <td>u_59751224</td>\n",
       "      <td>s_3726779</td>\n",
       "      <td>2017-08-08 16:00</td>\n",
       "      <td>117.256041</td>\n",
       "      <td>36.899700</td>\n",
       "      <td>b_39574079|-78|false;b_30379445|-73|false;b_40209888|-54|false;b_12705141|-77|false;b_51726487|-79|false;b_40362685|-78|false;b_49278725|-77|false;b_55289296|-76|false;b_52048607|-75|false;b_51271474|-73|false</td>\n",
       "      <td>c_28</td>\n",
       "      <td>117.25607</td>\n",
       "      <td>36.899736</td>\n",
       "      <td>40</td>\n",
       "      <td>...</td>\n",
       "      <td>(10, [], [(b_39574079, -78), (b_30379445, -73), (b_40209888, -54), (b_12705141, -77), (b_51726487, -79), (b_40362685, -78), (b_49278725, -77), (b_55289296, -76), (b_52048607, -75), (b_51271474, -73)])</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>16263</td>\n",
       "      <td>220</td>\n",
       "      <td>0</td>\n",
       "      <td>960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1132673</th>\n",
       "      <td>u_62195578</td>\n",
       "      <td>s_3726779</td>\n",
       "      <td>2017-08-05 18:10</td>\n",
       "      <td>117.256067</td>\n",
       "      <td>36.899711</td>\n",
       "      <td>b_40209888|-59|false;b_15638004|-87|false;b_15638003|-69|false;b_49278725|-77|false;b_17079182|-87|false;b_48793119|-76|false;b_52048607|-76|false;b_38366281|-76|false;b_52048604|-68|false;b_22472878|-72|false</td>\n",
       "      <td>c_28</td>\n",
       "      <td>117.25607</td>\n",
       "      <td>36.899736</td>\n",
       "      <td>40</td>\n",
       "      <td>...</td>\n",
       "      <td>(10, [], [(b_40209888, -59), (b_15638004, -87), (b_15638003, -69), (b_49278725, -77), (b_17079182, -87), (b_48793119, -76), (b_52048607, -76), (b_38366281, -76), (b_52048604, -68), (b_22472878, -72)])</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>16400</td>\n",
       "      <td>217</td>\n",
       "      <td>10</td>\n",
       "      <td>1090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1134060</th>\n",
       "      <td>u_62613582</td>\n",
       "      <td>s_3726779</td>\n",
       "      <td>2017-08-16 11:00</td>\n",
       "      <td>117.256048</td>\n",
       "      <td>36.899725</td>\n",
       "      <td>b_40209888|-53|false;b_53667051|-89|false;b_53667045|-86|false;b_52025240|-76|false;b_52048607|-73|false;b_49278725|-78|false;b_52048604|-70|false;b_22472878|-67|false;b_53667061|-91|false;b_51726487|-72|false</td>\n",
       "      <td>c_28</td>\n",
       "      <td>117.25607</td>\n",
       "      <td>36.899736</td>\n",
       "      <td>40</td>\n",
       "      <td>...</td>\n",
       "      <td>(10, [], [(b_40209888, -53), (b_53667051, -89), (b_53667045, -86), (b_52025240, -76), (b_52048607, -73), (b_49278725, -78), (b_52048604, -70), (b_22472878, -67), (b_53667061, -91), (b_51726487, -72)])</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>16415</td>\n",
       "      <td>228</td>\n",
       "      <td>0</td>\n",
       "      <td>660</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>405 rows × 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            user_id    shop_id        time_stamp   longitude   latitude  \\\n",
       "2374       u_383311  s_3726779  2017-08-11 18:00  117.256029  36.899741   \n",
       "3217       u_523659  s_3726779  2017-08-10 20:10  117.256025  36.899719   \n",
       "5512       u_900254  s_3726779  2017-08-19 20:40  117.256053  36.899719   \n",
       "5603       u_918067  s_3726779  2017-08-23 18:40  117.256061  36.899698   \n",
       "6358      u_1045586  s_3726779  2017-08-17 20:30  117.256027  36.899709   \n",
       "7692      u_1263622  s_3726779  2017-08-10 20:20  117.256092  36.899723   \n",
       "7919      u_1297990  s_3726779  2017-08-24 21:10  117.256005  36.899658   \n",
       "15241     u_2603632  s_3726779  2017-08-09 18:10  117.255977  36.899716   \n",
       "15242     u_2603632  s_3726779  2017-08-17 11:50  117.256020  36.899713   \n",
       "16201     u_2798769  s_3726779  2017-08-13 13:50  117.256050  36.899714   \n",
       "17102     u_2959695  s_3726779  2017-08-17 20:40  117.256042  36.899703   \n",
       "17267     u_2991298  s_3726779  2017-08-14 13:20  117.256038  36.899714   \n",
       "19018     u_3347503  s_3726779  2017-08-24 19:40  117.256037  36.899731   \n",
       "22419     u_4005712  s_3726779  2017-08-04 20:10  117.256024  36.899730   \n",
       "24196     u_4404917  s_3726779  2017-08-17 20:10  117.255965  36.899722   \n",
       "45444    u_10665203  s_3726779  2017-08-19 15:20  117.256011  36.899715   \n",
       "46295    u_10842859  s_3726779  2017-08-20 12:00  117.256064  36.899701   \n",
       "48004    u_11193921  s_3726779  2017-08-16 19:20  117.256072  36.899742   \n",
       "56501    u_13109059  s_3726779  2017-08-24 20:20  117.256044  36.899707   \n",
       "57408    u_13371404  s_3726779  2017-08-22 21:00  117.256056  36.899699   \n",
       "69517    u_17056510  s_3726779  2017-08-13 18:10  117.256048  36.899721   \n",
       "73730    u_18366431  s_3726779  2017-08-14 12:10  117.256055  36.899719   \n",
       "73966    u_18428121  s_3726779  2017-08-09 12:30  117.256044  36.899713   \n",
       "74309    u_18492188  s_3726779  2017-08-01 19:00  117.255977  36.899728   \n",
       "83993    u_20963816  s_3726779  2017-08-20 18:30  117.256016  36.899759   \n",
       "88117    u_22269170  s_3726779  2017-08-20 21:00  117.256073  36.899744   \n",
       "92536    u_23639090  s_3726779  2017-08-08 20:20  117.256001  36.899724   \n",
       "94712    u_24267847  s_3726779  2017-08-24 22:10  117.256054  36.899686   \n",
       "99169    u_25320118  s_3726779  2017-08-10 10:50  117.255980  36.899720   \n",
       "99597    u_25413152  s_3726779  2017-08-20 13:30  117.256024  36.899741   \n",
       "...             ...        ...               ...         ...        ...   \n",
       "1058701  u_40262017  s_3726779  2017-08-05 13:00  117.256021  36.899724   \n",
       "1069387  u_43850085  s_3726779  2017-08-15 17:30  117.256071  36.899687   \n",
       "1070075  u_44095801  s_3726779  2017-08-12 10:30  117.256036  36.899722   \n",
       "1075865  u_45568463  s_3726779  2017-08-11 21:20  117.256026  36.899722   \n",
       "1076618  u_45787430  s_3726779  2017-08-11 19:00  117.255956  36.899728   \n",
       "1082274  u_47288216  s_3726779  2017-08-04 16:10  117.256001  36.899742   \n",
       "1083580  u_47645769  s_3726779  2017-08-16 20:30  117.256056  36.899712   \n",
       "1083967  u_47756473  s_3726779  2017-08-12 21:20  117.255982  36.899734   \n",
       "1086135  u_48308349  s_3726779  2017-08-17 18:10  117.256032  36.899757   \n",
       "1089420  u_49215617  s_3726779  2017-08-14 13:10  117.256060  36.899721   \n",
       "1092357  u_50180800  s_3726779  2017-08-24 19:20  117.256034  36.899707   \n",
       "1094189  u_50710973  s_3726779  2017-08-04 15:40  117.256025  36.899704   \n",
       "1101291  u_53098611  s_3726779  2017-08-24 21:00  117.256059  36.899743   \n",
       "1104122  u_53858511  s_3726779  2017-08-20 22:10  117.256050  36.899722   \n",
       "1106217  u_54459522  s_3726779  2017-08-15 21:20  117.255984  36.899730   \n",
       "1107285  u_54788026  s_3726779  2017-08-18 17:40  117.256082  36.899738   \n",
       "1107760  u_54933112  s_3726779  2017-08-24 11:00  117.256054  36.899701   \n",
       "1107808  u_54949084  s_3726779  2017-08-05 18:10  117.258360  36.898480   \n",
       "1110020  u_55514576  s_3726779  2017-08-10 11:40  117.256047  36.899709   \n",
       "1110021  u_55514576  s_3726779  2017-08-15 13:10  117.256031  36.899726   \n",
       "1110029  u_55517851  s_3726779  2017-08-11 11:20  117.255965  36.899742   \n",
       "1117927  u_57819177  s_3726779  2017-08-07 13:50  117.256054  36.899729   \n",
       "1118268  u_57906745  s_3726779  2017-08-18 12:00  117.256063  36.899732   \n",
       "1119906  u_58504303  s_3726779  2017-08-07 19:40  117.255992  36.899717   \n",
       "1120462  u_58661155  s_3726779  2017-08-03 12:10  117.256026  36.899728   \n",
       "1120895  u_58767256  s_3726779  2017-08-13 19:50  117.256033  36.899723   \n",
       "1124211  u_59751224  s_3726779  2017-08-16 15:50  117.256055  36.899730   \n",
       "1124212  u_59751224  s_3726779  2017-08-08 16:00  117.256041  36.899700   \n",
       "1132673  u_62195578  s_3726779  2017-08-05 18:10  117.256067  36.899711   \n",
       "1134060  u_62613582  s_3726779  2017-08-16 11:00  117.256048  36.899725   \n",
       "\n",
       "                                                                                                                                                                                                                wifi_infos  \\\n",
       "2374     b_52168829|-78|false;b_26467949|-73|false;b_17079182|-74|false;b_40209888|-54|false;b_44415092|-78|false;b_51726487|-75|false;b_52048604|-76|false;b_44200910|-81|false;b_39574079|-79|false;b_51048975|-77|false   \n",
       "3217     b_40362685|-86|false;b_17079184|-87|false;b_51726487|-86|false;b_48793119|-90|false;b_40209888|-68|false;b_15638003|-77|false;b_49278725|-92|false;b_15638004|-87|false;b_52048604|-87|false;b_51271474|-83|false   \n",
       "5512     b_51048975|-75|false;b_12705141|-75|false;b_49278725|-76|false;b_51726487|-68|false;b_40209888|-65|false;b_22472878|-74|false;b_52025240|-81|false;b_40362685|-79|false;b_53340470|-66|false;b_44415092|-75|false   \n",
       "5603     b_38366281|-78|false;b_51726487|-73|false;b_17079184|-78|false;b_40059953|-80|false;b_30559295|-77|false;b_55016265|-76|false;b_40209888|-52|false;b_17079183|-78|false;b_15638003|-79|false;b_44415092|-77|false   \n",
       "6358     b_15638003|-83|false;b_48793119|-81|false;b_44200910|-86|false;b_49278725|-83|false;b_55289296|-84|false;b_39574079|-86|false;b_51726487|-76|false;b_28660854|-86|false;b_22472878|-76|false;b_40209888|-59|false   \n",
       "7692     b_49278725|-76|false;b_48793119|-78|false;b_40209888|-57|false;b_26467980|-71|false;b_51271474|-69|false;b_12705141|-76|false;b_30379445|-80|false;b_52048604|-67|false;b_17079182|-84|false;b_53667045|-87|false   \n",
       "7919                                               b_40059953|-86|false;b_26467980|-80|false;b_40209888|-61|false;b_51726487|-72|false;b_52048604|-79|false;b_52048607|-84|false;b_39574079|-80|false;b_40362685|-78|false   \n",
       "15241    b_51726487|-65|false;b_44322551|-80|false;b_26467949|-77|false;b_30118357|-86|false;b_44434668|-83|false;b_52048604|-77|false;b_52048607|-78|false;b_51048975|-84|false;b_40362685|-84|false;b_40209888|-61|false   \n",
       "15242    b_51726487|-68|false;b_40209888|-60|false;b_52048607|-69|false;b_14580929|-83|false;b_17079184|-81|false;b_30379445|-78|false;b_44415092|-75|false;b_49278725|-80|false;b_51048975|-74|false;b_22472878|-77|false   \n",
       "16201    b_26467980|-70|false;b_30379445|-88|false;b_48793119|-93|false;b_51726487|-80|false;b_52048607|-85|false;b_39574079|-83|false;b_44415092|-73|false;b_49278725|-90|false;b_40209888|-72|false;b_12705141|-87|false   \n",
       "17102     b_51726487|-73|false;b_3046873|-70|false;b_17079184|-81|false;b_40209888|-49|false;b_52048604|-64|false;b_17079183|-82|false;b_55016265|-80|false;b_48249102|-77|false;b_44415092|-76|false;b_30559295|-76|false   \n",
       "17267    b_39574079|-77|false;b_40362685|-77|false;b_51145806|-81|false;b_52048607|-71|false;b_40209888|-56|false;b_51726487|-71|false;b_31112294|-81|false;b_52025241|-80|false;b_26613990|-85|false;b_22472878|-71|false   \n",
       "19018    b_51726487|-77|false;b_48793119|-79|false;b_40209888|-61|false;b_40059953|-83|false;b_17079183|-86|false;b_44434668|-86|false;b_52048604|-88|false;b_44415092|-78|false;b_49278725|-78|false;b_52048607|-85|false   \n",
       "22419     b_20874926|-78|false;b_52048605|-83|false;b_52048607|-77|false;b_40209888|-60|false;b_51726487|-79|false;b_3114529|-76|false;b_28660854|-76|false;b_26467949|-82|false;b_51048975|-74|false;b_52048604|-78|false   \n",
       "24196    b_53667045|-89|false;b_40209888|-57|false;b_44200910|-86|false;b_49148817|-89|false;b_30379445|-79|false;b_44415092|-81|false;b_51726487|-76|false;b_15638004|-90|false;b_15638003|-82|false;b_22472878|-81|false   \n",
       "45444    b_51726487|-80|false;b_28660854|-78|false;b_31112294|-85|false;b_51048975|-71|false;b_40209888|-61|false;b_52048604|-68|false;b_44415092|-79|false;b_22472878|-74|false;b_52048607|-79|false;b_40362685|-71|false   \n",
       "46295     b_39574079|-73|false;b_12705141|-77|false;b_55289296|-81|false;b_52048604|-70|false;b_49278725|-76|false;b_48793119|-78|false;b_51726487|-76|false;b_9025795|-85|false;b_40209888|-55|false;b_52048607|-77|false   \n",
       "48004    b_48793119|-79|false;b_22472878|-72|false;b_40059953|-80|false;b_40209888|-59|false;b_52048604|-75|false;b_44415092|-79|false;b_51726487|-76|false;b_49278725|-79|false;b_55289296|-81|false;b_26467980|-79|false   \n",
       "56501    b_39574079|-90|false;b_55289296|-79|false;b_40209888|-66|false;b_51726487|-87|false;b_30379445|-81|false;b_22472878|-85|false;b_49278725|-91|false;b_52048604|-87|false;b_48793119|-89|false;b_49148816|-87|false   \n",
       "57408    b_22472878|-81|false;b_40059952|-78|false;b_44415092|-81|false;b_55289296|-91|false;b_40059953|-77|false;b_52048604|-73|false;b_26467980|-79|false;b_40209888|-73|false;b_30379445|-88|false;b_51726487|-80|false   \n",
       "69517    b_51726487|-64|false;b_44415092|-73|false;b_26467980|-74|false;b_30379445|-64|false;b_52048604|-63|false;b_40209888|-48|false;b_55289296|-63|false;b_22472878|-66|false;b_12705141|-71|false;b_49148817|-88|false   \n",
       "73730    b_22472878|-76|false;b_39574079|-78|false;b_12705141|-69|false;b_30379445|-76|false;b_52025241|-82|false;b_14580929|-76|false;b_49148817|-84|false;b_51726487|-79|false;b_55289296|-76|false;b_40209888|-58|false   \n",
       "73966    b_49148817|-86|false;b_55289296|-83|false;b_30379445|-82|false;b_40209888|-58|false;b_44434668|-82|false;b_44200910|-80|false;b_38366281|-71|false;b_17079182|-88|false;b_52048604|-68|false;b_51726487|-74|false   \n",
       "74309                                                                   b_15638003|-88|false;b_26468126|-91|false;b_40209888|-65|false;b_34893889|-82|false;b_51048975|-81|false;b_44434668|-84|false;b_48793119|-82|false   \n",
       "83993    b_12705141|-84|false;b_40059953|-86|false;b_48793119|-91|false;b_26467980|-86|false;b_52025240|-87|false;b_40209888|-63|false;b_44415092|-88|false;b_51726487|-74|false;b_40059952|-85|false;b_48249102|-92|false   \n",
       "88117    b_52048604|-60|false;b_52025240|-76|false;b_44200910|-72|false;b_40059953|-76|false;b_40209888|-58|false;b_40059952|-70|false;b_17079183|-67|false;b_38300990|-74|false;b_52048607|-75|false;b_51726487|-66|false   \n",
       "92536    b_55016265|-88|false;b_40362685|-83|false;b_30379445|-88|false;b_40209888|-67|false;b_51726487|-86|false;b_51271474|-86|false;b_52048607|-85|false;b_48249102|-86|false;b_49278725|-84|false;b_48793119|-82|false   \n",
       "94712    b_23895926|-86|false;b_55289296|-65|false;b_51726487|-72|false;b_52048604|-74|false;b_52048607|-72|false;b_44415092|-74|false;b_40059952|-87|false;b_19770055|-88|false;b_30379445|-69|false;b_40209888|-60|false   \n",
       "99169    b_26467981|-92|false;b_55312507|-80|false;b_52048599|-91|false;b_48793119|-84|false;b_40209888|-68|false;b_51726487|-79|false;b_40362685|-79|false;b_51048975|-77|false;b_50640451|-86|false;b_53667047|-90|false   \n",
       "99597     b_40209888|-63|false;b_52048607|-77|false;b_44415092|-81|false;b_48793119|-86|false;b_51726487|-81|false;b_3918679|-81|false;b_49278725|-86|false;b_48249102|-84|false;b_40059952|-84|false;b_40059953|-78|false   \n",
       "...                                                                                                                                                                                                                    ...   \n",
       "1058701  b_22472878|-83|false;b_17079183|-86|false;b_40209888|-60|false;b_51726487|-75|false;b_52048607|-79|false;b_40362685|-84|false;b_52025241|-89|false;b_52048604|-80|false;b_17079184|-85|false;b_51048975|-87|false   \n",
       "1069387  b_22472878|-71|false;b_52048604|-74|false;b_51726487|-77|false;b_26467980|-81|false;b_40209888|-63|false;b_55289296|-85|false;b_44415092|-83|false;b_30379445|-88|false;b_40059952|-74|false;b_52048607|-79|false   \n",
       "1070075  b_55289296|-75|false;b_44415092|-64|false;b_40209888|-58|false;b_40362685|-81|false;b_51726487|-69|false;b_30379445|-73|false;b_26467981|-85|false;b_39574079|-72|false;b_22472878|-74|false;b_12705141|-74|false   \n",
       "1075865  b_33962055|-85|false;b_52048604|-74|false;b_28660854|-76|false;b_55289296|-78|false;b_51726487|-75|false;b_30118357|-86|false;b_44415092|-72|false;b_14580929|-72|false;b_40209888|-60|false;b_30379445|-78|false   \n",
       "1076618  b_15638003|-84|false;b_42415389|-81|false;b_12705141|-80|false;b_49278725|-86|false;b_52048604|-84|false;b_44415092|-88|false;b_51726487|-77|false;b_40209888|-56|false;b_53667047|-88|false;b_53667045|-89|false   \n",
       "1082274                                                                                                                                b_44434668|-91|false;b_48793119|-84|false;b_52048604|-84|false;b_40209888|-62|false   \n",
       "1083580  b_51696066|-82|false;b_51726487|-72|false;b_52048607|-88|false;b_52048604|-76|false;b_40209888|-64|false;b_22472878|-77|false;b_48793119|-79|false;b_49278725|-79|false;b_40059953|-75|false;b_26467949|-82|false   \n",
       "1083967  b_26467980|-82|false;b_30379445|-81|false;b_52048607|-73|false;b_22472878|-83|false;b_44959978|-58|false;b_50640451|-80|false;b_40362685|-72|false;b_51726487|-78|false;b_40209888|-55|false;b_44415092|-75|false   \n",
       "1086135  b_51726487|-70|false;b_40209888|-52|false;b_44200910|-77|false;b_44415092|-72|false;b_22472878|-69|false;b_40059953|-77|false;b_40059952|-70|false;b_26467980|-78|false;b_55016265|-72|false;b_28660854|-77|false   \n",
       "1089420  b_49148817|-92|false;b_51726487|-73|false;b_55289296|-83|false;b_52048604|-66|false;b_40059953|-88|false;b_44415092|-85|false;b_53667045|-93|false;b_22472878|-73|false;b_40209888|-58|false;b_49278725|-84|false   \n",
       "1092357  b_28660854|-79|false;b_22472878|-77|false;b_12705141|-80|false;b_44874709|-78|false;b_52048604|-77|false;b_49278725|-77|false;b_51726487|-77|false;b_48793119|-80|false;b_55289296|-80|false;b_40209888|-51|false   \n",
       "1094189  b_53667045|-87|false;b_22472878|-74|false;b_49278725|-78|false;b_52048607|-71|false;b_51271474|-68|false;b_26467949|-77|false;b_53667051|-88|false;b_48793119|-78|false;b_52048604|-71|false;b_40209888|-55|false   \n",
       "1101291  b_51726487|-82|false;b_40209888|-50|false;b_22472878|-74|false;b_52048604|-72|false;b_40059952|-77|false;b_23895926|-86|false;b_30379445|-75|false;b_44200910|-72|false;b_55289296|-74|false;b_44415092|-66|false   \n",
       "1104122  b_52048607|-81|false;b_40209888|-57|false;b_40059952|-78|false;b_44317488|-88|false;b_44415092|-80|false;b_26467980|-79|false;b_51726487|-81|false;b_40059953|-81|false;b_31112294|-90|false;b_40362685|-77|false   \n",
       "1106217   b_22472878|-67|false;b_28660854|-77|false;b_40209888|-47|true;b_51726487|-72|false;b_53667050|-66|false;b_26613990|-84|false;b_44415092|-67|false;b_14580929|-75|false;b_40362685|-80|false;b_16409372|-71|false   \n",
       "1107285   b_40209888|-54|false;b_44415092|-73|false;b_22472878|-67|false;b_30379445|-73|false;b_49278725|-75|false;b_52048604|-67|false;b_40059953|-73|false;b_40059952|-76|false;b_51726487|-76|false;b_3933545|-43|false   \n",
       "1107760  b_30379445|-75|false;b_51048975|-75|false;b_26467980|-79|false;b_40059952|-82|false;b_52048607|-78|false;b_40209888|-62|false;b_12705141|-76|false;b_22472878|-77|false;b_52048604|-74|false;b_55289296|-79|false   \n",
       "1107808  b_21514052|-67|false;b_51726487|-81|false;b_14753472|-60|false;b_26467980|-77|false;b_52048607|-68|false;b_52048604|-73|false;b_15638003|-71|false;b_40209888|-53|false;b_40362685|-78|false;b_51048975|-67|false   \n",
       "1110020  b_31112294|-87|false;b_22472878|-84|false;b_28660854|-85|false;b_12705141|-74|false;b_38366281|-88|false;b_26467980|-75|false;b_52048604|-78|false;b_40209888|-69|false;b_51271474|-76|false;b_30559295|-81|false   \n",
       "1110021  b_55289296|-78|false;b_51726487|-74|false;b_40059952|-79|false;b_53667045|-83|false;b_22472878|-75|false;b_40059953|-75|false;b_30379445|-78|false;b_44415092|-79|false;b_15638003|-75|false;b_40209888|-58|false   \n",
       "1110029  b_44434668|-74|false;b_53667047|-84|false;b_20874926|-74|false;b_52048607|-75|false;b_51726487|-73|false;b_28660854|-76|false;b_53667045|-89|false;b_40209888|-54|false;b_53667051|-86|false;b_40362685|-77|false   \n",
       "1117927  b_48793119|-76|false;b_39574079|-76|false;b_53667045|-92|false;b_51271474|-66|false;b_12705141|-67|false;b_22472878|-69|false;b_40362685|-73|false;b_34039495|-77|false;b_51726487|-71|false;b_40209888|-55|false   \n",
       "1118268  b_52048604|-69|false;b_26467980|-76|false;b_26467981|-88|false;b_38366281|-80|false;b_40209888|-58|false;b_44415092|-70|false;b_51048975|-79|false;b_39574079|-81|false;b_12705141|-75|false;b_51726487|-84|false   \n",
       "1119906  b_15638003|-88|false;b_51726487|-82|false;b_22472878|-87|false;b_48793119|-89|false;b_44200910|-84|false;b_26467980|-88|false;b_52048604|-81|false;b_49278725|-89|false;b_51048975|-73|false;b_40209888|-64|false   \n",
       "1120462   b_51048975|-76|false;b_44434668|-77|false;b_44499866|-73|false;b_17079184|-74|false;b_40209888|-58|false;b_51726487|-71|false;b_52048607|-72|false;b_55289296|-85|false;b_22472878|-76|false;b_3417117|-78|false   \n",
       "1120895  b_52048607|-79|false;b_51726487|-78|false;b_22472878|-74|false;b_52048604|-69|false;b_54149190|-85|false;b_31112294|-78|false;b_40209888|-48|false;b_55016265|-75|false;b_26467980|-73|false;b_44415092|-73|false   \n",
       "1124211  b_49278725|-76|false;b_22472878|-71|false;b_55312507|-77|false;b_12705141|-70|false;b_40059953|-78|false;b_53667045|-89|false;b_40209888|-61|false;b_38366281|-77|false;b_40059952|-75|false;b_40362685|-74|false   \n",
       "1124212  b_39574079|-78|false;b_30379445|-73|false;b_40209888|-54|false;b_12705141|-77|false;b_51726487|-79|false;b_40362685|-78|false;b_49278725|-77|false;b_55289296|-76|false;b_52048607|-75|false;b_51271474|-73|false   \n",
       "1132673  b_40209888|-59|false;b_15638004|-87|false;b_15638003|-69|false;b_49278725|-77|false;b_17079182|-87|false;b_48793119|-76|false;b_52048607|-76|false;b_38366281|-76|false;b_52048604|-68|false;b_22472878|-72|false   \n",
       "1134060  b_40209888|-53|false;b_53667051|-89|false;b_53667045|-86|false;b_52025240|-76|false;b_52048607|-73|false;b_49278725|-78|false;b_52048604|-70|false;b_22472878|-67|false;b_53667061|-91|false;b_51726487|-72|false   \n",
       "\n",
       "        category_id  shop_longitude  shop_latitude  price     ...       \\\n",
       "2374           c_28       117.25607      36.899736     40     ...        \n",
       "3217           c_28       117.25607      36.899736     40     ...        \n",
       "5512           c_28       117.25607      36.899736     40     ...        \n",
       "5603           c_28       117.25607      36.899736     40     ...        \n",
       "6358           c_28       117.25607      36.899736     40     ...        \n",
       "7692           c_28       117.25607      36.899736     40     ...        \n",
       "7919           c_28       117.25607      36.899736     40     ...        \n",
       "15241          c_28       117.25607      36.899736     40     ...        \n",
       "15242          c_28       117.25607      36.899736     40     ...        \n",
       "16201          c_28       117.25607      36.899736     40     ...        \n",
       "17102          c_28       117.25607      36.899736     40     ...        \n",
       "17267          c_28       117.25607      36.899736     40     ...        \n",
       "19018          c_28       117.25607      36.899736     40     ...        \n",
       "22419          c_28       117.25607      36.899736     40     ...        \n",
       "24196          c_28       117.25607      36.899736     40     ...        \n",
       "45444          c_28       117.25607      36.899736     40     ...        \n",
       "46295          c_28       117.25607      36.899736     40     ...        \n",
       "48004          c_28       117.25607      36.899736     40     ...        \n",
       "56501          c_28       117.25607      36.899736     40     ...        \n",
       "57408          c_28       117.25607      36.899736     40     ...        \n",
       "69517          c_28       117.25607      36.899736     40     ...        \n",
       "73730          c_28       117.25607      36.899736     40     ...        \n",
       "73966          c_28       117.25607      36.899736     40     ...        \n",
       "74309          c_28       117.25607      36.899736     40     ...        \n",
       "83993          c_28       117.25607      36.899736     40     ...        \n",
       "88117          c_28       117.25607      36.899736     40     ...        \n",
       "92536          c_28       117.25607      36.899736     40     ...        \n",
       "94712          c_28       117.25607      36.899736     40     ...        \n",
       "99169          c_28       117.25607      36.899736     40     ...        \n",
       "99597          c_28       117.25607      36.899736     40     ...        \n",
       "...             ...             ...            ...    ...     ...        \n",
       "1058701        c_28       117.25607      36.899736     40     ...        \n",
       "1069387        c_28       117.25607      36.899736     40     ...        \n",
       "1070075        c_28       117.25607      36.899736     40     ...        \n",
       "1075865        c_28       117.25607      36.899736     40     ...        \n",
       "1076618        c_28       117.25607      36.899736     40     ...        \n",
       "1082274        c_28       117.25607      36.899736     40     ...        \n",
       "1083580        c_28       117.25607      36.899736     40     ...        \n",
       "1083967        c_28       117.25607      36.899736     40     ...        \n",
       "1086135        c_28       117.25607      36.899736     40     ...        \n",
       "1089420        c_28       117.25607      36.899736     40     ...        \n",
       "1092357        c_28       117.25607      36.899736     40     ...        \n",
       "1094189        c_28       117.25607      36.899736     40     ...        \n",
       "1101291        c_28       117.25607      36.899736     40     ...        \n",
       "1104122        c_28       117.25607      36.899736     40     ...        \n",
       "1106217        c_28       117.25607      36.899736     40     ...        \n",
       "1107285        c_28       117.25607      36.899736     40     ...        \n",
       "1107760        c_28       117.25607      36.899736     40     ...        \n",
       "1107808        c_28       117.25607      36.899736     40     ...        \n",
       "1110020        c_28       117.25607      36.899736     40     ...        \n",
       "1110021        c_28       117.25607      36.899736     40     ...        \n",
       "1110029        c_28       117.25607      36.899736     40     ...        \n",
       "1117927        c_28       117.25607      36.899736     40     ...        \n",
       "1118268        c_28       117.25607      36.899736     40     ...        \n",
       "1119906        c_28       117.25607      36.899736     40     ...        \n",
       "1120462        c_28       117.25607      36.899736     40     ...        \n",
       "1120895        c_28       117.25607      36.899736     40     ...        \n",
       "1124211        c_28       117.25607      36.899736     40     ...        \n",
       "1124212        c_28       117.25607      36.899736     40     ...        \n",
       "1132673        c_28       117.25607      36.899736     40     ...        \n",
       "1134060        c_28       117.25607      36.899736     40     ...        \n",
       "\n",
       "                                                                                                                                                                                                  basic_wifi_info  \\\n",
       "2374     (10, [], [(b_52168829, -78), (b_26467949, -73), (b_17079182, -74), (b_40209888, -54), (b_44415092, -78), (b_51726487, -75), (b_52048604, -76), (b_44200910, -81), (b_39574079, -79), (b_51048975, -77)])   \n",
       "3217     (10, [], [(b_40362685, -86), (b_17079184, -87), (b_51726487, -86), (b_48793119, -90), (b_40209888, -68), (b_15638003, -77), (b_49278725, -92), (b_15638004, -87), (b_52048604, -87), (b_51271474, -83)])   \n",
       "5512     (10, [], [(b_51048975, -75), (b_12705141, -75), (b_49278725, -76), (b_51726487, -68), (b_40209888, -65), (b_22472878, -74), (b_52025240, -81), (b_40362685, -79), (b_53340470, -66), (b_44415092, -75)])   \n",
       "5603     (10, [], [(b_38366281, -78), (b_51726487, -73), (b_17079184, -78), (b_40059953, -80), (b_30559295, -77), (b_55016265, -76), (b_40209888, -52), (b_17079183, -78), (b_15638003, -79), (b_44415092, -77)])   \n",
       "6358     (10, [], [(b_15638003, -83), (b_48793119, -81), (b_44200910, -86), (b_49278725, -83), (b_55289296, -84), (b_39574079, -86), (b_51726487, -76), (b_28660854, -86), (b_22472878, -76), (b_40209888, -59)])   \n",
       "7692     (10, [], [(b_49278725, -76), (b_48793119, -78), (b_40209888, -57), (b_26467980, -71), (b_51271474, -69), (b_12705141, -76), (b_30379445, -80), (b_52048604, -67), (b_17079182, -84), (b_53667045, -87)])   \n",
       "7919                                            (8, [], [(b_40059953, -86), (b_26467980, -80), (b_40209888, -61), (b_51726487, -72), (b_52048604, -79), (b_52048607, -84), (b_39574079, -80), (b_40362685, -78)])   \n",
       "15241    (10, [], [(b_51726487, -65), (b_44322551, -80), (b_26467949, -77), (b_30118357, -86), (b_44434668, -83), (b_52048604, -77), (b_52048607, -78), (b_51048975, -84), (b_40362685, -84), (b_40209888, -61)])   \n",
       "15242    (10, [], [(b_51726487, -68), (b_40209888, -60), (b_52048607, -69), (b_14580929, -83), (b_17079184, -81), (b_30379445, -78), (b_44415092, -75), (b_49278725, -80), (b_51048975, -74), (b_22472878, -77)])   \n",
       "16201    (10, [], [(b_26467980, -70), (b_30379445, -88), (b_48793119, -93), (b_51726487, -80), (b_52048607, -85), (b_39574079, -83), (b_44415092, -73), (b_49278725, -90), (b_40209888, -72), (b_12705141, -87)])   \n",
       "17102     (10, [], [(b_51726487, -73), (b_3046873, -70), (b_17079184, -81), (b_40209888, -49), (b_52048604, -64), (b_17079183, -82), (b_55016265, -80), (b_48249102, -77), (b_44415092, -76), (b_30559295, -76)])   \n",
       "17267    (10, [], [(b_39574079, -77), (b_40362685, -77), (b_51145806, -81), (b_52048607, -71), (b_40209888, -56), (b_51726487, -71), (b_31112294, -81), (b_52025241, -80), (b_26613990, -85), (b_22472878, -71)])   \n",
       "19018    (10, [], [(b_51726487, -77), (b_48793119, -79), (b_40209888, -61), (b_40059953, -83), (b_17079183, -86), (b_44434668, -86), (b_52048604, -88), (b_44415092, -78), (b_49278725, -78), (b_52048607, -85)])   \n",
       "22419     (10, [], [(b_20874926, -78), (b_52048605, -83), (b_52048607, -77), (b_40209888, -60), (b_51726487, -79), (b_3114529, -76), (b_28660854, -76), (b_26467949, -82), (b_51048975, -74), (b_52048604, -78)])   \n",
       "24196    (10, [], [(b_53667045, -89), (b_40209888, -57), (b_44200910, -86), (b_49148817, -89), (b_30379445, -79), (b_44415092, -81), (b_51726487, -76), (b_15638004, -90), (b_15638003, -82), (b_22472878, -81)])   \n",
       "45444    (10, [], [(b_51726487, -80), (b_28660854, -78), (b_31112294, -85), (b_51048975, -71), (b_40209888, -61), (b_52048604, -68), (b_44415092, -79), (b_22472878, -74), (b_52048607, -79), (b_40362685, -71)])   \n",
       "46295     (10, [], [(b_39574079, -73), (b_12705141, -77), (b_55289296, -81), (b_52048604, -70), (b_49278725, -76), (b_48793119, -78), (b_51726487, -76), (b_9025795, -85), (b_40209888, -55), (b_52048607, -77)])   \n",
       "48004    (10, [], [(b_48793119, -79), (b_22472878, -72), (b_40059953, -80), (b_40209888, -59), (b_52048604, -75), (b_44415092, -79), (b_51726487, -76), (b_49278725, -79), (b_55289296, -81), (b_26467980, -79)])   \n",
       "56501    (10, [], [(b_39574079, -90), (b_55289296, -79), (b_40209888, -66), (b_51726487, -87), (b_30379445, -81), (b_22472878, -85), (b_49278725, -91), (b_52048604, -87), (b_48793119, -89), (b_49148816, -87)])   \n",
       "57408    (10, [], [(b_22472878, -81), (b_40059952, -78), (b_44415092, -81), (b_55289296, -91), (b_40059953, -77), (b_52048604, -73), (b_26467980, -79), (b_40209888, -73), (b_30379445, -88), (b_51726487, -80)])   \n",
       "69517    (10, [], [(b_51726487, -64), (b_44415092, -73), (b_26467980, -74), (b_30379445, -64), (b_52048604, -63), (b_40209888, -48), (b_55289296, -63), (b_22472878, -66), (b_12705141, -71), (b_49148817, -88)])   \n",
       "73730    (10, [], [(b_22472878, -76), (b_39574079, -78), (b_12705141, -69), (b_30379445, -76), (b_52025241, -82), (b_14580929, -76), (b_49148817, -84), (b_51726487, -79), (b_55289296, -76), (b_40209888, -58)])   \n",
       "73966    (10, [], [(b_49148817, -86), (b_55289296, -83), (b_30379445, -82), (b_40209888, -58), (b_44434668, -82), (b_44200910, -80), (b_38366281, -71), (b_17079182, -88), (b_52048604, -68), (b_51726487, -74)])   \n",
       "74309                                                              (7, [], [(b_15638003, -88), (b_26468126, -91), (b_40209888, -65), (b_34893889, -82), (b_51048975, -81), (b_44434668, -84), (b_48793119, -82)])   \n",
       "83993    (10, [], [(b_12705141, -84), (b_40059953, -86), (b_48793119, -91), (b_26467980, -86), (b_52025240, -87), (b_40209888, -63), (b_44415092, -88), (b_51726487, -74), (b_40059952, -85), (b_48249102, -92)])   \n",
       "88117    (10, [], [(b_52048604, -60), (b_52025240, -76), (b_44200910, -72), (b_40059953, -76), (b_40209888, -58), (b_40059952, -70), (b_17079183, -67), (b_38300990, -74), (b_52048607, -75), (b_51726487, -66)])   \n",
       "92536    (10, [], [(b_55016265, -88), (b_40362685, -83), (b_30379445, -88), (b_40209888, -67), (b_51726487, -86), (b_51271474, -86), (b_52048607, -85), (b_48249102, -86), (b_49278725, -84), (b_48793119, -82)])   \n",
       "94712    (10, [], [(b_23895926, -86), (b_55289296, -65), (b_51726487, -72), (b_52048604, -74), (b_52048607, -72), (b_44415092, -74), (b_40059952, -87), (b_19770055, -88), (b_30379445, -69), (b_40209888, -60)])   \n",
       "99169    (10, [], [(b_26467981, -92), (b_55312507, -80), (b_52048599, -91), (b_48793119, -84), (b_40209888, -68), (b_51726487, -79), (b_40362685, -79), (b_51048975, -77), (b_50640451, -86), (b_53667047, -90)])   \n",
       "99597     (10, [], [(b_40209888, -63), (b_52048607, -77), (b_44415092, -81), (b_48793119, -86), (b_51726487, -81), (b_3918679, -81), (b_49278725, -86), (b_48249102, -84), (b_40059952, -84), (b_40059953, -78)])   \n",
       "...                                                                                                                                                                                                           ...   \n",
       "1058701  (10, [], [(b_22472878, -83), (b_17079183, -86), (b_40209888, -60), (b_51726487, -75), (b_52048607, -79), (b_40362685, -84), (b_52025241, -89), (b_52048604, -80), (b_17079184, -85), (b_51048975, -87)])   \n",
       "1069387  (10, [], [(b_22472878, -71), (b_52048604, -74), (b_51726487, -77), (b_26467980, -81), (b_40209888, -63), (b_55289296, -85), (b_44415092, -83), (b_30379445, -88), (b_40059952, -74), (b_52048607, -79)])   \n",
       "1070075  (10, [], [(b_55289296, -75), (b_44415092, -64), (b_40209888, -58), (b_40362685, -81), (b_51726487, -69), (b_30379445, -73), (b_26467981, -85), (b_39574079, -72), (b_22472878, -74), (b_12705141, -74)])   \n",
       "1075865  (10, [], [(b_33962055, -85), (b_52048604, -74), (b_28660854, -76), (b_55289296, -78), (b_51726487, -75), (b_30118357, -86), (b_44415092, -72), (b_14580929, -72), (b_40209888, -60), (b_30379445, -78)])   \n",
       "1076618  (10, [], [(b_15638003, -84), (b_42415389, -81), (b_12705141, -80), (b_49278725, -86), (b_52048604, -84), (b_44415092, -88), (b_51726487, -77), (b_40209888, -56), (b_53667047, -88), (b_53667045, -89)])   \n",
       "1082274                                                                                                                     (4, [], [(b_44434668, -91), (b_48793119, -84), (b_52048604, -84), (b_40209888, -62)])   \n",
       "1083580  (10, [], [(b_51696066, -82), (b_51726487, -72), (b_52048607, -88), (b_52048604, -76), (b_40209888, -64), (b_22472878, -77), (b_48793119, -79), (b_49278725, -79), (b_40059953, -75), (b_26467949, -82)])   \n",
       "1083967  (10, [], [(b_26467980, -82), (b_30379445, -81), (b_52048607, -73), (b_22472878, -83), (b_44959978, -58), (b_50640451, -80), (b_40362685, -72), (b_51726487, -78), (b_40209888, -55), (b_44415092, -75)])   \n",
       "1086135  (10, [], [(b_51726487, -70), (b_40209888, -52), (b_44200910, -77), (b_44415092, -72), (b_22472878, -69), (b_40059953, -77), (b_40059952, -70), (b_26467980, -78), (b_55016265, -72), (b_28660854, -77)])   \n",
       "1089420  (10, [], [(b_49148817, -92), (b_51726487, -73), (b_55289296, -83), (b_52048604, -66), (b_40059953, -88), (b_44415092, -85), (b_53667045, -93), (b_22472878, -73), (b_40209888, -58), (b_49278725, -84)])   \n",
       "1092357  (10, [], [(b_28660854, -79), (b_22472878, -77), (b_12705141, -80), (b_44874709, -78), (b_52048604, -77), (b_49278725, -77), (b_51726487, -77), (b_48793119, -80), (b_55289296, -80), (b_40209888, -51)])   \n",
       "1094189  (10, [], [(b_53667045, -87), (b_22472878, -74), (b_49278725, -78), (b_52048607, -71), (b_51271474, -68), (b_26467949, -77), (b_53667051, -88), (b_48793119, -78), (b_52048604, -71), (b_40209888, -55)])   \n",
       "1101291  (10, [], [(b_51726487, -82), (b_40209888, -50), (b_22472878, -74), (b_52048604, -72), (b_40059952, -77), (b_23895926, -86), (b_30379445, -75), (b_44200910, -72), (b_55289296, -74), (b_44415092, -66)])   \n",
       "1104122  (10, [], [(b_52048607, -81), (b_40209888, -57), (b_40059952, -78), (b_44317488, -88), (b_44415092, -80), (b_26467980, -79), (b_51726487, -81), (b_40059953, -81), (b_31112294, -90), (b_40362685, -77)])   \n",
       "1106217    (10, [(b_40209888, -47)], [(b_22472878, -67), (b_28660854, -77), (b_51726487, -72), (b_53667050, -66), (b_26613990, -84), (b_44415092, -67), (b_14580929, -75), (b_40362685, -80), (b_16409372, -71)])   \n",
       "1107285   (10, [], [(b_40209888, -54), (b_44415092, -73), (b_22472878, -67), (b_30379445, -73), (b_49278725, -75), (b_52048604, -67), (b_40059953, -73), (b_40059952, -76), (b_51726487, -76), (b_3933545, -43)])   \n",
       "1107760  (10, [], [(b_30379445, -75), (b_51048975, -75), (b_26467980, -79), (b_40059952, -82), (b_52048607, -78), (b_40209888, -62), (b_12705141, -76), (b_22472878, -77), (b_52048604, -74), (b_55289296, -79)])   \n",
       "1107808  (10, [], [(b_21514052, -67), (b_51726487, -81), (b_14753472, -60), (b_26467980, -77), (b_52048607, -68), (b_52048604, -73), (b_15638003, -71), (b_40209888, -53), (b_40362685, -78), (b_51048975, -67)])   \n",
       "1110020  (10, [], [(b_31112294, -87), (b_22472878, -84), (b_28660854, -85), (b_12705141, -74), (b_38366281, -88), (b_26467980, -75), (b_52048604, -78), (b_40209888, -69), (b_51271474, -76), (b_30559295, -81)])   \n",
       "1110021  (10, [], [(b_55289296, -78), (b_51726487, -74), (b_40059952, -79), (b_53667045, -83), (b_22472878, -75), (b_40059953, -75), (b_30379445, -78), (b_44415092, -79), (b_15638003, -75), (b_40209888, -58)])   \n",
       "1110029  (10, [], [(b_44434668, -74), (b_53667047, -84), (b_20874926, -74), (b_52048607, -75), (b_51726487, -73), (b_28660854, -76), (b_53667045, -89), (b_40209888, -54), (b_53667051, -86), (b_40362685, -77)])   \n",
       "1117927  (10, [], [(b_48793119, -76), (b_39574079, -76), (b_53667045, -92), (b_51271474, -66), (b_12705141, -67), (b_22472878, -69), (b_40362685, -73), (b_34039495, -77), (b_51726487, -71), (b_40209888, -55)])   \n",
       "1118268  (10, [], [(b_52048604, -69), (b_26467980, -76), (b_26467981, -88), (b_38366281, -80), (b_40209888, -58), (b_44415092, -70), (b_51048975, -79), (b_39574079, -81), (b_12705141, -75), (b_51726487, -84)])   \n",
       "1119906  (10, [], [(b_15638003, -88), (b_51726487, -82), (b_22472878, -87), (b_48793119, -89), (b_44200910, -84), (b_26467980, -88), (b_52048604, -81), (b_49278725, -89), (b_51048975, -73), (b_40209888, -64)])   \n",
       "1120462   (10, [], [(b_51048975, -76), (b_44434668, -77), (b_44499866, -73), (b_17079184, -74), (b_40209888, -58), (b_51726487, -71), (b_52048607, -72), (b_55289296, -85), (b_22472878, -76), (b_3417117, -78)])   \n",
       "1120895  (10, [], [(b_52048607, -79), (b_51726487, -78), (b_22472878, -74), (b_52048604, -69), (b_54149190, -85), (b_31112294, -78), (b_40209888, -48), (b_55016265, -75), (b_26467980, -73), (b_44415092, -73)])   \n",
       "1124211  (10, [], [(b_49278725, -76), (b_22472878, -71), (b_55312507, -77), (b_12705141, -70), (b_40059953, -78), (b_53667045, -89), (b_40209888, -61), (b_38366281, -77), (b_40059952, -75), (b_40362685, -74)])   \n",
       "1124212  (10, [], [(b_39574079, -78), (b_30379445, -73), (b_40209888, -54), (b_12705141, -77), (b_51726487, -79), (b_40362685, -78), (b_49278725, -77), (b_55289296, -76), (b_52048607, -75), (b_51271474, -73)])   \n",
       "1132673  (10, [], [(b_40209888, -59), (b_15638004, -87), (b_15638003, -69), (b_49278725, -77), (b_17079182, -87), (b_48793119, -76), (b_52048607, -76), (b_38366281, -76), (b_52048604, -68), (b_22472878, -72)])   \n",
       "1134060  (10, [], [(b_40209888, -53), (b_53667051, -89), (b_53667045, -86), (b_52025240, -76), (b_52048607, -73), (b_49278725, -78), (b_52048604, -70), (b_22472878, -67), (b_53667061, -91), (b_51726487, -72)])   \n",
       "\n",
       "        wifi_size  use_wifi_size  no_use_wifi_size  use_wifi_freq  \\\n",
       "2374           10              0                10            0.0   \n",
       "3217           10              0                10            0.0   \n",
       "5512           10              0                10            0.0   \n",
       "5603           10              0                10            0.0   \n",
       "6358           10              0                10            0.0   \n",
       "7692           10              0                10            0.0   \n",
       "7919            8              0                 8            0.0   \n",
       "15241          10              0                10            0.0   \n",
       "15242          10              0                10            0.0   \n",
       "16201          10              0                10            0.0   \n",
       "17102          10              0                10            0.0   \n",
       "17267          10              0                10            0.0   \n",
       "19018          10              0                10            0.0   \n",
       "22419          10              0                10            0.0   \n",
       "24196          10              0                10            0.0   \n",
       "45444          10              0                10            0.0   \n",
       "46295          10              0                10            0.0   \n",
       "48004          10              0                10            0.0   \n",
       "56501          10              0                10            0.0   \n",
       "57408          10              0                10            0.0   \n",
       "69517          10              0                10            0.0   \n",
       "73730          10              0                10            0.0   \n",
       "73966          10              0                10            0.0   \n",
       "74309           7              0                 7            0.0   \n",
       "83993          10              0                10            0.0   \n",
       "88117          10              0                10            0.0   \n",
       "92536          10              0                10            0.0   \n",
       "94712          10              0                10            0.0   \n",
       "99169          10              0                10            0.0   \n",
       "99597          10              0                10            0.0   \n",
       "...           ...            ...               ...            ...   \n",
       "1058701        10              0                10            0.0   \n",
       "1069387        10              0                10            0.0   \n",
       "1070075        10              0                10            0.0   \n",
       "1075865        10              0                10            0.0   \n",
       "1076618        10              0                10            0.0   \n",
       "1082274         4              0                 4            0.0   \n",
       "1083580        10              0                10            0.0   \n",
       "1083967        10              0                10            0.0   \n",
       "1086135        10              0                10            0.0   \n",
       "1089420        10              0                10            0.0   \n",
       "1092357        10              0                10            0.0   \n",
       "1094189        10              0                10            0.0   \n",
       "1101291        10              0                10            0.0   \n",
       "1104122        10              0                10            0.0   \n",
       "1106217        10              1                 9            0.1   \n",
       "1107285        10              0                10            0.0   \n",
       "1107760        10              0                10            0.0   \n",
       "1107808        10              0                10            0.0   \n",
       "1110020        10              0                10            0.0   \n",
       "1110021        10              0                10            0.0   \n",
       "1110029        10              0                10            0.0   \n",
       "1117927        10              0                10            0.0   \n",
       "1118268        10              0                10            0.0   \n",
       "1119906        10              0                10            0.0   \n",
       "1120462        10              0                10            0.0   \n",
       "1120895        10              0                10            0.0   \n",
       "1124211        10              0                10            0.0   \n",
       "1124212        10              0                10            0.0   \n",
       "1132673        10              0                10            0.0   \n",
       "1134060        10              0                10            0.0   \n",
       "\n",
       "        no_use_wifi_freq  i_loc  dayofyear  minute  hour_minute  \n",
       "2374                 1.0     14        223       0         1080  \n",
       "3217                 1.0     27        222      10         1210  \n",
       "5512                 1.0     81        231      40         1240  \n",
       "5603                 1.0     83        235      40         1120  \n",
       "6358                 1.0     92        229      30         1230  \n",
       "7692                 1.0     98        222      20         1220  \n",
       "7919                 1.0    101        236      10         1270  \n",
       "15241                1.0    176        221      10         1090  \n",
       "15242                1.0    177        229      50          710  \n",
       "16201                1.0    183        225      50          830  \n",
       "17102                1.0    197        229      40         1240  \n",
       "17267                1.0    200        226      20          800  \n",
       "19018                1.0    214        236      40         1180  \n",
       "22419                1.0    253        216      10         1210  \n",
       "24196                1.0    288        229      10         1210  \n",
       "45444                1.0    679        231      20          920  \n",
       "46295                1.0    690        232       0          720  \n",
       "48004                1.0    731        228      20         1160  \n",
       "56501                1.0    841        236      20         1220  \n",
       "57408                1.0    848        234       0         1260  \n",
       "69517                1.0   1017        225      10         1090  \n",
       "73730                1.0   1087        226      10          730  \n",
       "73966                1.0   1092        221      30          750  \n",
       "74309                1.0   1098        213       0         1140  \n",
       "83993                1.0   1229        232      30         1110  \n",
       "88117                1.0   1286        232       0         1260  \n",
       "92536                1.0   1341        220      20         1220  \n",
       "94712                1.0   1370        236      10         1330  \n",
       "99169                1.0   1405        222      50          650  \n",
       "99597                1.0   1415        232      30          810  \n",
       "...                  ...    ...        ...     ...          ...  \n",
       "1058701              1.0  15336        217       0          780  \n",
       "1069387              1.0  15499        227      30         1050  \n",
       "1070075              1.0  15503        224      30          630  \n",
       "1075865              1.0  15574        223      20         1280  \n",
       "1076618              1.0  15586        223       0         1140  \n",
       "1082274              1.0  15674        216      10          970  \n",
       "1083580              1.0  15688        228      30         1230  \n",
       "1083967              1.0  15691        224      20         1280  \n",
       "1086135              1.0  15732        229      10         1090  \n",
       "1089420              1.0  15791        226      10          790  \n",
       "1092357              1.0  15825        236      20         1160  \n",
       "1094189              1.0  15855        216      40          940  \n",
       "1101291              1.0  15928        236       0         1260  \n",
       "1104122              1.0  15986        232      10         1330  \n",
       "1106217              0.9  16019        227      20         1280  \n",
       "1107285              1.0  16039        230      40         1060  \n",
       "1107760              1.0  16051        236       0          660  \n",
       "1107808              1.0  16052        217      10         1090  \n",
       "1110020              1.0  16099        222      40          700  \n",
       "1110021              1.0  16100        227      10          790  \n",
       "1110029              1.0  16104        223      20          680  \n",
       "1117927              1.0  16186        219      50          830  \n",
       "1118268              1.0  16188        230       0          720  \n",
       "1119906              1.0  16207        219      40         1180  \n",
       "1120462              1.0  16219        215      10          730  \n",
       "1120895              1.0  16224        225      50         1190  \n",
       "1124211              1.0  16262        228      50          950  \n",
       "1124212              1.0  16263        220       0          960  \n",
       "1132673              1.0  16400        217      10         1090  \n",
       "1134060              1.0  16415        228       0          660  \n",
       "\n",
       "[405 rows x 25 columns]"
      ]
     },
     "execution_count": 331,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s1_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>shop_id</th>\n",
       "      <th>time_stamp</th>\n",
       "      <th>longitude</th>\n",
       "      <th>latitude</th>\n",
       "      <th>wifi_infos</th>\n",
       "      <th>category_id</th>\n",
       "      <th>shop_longitude</th>\n",
       "      <th>shop_latitude</th>\n",
       "      <th>price</th>\n",
       "      <th>...</th>\n",
       "      <th>basic_wifi_info</th>\n",
       "      <th>wifi_size</th>\n",
       "      <th>use_wifi_size</th>\n",
       "      <th>no_use_wifi_size</th>\n",
       "      <th>use_wifi_freq</th>\n",
       "      <th>no_use_wifi_freq</th>\n",
       "      <th>i_loc</th>\n",
       "      <th>dayofyear</th>\n",
       "      <th>minute</th>\n",
       "      <th>hour_minute</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3634</th>\n",
       "      <td>u_584348</td>\n",
       "      <td>s_4009954</td>\n",
       "      <td>2017-08-20 12:10</td>\n",
       "      <td>117.255877</td>\n",
       "      <td>36.899749</td>\n",
       "      <td>b_51726487|-56|false;b_44415092|-76|false;b_55312507|-80|false;b_28660854|-84|false;b_40362685|-77|false;b_52025240|-71|false;b_40209888|-64|false;b_22472878|-76|false;b_44317506|-82|false;b_52048607|-74|false</td>\n",
       "      <td>c_38</td>\n",
       "      <td>117.255892</td>\n",
       "      <td>36.899805</td>\n",
       "      <td>43</td>\n",
       "      <td>...</td>\n",
       "      <td>(10, [], [(b_51726487, -56), (b_44415092, -76), (b_55312507, -80), (b_28660854, -84), (b_40362685, -77), (b_52025240, -71), (b_40209888, -64), (b_22472878, -76), (b_44317506, -82), (b_52048607, -74)])</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>45</td>\n",
       "      <td>232</td>\n",
       "      <td>10</td>\n",
       "      <td>730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12035</th>\n",
       "      <td>u_2013372</td>\n",
       "      <td>s_4009954</td>\n",
       "      <td>2017-08-23 20:50</td>\n",
       "      <td>117.256029</td>\n",
       "      <td>36.899706</td>\n",
       "      <td>b_55312507|-75|false;b_20874926|-73|false;b_44322551|-76|false;b_22472878|-79|false;b_40209888|-71|false;b_51726487|-68|false;b_40362685|-71|false;b_40059952|-79|false;b_50640451|-79|false;b_52048607|-71|false</td>\n",
       "      <td>c_38</td>\n",
       "      <td>117.255892</td>\n",
       "      <td>36.899805</td>\n",
       "      <td>43</td>\n",
       "      <td>...</td>\n",
       "      <td>(10, [], [(b_55312507, -75), (b_20874926, -73), (b_44322551, -76), (b_22472878, -79), (b_40209888, -71), (b_51726487, -68), (b_40362685, -71), (b_40059952, -79), (b_50640451, -79), (b_52048607, -71)])</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>149</td>\n",
       "      <td>235</td>\n",
       "      <td>50</td>\n",
       "      <td>1250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39001</th>\n",
       "      <td>u_8563349</td>\n",
       "      <td>s_4009954</td>\n",
       "      <td>2017-08-20 13:00</td>\n",
       "      <td>117.256001</td>\n",
       "      <td>36.899697</td>\n",
       "      <td>b_48793119|-76|false;b_51726487|-60|false;b_52025240|-82|false;b_19691207|-73|false;b_44415092|-72|false;b_40209888|-65|false;b_49278725|-77|false;b_22472878|-79|false;b_696895|-70|false;b_40362685|-78|false</td>\n",
       "      <td>c_38</td>\n",
       "      <td>117.255892</td>\n",
       "      <td>36.899805</td>\n",
       "      <td>43</td>\n",
       "      <td>...</td>\n",
       "      <td>(10, [], [(b_48793119, -76), (b_51726487, -60), (b_52025240, -82), (b_19691207, -73), (b_44415092, -72), (b_40209888, -65), (b_49278725, -77), (b_22472878, -79), (b_696895, -70), (b_40362685, -78)])</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>475</td>\n",
       "      <td>232</td>\n",
       "      <td>0</td>\n",
       "      <td>780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55657</th>\n",
       "      <td>u_12867996</td>\n",
       "      <td>s_4009954</td>\n",
       "      <td>2017-08-22 20:00</td>\n",
       "      <td>117.255868</td>\n",
       "      <td>36.899704</td>\n",
       "      <td>b_44322551|-76|false;b_40362685|-72|false;b_51726487|-53|false;b_50640451|-69|false;b_22472878|-78|false;b_40209888|-56|false;b_16118920|-69|false;b_52048607|-64|false;b_38366281|-84|false;b_44415092|-80|false</td>\n",
       "      <td>c_38</td>\n",
       "      <td>117.255892</td>\n",
       "      <td>36.899805</td>\n",
       "      <td>43</td>\n",
       "      <td>...</td>\n",
       "      <td>(10, [], [(b_44322551, -76), (b_40362685, -72), (b_51726487, -53), (b_50640451, -69), (b_22472878, -78), (b_40209888, -56), (b_16118920, -69), (b_52048607, -64), (b_38366281, -84), (b_44415092, -80)])</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>833</td>\n",
       "      <td>234</td>\n",
       "      <td>0</td>\n",
       "      <td>1200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64179</th>\n",
       "      <td>u_15312645</td>\n",
       "      <td>s_4009954</td>\n",
       "      <td>2017-08-24 18:40</td>\n",
       "      <td>117.255924</td>\n",
       "      <td>36.899711</td>\n",
       "      <td>b_40059953|-88|false;b_52048607|-65|false;b_22472878|-73|false;b_44317488|-81|false;b_44499866|-80|false;b_48793119|-79|false;b_40209888|-60|false;b_20874926|-69|false;b_15638004|-93|false;b_31517286|-69|false</td>\n",
       "      <td>c_38</td>\n",
       "      <td>117.255892</td>\n",
       "      <td>36.899805</td>\n",
       "      <td>43</td>\n",
       "      <td>...</td>\n",
       "      <td>(10, [], [(b_40059953, -88), (b_52048607, -65), (b_22472878, -73), (b_44317488, -81), (b_44499866, -80), (b_48793119, -79), (b_40209888, -60), (b_20874926, -69), (b_15638004, -93), (b_31517286, -69)])</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>960</td>\n",
       "      <td>236</td>\n",
       "      <td>40</td>\n",
       "      <td>1120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69768</th>\n",
       "      <td>u_17144306</td>\n",
       "      <td>s_4009954</td>\n",
       "      <td>2017-08-22 18:20</td>\n",
       "      <td>117.255994</td>\n",
       "      <td>36.899715</td>\n",
       "      <td>b_22472878|-65|false;b_40059953|-81|false;b_51048975|-66|false;b_49278725|-76|false;b_44415092|-73|false;b_40209888|-62|false;b_30379445|-77|false;b_44200910|-73|false;b_25339934|-69|false;b_40362685|-71|false</td>\n",
       "      <td>c_38</td>\n",
       "      <td>117.255892</td>\n",
       "      <td>36.899805</td>\n",
       "      <td>43</td>\n",
       "      <td>...</td>\n",
       "      <td>(10, [], [(b_22472878, -65), (b_40059953, -81), (b_51048975, -66), (b_49278725, -76), (b_44415092, -73), (b_40209888, -62), (b_30379445, -77), (b_44200910, -73), (b_25339934, -69), (b_40362685, -71)])</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1027</td>\n",
       "      <td>234</td>\n",
       "      <td>20</td>\n",
       "      <td>1100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83995</th>\n",
       "      <td>u_20963816</td>\n",
       "      <td>s_4009954</td>\n",
       "      <td>2017-08-20 18:40</td>\n",
       "      <td>117.255865</td>\n",
       "      <td>36.899757</td>\n",
       "      <td>b_40209888|-71|false;b_51048975|-80|false;b_52025240|-82|false;b_52048607|-79|false;b_51726487|-51|false;b_53667061|-90|false;b_40362685|-75|false;b_44415092|-81|false;b_30118357|-82|false;b_54134135|-84|false</td>\n",
       "      <td>c_38</td>\n",
       "      <td>117.255892</td>\n",
       "      <td>36.899805</td>\n",
       "      <td>43</td>\n",
       "      <td>...</td>\n",
       "      <td>(10, [], [(b_40209888, -71), (b_51048975, -80), (b_52025240, -82), (b_52048607, -79), (b_51726487, -51), (b_53667061, -90), (b_40362685, -75), (b_44415092, -81), (b_30118357, -82), (b_54134135, -84)])</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1231</td>\n",
       "      <td>232</td>\n",
       "      <td>40</td>\n",
       "      <td>1120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84239</th>\n",
       "      <td>u_21031202</td>\n",
       "      <td>s_4009954</td>\n",
       "      <td>2017-08-20 09:50</td>\n",
       "      <td>117.256053</td>\n",
       "      <td>36.899723</td>\n",
       "      <td>b_12705141|-76|false;b_38366281|-85|false;b_49148816|-78|false;b_12695816|-76|false;b_49148817|-86|false;b_38300990|-83|false;b_12695817|-86|false;b_49109253|-81|false;b_26464864|-79|false;b_44200910|-82|false</td>\n",
       "      <td>c_38</td>\n",
       "      <td>117.255892</td>\n",
       "      <td>36.899805</td>\n",
       "      <td>43</td>\n",
       "      <td>...</td>\n",
       "      <td>(10, [], [(b_12705141, -76), (b_38366281, -85), (b_49148816, -78), (b_12695816, -76), (b_49148817, -86), (b_38300990, -83), (b_12695817, -86), (b_49109253, -81), (b_26464864, -79), (b_44200910, -82)])</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1238</td>\n",
       "      <td>232</td>\n",
       "      <td>50</td>\n",
       "      <td>590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103309</th>\n",
       "      <td>u_26386978</td>\n",
       "      <td>s_4009954</td>\n",
       "      <td>2017-08-22 17:50</td>\n",
       "      <td>117.256019</td>\n",
       "      <td>36.899684</td>\n",
       "      <td>b_40059953|-78|false;b_51726487|-71|false;b_52048607|-77|false;b_44415092|-86|false;b_26613990|-87|false;b_50274240|-78|false;b_22472878|-76|false;b_15638003|-81|false;b_48249102|-87|false;b_28733484|-79|false</td>\n",
       "      <td>c_38</td>\n",
       "      <td>117.255892</td>\n",
       "      <td>36.899805</td>\n",
       "      <td>43</td>\n",
       "      <td>...</td>\n",
       "      <td>(10, [], [(b_40059953, -78), (b_51726487, -71), (b_52048607, -77), (b_44415092, -86), (b_26613990, -87), (b_50274240, -78), (b_22472878, -76), (b_15638003, -81), (b_48249102, -87), (b_28733484, -79)])</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1480</td>\n",
       "      <td>234</td>\n",
       "      <td>50</td>\n",
       "      <td>1070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103398</th>\n",
       "      <td>u_26415883</td>\n",
       "      <td>s_4009954</td>\n",
       "      <td>2017-08-24 12:10</td>\n",
       "      <td>117.255861</td>\n",
       "      <td>36.899714</td>\n",
       "      <td>b_52025240|-63|false;b_22472878|-72|false;b_38362969|-82|false;b_40362685|-58|false;b_40059953|-75|false;b_26613990|-77|false;b_44434668|-69|false;b_51726487|-51|false;b_40209888|-57|false;b_44415092|-71|false</td>\n",
       "      <td>c_38</td>\n",
       "      <td>117.255892</td>\n",
       "      <td>36.899805</td>\n",
       "      <td>43</td>\n",
       "      <td>...</td>\n",
       "      <td>(10, [], [(b_52025240, -63), (b_22472878, -72), (b_38362969, -82), (b_40362685, -58), (b_40059953, -75), (b_26613990, -77), (b_44434668, -69), (b_51726487, -51), (b_40209888, -57), (b_44415092, -71)])</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1491</td>\n",
       "      <td>236</td>\n",
       "      <td>10</td>\n",
       "      <td>730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110388</th>\n",
       "      <td>u_28212891</td>\n",
       "      <td>s_4009954</td>\n",
       "      <td>2017-08-24 18:00</td>\n",
       "      <td>117.255951</td>\n",
       "      <td>36.899715</td>\n",
       "      <td>b_49278725|-86|false;b_51726487|-53|false;b_51048975|-71|false;b_40209888|-75|false;b_28660854|-81|false;b_39574079|-85|false;b_34039495|-88|false;b_22472878|-83|false;b_52048607|-71|false;b_52048604|-77|false</td>\n",
       "      <td>c_38</td>\n",
       "      <td>117.255892</td>\n",
       "      <td>36.899805</td>\n",
       "      <td>43</td>\n",
       "      <td>...</td>\n",
       "      <td>(10, [], [(b_49278725, -86), (b_51726487, -53), (b_51048975, -71), (b_40209888, -75), (b_28660854, -81), (b_39574079, -85), (b_34039495, -88), (b_22472878, -83), (b_52048607, -71), (b_52048604, -77)])</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1606</td>\n",
       "      <td>236</td>\n",
       "      <td>0</td>\n",
       "      <td>1080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117931</th>\n",
       "      <td>u_30471722</td>\n",
       "      <td>s_4009954</td>\n",
       "      <td>2017-08-22 20:20</td>\n",
       "      <td>117.256009</td>\n",
       "      <td>36.899705</td>\n",
       "      <td>b_55289296|-83|false;b_22472878|-79|false;b_40059953|-84|false;b_52048607|-74|false;b_40362685|-80|false;b_44415092|-80|false;b_30379445|-82|false;b_40209888|-64|false;b_16118920|-80|false;b_51726487|-63|false</td>\n",
       "      <td>c_38</td>\n",
       "      <td>117.255892</td>\n",
       "      <td>36.899805</td>\n",
       "      <td>43</td>\n",
       "      <td>...</td>\n",
       "      <td>(10, [], [(b_55289296, -83), (b_22472878, -79), (b_40059953, -84), (b_52048607, -74), (b_40362685, -80), (b_44415092, -80), (b_30379445, -82), (b_40209888, -64), (b_16118920, -80), (b_51726487, -63)])</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1724</td>\n",
       "      <td>234</td>\n",
       "      <td>20</td>\n",
       "      <td>1220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133574</th>\n",
       "      <td>u_35191941</td>\n",
       "      <td>s_4009954</td>\n",
       "      <td>2017-08-19 17:30</td>\n",
       "      <td>117.256008</td>\n",
       "      <td>36.899742</td>\n",
       "      <td>b_52048607|-76|false;b_40362685|-80|false;b_51726487|-60|false;b_40059953|-79|false;b_51048975|-79|false;b_44415092|-80|false;b_44317488|-86|false;b_40209888|-75|false;b_55289296|-89|false;b_52025240|-87|false</td>\n",
       "      <td>c_38</td>\n",
       "      <td>117.255892</td>\n",
       "      <td>36.899805</td>\n",
       "      <td>43</td>\n",
       "      <td>...</td>\n",
       "      <td>(10, [], [(b_52048607, -76), (b_40362685, -80), (b_51726487, -60), (b_40059953, -79), (b_51048975, -79), (b_44415092, -80), (b_44317488, -86), (b_40209888, -75), (b_55289296, -89), (b_52025240, -87)])</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1966</td>\n",
       "      <td>231</td>\n",
       "      <td>30</td>\n",
       "      <td>1050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138148</th>\n",
       "      <td>u_36631548</td>\n",
       "      <td>s_4009954</td>\n",
       "      <td>2017-08-22 22:10</td>\n",
       "      <td>117.255997</td>\n",
       "      <td>36.899714</td>\n",
       "      <td>b_49111853|-89|false;b_40059953|-84|false;b_40209888|-80|false;b_38753526|-93|false;b_52025241|-94|false;b_52048607|-80|false;b_40362685|-89|false;b_26467980|-90|false;b_52025240|-91|false;b_50640451|-93|false</td>\n",
       "      <td>c_38</td>\n",
       "      <td>117.255892</td>\n",
       "      <td>36.899805</td>\n",
       "      <td>43</td>\n",
       "      <td>...</td>\n",
       "      <td>(10, [], [(b_49111853, -89), (b_40059953, -84), (b_40209888, -80), (b_38753526, -93), (b_52025241, -94), (b_52048607, -80), (b_40362685, -89), (b_26467980, -90), (b_52025240, -91), (b_50640451, -93)])</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2022</td>\n",
       "      <td>234</td>\n",
       "      <td>10</td>\n",
       "      <td>1330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149329</th>\n",
       "      <td>u_40215514</td>\n",
       "      <td>s_4009954</td>\n",
       "      <td>2017-08-24 18:30</td>\n",
       "      <td>117.255945</td>\n",
       "      <td>36.899710</td>\n",
       "      <td>b_51048975|-66|false;b_51726487|-55|false;b_30379445|-67|false;b_26613990|-69|false;b_49278725|-74|false;b_20874926|-66|false;b_52048604|-75|false;b_40209888|-57|false;b_48793119|-76|false;b_22472878|-75|false</td>\n",
       "      <td>c_38</td>\n",
       "      <td>117.255892</td>\n",
       "      <td>36.899805</td>\n",
       "      <td>43</td>\n",
       "      <td>...</td>\n",
       "      <td>(10, [], [(b_51048975, -66), (b_51726487, -55), (b_30379445, -67), (b_26613990, -69), (b_49278725, -74), (b_20874926, -66), (b_52048604, -75), (b_40209888, -57), (b_48793119, -76), (b_22472878, -75)])</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2203</td>\n",
       "      <td>236</td>\n",
       "      <td>30</td>\n",
       "      <td>1110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149331</th>\n",
       "      <td>u_40215514</td>\n",
       "      <td>s_4009954</td>\n",
       "      <td>2017-08-23 19:10</td>\n",
       "      <td>117.255963</td>\n",
       "      <td>36.899730</td>\n",
       "      <td>b_22472878|-67|false;b_52048607|-62|false;b_20874926|-69|false;b_40362685|-67|false;b_50640451|-64|false;b_44317506|-74|false;b_55312507|-69|false;b_51726487|-60|false;b_44520897|-69|false;b_40209888|-54|false</td>\n",
       "      <td>c_38</td>\n",
       "      <td>117.255892</td>\n",
       "      <td>36.899805</td>\n",
       "      <td>43</td>\n",
       "      <td>...</td>\n",
       "      <td>(10, [], [(b_22472878, -67), (b_52048607, -62), (b_20874926, -69), (b_40362685, -67), (b_50640451, -64), (b_44317506, -74), (b_55312507, -69), (b_51726487, -60), (b_44520897, -69), (b_40209888, -54)])</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2205</td>\n",
       "      <td>235</td>\n",
       "      <td>10</td>\n",
       "      <td>1150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156834</th>\n",
       "      <td>u_42572916</td>\n",
       "      <td>s_4009954</td>\n",
       "      <td>2017-08-21 14:20</td>\n",
       "      <td>117.255992</td>\n",
       "      <td>36.899728</td>\n",
       "      <td>b_48238061|-76|false;b_44322551|-80|false;b_26740399|-77|false;b_52048607|-70|false;b_55312507|-74|false;b_49111853|-80|false;b_40059953|-78|false;b_44434668|-75|false;b_50640451|-69|false;b_40362685|-73|false</td>\n",
       "      <td>c_38</td>\n",
       "      <td>117.255892</td>\n",
       "      <td>36.899805</td>\n",
       "      <td>43</td>\n",
       "      <td>...</td>\n",
       "      <td>(10, [], [(b_48238061, -76), (b_44322551, -80), (b_26740399, -77), (b_52048607, -70), (b_55312507, -74), (b_49111853, -80), (b_40059953, -78), (b_44434668, -75), (b_50640451, -69), (b_40362685, -73)])</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2357</td>\n",
       "      <td>233</td>\n",
       "      <td>20</td>\n",
       "      <td>860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>188551</th>\n",
       "      <td>u_52175426</td>\n",
       "      <td>s_4009954</td>\n",
       "      <td>2017-08-24 20:00</td>\n",
       "      <td>117.255914</td>\n",
       "      <td>36.899698</td>\n",
       "      <td>b_20201918|-83|false;b_26467949|-75|false;b_51726487|-56|false;b_40059953|-89|false;b_40362685|-71|false;b_52048607|-67|false;b_40209888|-59|false;b_20874926|-71|false;b_44415092|-83|false;b_22472878|-78|false</td>\n",
       "      <td>c_38</td>\n",
       "      <td>117.255892</td>\n",
       "      <td>36.899805</td>\n",
       "      <td>43</td>\n",
       "      <td>...</td>\n",
       "      <td>(10, [], [(b_20201918, -83), (b_26467949, -75), (b_51726487, -56), (b_40059953, -89), (b_40362685, -71), (b_52048607, -67), (b_40209888, -59), (b_20874926, -71), (b_44415092, -83), (b_22472878, -78)])</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2806</td>\n",
       "      <td>236</td>\n",
       "      <td>0</td>\n",
       "      <td>1200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189523</th>\n",
       "      <td>u_52552188</td>\n",
       "      <td>s_4009954</td>\n",
       "      <td>2017-08-24 18:20</td>\n",
       "      <td>117.255852</td>\n",
       "      <td>36.899730</td>\n",
       "      <td>b_22472878|-78|false;b_48793119|-80|false;b_44322551|-76|false;b_55312507|-77|false;b_52025240|-74|false;b_40362685|-63|false;b_51726487|-60|false;b_20874926|-76|false;b_52048607|-64|false;b_40209888|-63|false</td>\n",
       "      <td>c_38</td>\n",
       "      <td>117.255892</td>\n",
       "      <td>36.899805</td>\n",
       "      <td>43</td>\n",
       "      <td>...</td>\n",
       "      <td>(10, [], [(b_22472878, -78), (b_48793119, -80), (b_44322551, -76), (b_55312507, -77), (b_52025240, -74), (b_40362685, -63), (b_51726487, -60), (b_20874926, -76), (b_52048607, -64), (b_40209888, -63)])</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2821</td>\n",
       "      <td>236</td>\n",
       "      <td>20</td>\n",
       "      <td>1100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>191582</th>\n",
       "      <td>u_53100482</td>\n",
       "      <td>s_4009954</td>\n",
       "      <td>2017-08-24 20:10</td>\n",
       "      <td>117.255827</td>\n",
       "      <td>36.899701</td>\n",
       "      <td>b_51726487|-54|false;b_40209888|-67|false;b_40059953|-87|false;b_20874926|-79|false;b_52048607|-69|false;b_44322551|-78|false;b_22472878|-74|false;b_40059952|-84|false;b_40362685|-68|false;b_19555539|-70|false</td>\n",
       "      <td>c_38</td>\n",
       "      <td>117.255892</td>\n",
       "      <td>36.899805</td>\n",
       "      <td>43</td>\n",
       "      <td>...</td>\n",
       "      <td>(10, [], [(b_51726487, -54), (b_40209888, -67), (b_40059953, -87), (b_20874926, -79), (b_52048607, -69), (b_44322551, -78), (b_22472878, -74), (b_40059952, -84), (b_40362685, -68), (b_19555539, -70)])</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2861</td>\n",
       "      <td>236</td>\n",
       "      <td>10</td>\n",
       "      <td>1210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>191793</th>\n",
       "      <td>u_53159700</td>\n",
       "      <td>s_4009954</td>\n",
       "      <td>2017-08-21 21:40</td>\n",
       "      <td>117.255956</td>\n",
       "      <td>36.899725</td>\n",
       "      <td>b_40059952|-88|false;b_51726487|-64|false;b_40362685|-76|false;b_40059953|-84|false;b_53667045|-88|false;b_44499866|-80|false;b_40209888|-65|false;b_50640451|-81|false;b_44415092|-83|false;b_52048607|-74|false</td>\n",
       "      <td>c_38</td>\n",
       "      <td>117.255892</td>\n",
       "      <td>36.899805</td>\n",
       "      <td>43</td>\n",
       "      <td>...</td>\n",
       "      <td>(10, [], [(b_40059952, -88), (b_51726487, -64), (b_40362685, -76), (b_40059953, -84), (b_53667045, -88), (b_44499866, -80), (b_40209888, -65), (b_50640451, -81), (b_44415092, -83), (b_52048607, -74)])</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2862</td>\n",
       "      <td>233</td>\n",
       "      <td>40</td>\n",
       "      <td>1300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196151</th>\n",
       "      <td>u_54322990</td>\n",
       "      <td>s_4009954</td>\n",
       "      <td>2017-08-20 14:20</td>\n",
       "      <td>117.255897</td>\n",
       "      <td>36.899724</td>\n",
       "      <td>b_40209888|-60|false;b_51726487|-57|false;b_52048607|-71|false;b_40362685|-73|false;b_52025240|-77|false;b_49111853|-79|false;b_44322551|-82|false;b_22472878|-75|false;b_40059953|-90|false;b_26467949|-80|false</td>\n",
       "      <td>c_38</td>\n",
       "      <td>117.255892</td>\n",
       "      <td>36.899805</td>\n",
       "      <td>43</td>\n",
       "      <td>...</td>\n",
       "      <td>(10, [], [(b_40209888, -60), (b_51726487, -57), (b_52048607, -71), (b_40362685, -73), (b_52025240, -77), (b_49111853, -79), (b_44322551, -82), (b_22472878, -75), (b_40059953, -90), (b_26467949, -80)])</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2922</td>\n",
       "      <td>232</td>\n",
       "      <td>20</td>\n",
       "      <td>860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196710</th>\n",
       "      <td>u_54485550</td>\n",
       "      <td>s_4009954</td>\n",
       "      <td>2017-08-22 17:50</td>\n",
       "      <td>117.256012</td>\n",
       "      <td>36.899711</td>\n",
       "      <td>b_51726487|-62|false;b_51048975|-77|false;b_55289296|-79|false;b_40209888|-72|false;b_33837518|-63|false;b_22472878|-75|false;b_40059953|-87|false;b_40362685|-81|false;b_52048607|-70|false;b_30379445|-80|false</td>\n",
       "      <td>c_38</td>\n",
       "      <td>117.255892</td>\n",
       "      <td>36.899805</td>\n",
       "      <td>43</td>\n",
       "      <td>...</td>\n",
       "      <td>(10, [], [(b_51726487, -62), (b_51048975, -77), (b_55289296, -79), (b_40209888, -72), (b_33837518, -63), (b_22472878, -75), (b_40059953, -87), (b_40362685, -81), (b_52048607, -70), (b_30379445, -80)])</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2932</td>\n",
       "      <td>234</td>\n",
       "      <td>50</td>\n",
       "      <td>1070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197301</th>\n",
       "      <td>u_54640711</td>\n",
       "      <td>s_4009954</td>\n",
       "      <td>2017-08-21 18:20</td>\n",
       "      <td>117.256013</td>\n",
       "      <td>36.899744</td>\n",
       "      <td>b_44415092|-79|false;b_52048607|-74|false;b_49278725|-77|false;b_40209888|-65|false;b_22472878|-71|false;b_26467980|-75|false;b_48793119|-77|false;b_44434668|-76|false;b_40059953|-81|false;b_51726487|-65|false</td>\n",
       "      <td>c_38</td>\n",
       "      <td>117.255892</td>\n",
       "      <td>36.899805</td>\n",
       "      <td>43</td>\n",
       "      <td>...</td>\n",
       "      <td>(10, [], [(b_44415092, -79), (b_52048607, -74), (b_49278725, -77), (b_40209888, -65), (b_22472878, -71), (b_26467980, -75), (b_48793119, -77), (b_44434668, -76), (b_40059953, -81), (b_51726487, -65)])</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2939</td>\n",
       "      <td>233</td>\n",
       "      <td>20</td>\n",
       "      <td>1100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199140</th>\n",
       "      <td>u_55119625</td>\n",
       "      <td>s_4009954</td>\n",
       "      <td>2017-08-24 17:40</td>\n",
       "      <td>117.255962</td>\n",
       "      <td>36.899663</td>\n",
       "      <td>b_52048604|-77|false;b_38092938|-90|false;b_40362685|-73|false;b_51726487|-69|false;b_40209888|-64|false;b_14580929|-80|false;b_40059953|-86|false;b_22472878|-84|false;b_12705141|-87|false;b_44197194|-83|false</td>\n",
       "      <td>c_38</td>\n",
       "      <td>117.255892</td>\n",
       "      <td>36.899805</td>\n",
       "      <td>43</td>\n",
       "      <td>...</td>\n",
       "      <td>(10, [], [(b_52048604, -77), (b_38092938, -90), (b_40362685, -73), (b_51726487, -69), (b_40209888, -64), (b_14580929, -80), (b_40059953, -86), (b_22472878, -84), (b_12705141, -87), (b_44197194, -83)])</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2987</td>\n",
       "      <td>236</td>\n",
       "      <td>40</td>\n",
       "      <td>1060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212969</th>\n",
       "      <td>u_59184843</td>\n",
       "      <td>s_4009954</td>\n",
       "      <td>2017-08-20 15:40</td>\n",
       "      <td>117.255882</td>\n",
       "      <td>36.899734</td>\n",
       "      <td>b_52025240|-69|false;b_12695817|-87|false;b_52048607|-72|false;b_52025241|-75|false;b_40209888|-60|false;b_40362685|-75|false;b_40059953|-80|false;b_51726487|-62|false;b_44415092|-80|false;b_22472878|-76|false</td>\n",
       "      <td>c_38</td>\n",
       "      <td>117.255892</td>\n",
       "      <td>36.899805</td>\n",
       "      <td>43</td>\n",
       "      <td>...</td>\n",
       "      <td>(10, [], [(b_52025240, -69), (b_12695817, -87), (b_52048607, -72), (b_52025241, -75), (b_40209888, -60), (b_40362685, -75), (b_40059953, -80), (b_51726487, -62), (b_44415092, -80), (b_22472878, -76)])</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3202</td>\n",
       "      <td>232</td>\n",
       "      <td>40</td>\n",
       "      <td>940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>245974</th>\n",
       "      <td>u_3182669</td>\n",
       "      <td>s_4009954</td>\n",
       "      <td>2017-08-22 09:20</td>\n",
       "      <td>117.255856</td>\n",
       "      <td>36.899739</td>\n",
       "      <td>b_51726487|-57|false;b_55312507|-75|false;b_52048604|-84|false;b_52048607|-60|false;b_40209888|-60|false;b_44499866|-78|false;b_44415092|-80|false;b_44434668|-80|false;b_55289296|-77|false;b_40362640|-89|false</td>\n",
       "      <td>c_38</td>\n",
       "      <td>117.255892</td>\n",
       "      <td>36.899805</td>\n",
       "      <td>43</td>\n",
       "      <td>...</td>\n",
       "      <td>(10, [], [(b_51726487, -57), (b_55312507, -75), (b_52048604, -84), (b_52048607, -60), (b_40209888, -60), (b_44499866, -78), (b_44415092, -80), (b_44434668, -80), (b_55289296, -77), (b_40362640, -89)])</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3633</td>\n",
       "      <td>234</td>\n",
       "      <td>20</td>\n",
       "      <td>560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>262181</th>\n",
       "      <td>u_7131012</td>\n",
       "      <td>s_4009954</td>\n",
       "      <td>2017-08-22 16:50</td>\n",
       "      <td>117.255864</td>\n",
       "      <td>36.899752</td>\n",
       "      <td>b_40209888|-64|false;b_52048604|-76|false;b_22472878|-76|false;b_48793119|-81|false;b_30379445|-85|false;b_26467980|-74|false;b_51048975|-73|false;b_40059952|-76|false;b_49278725|-79|false;b_44415092|-83|false</td>\n",
       "      <td>c_38</td>\n",
       "      <td>117.255892</td>\n",
       "      <td>36.899805</td>\n",
       "      <td>43</td>\n",
       "      <td>...</td>\n",
       "      <td>(10, [], [(b_40209888, -64), (b_52048604, -76), (b_22472878, -76), (b_48793119, -81), (b_30379445, -85), (b_26467980, -74), (b_51048975, -73), (b_40059952, -76), (b_49278725, -79), (b_44415092, -83)])</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3854</td>\n",
       "      <td>234</td>\n",
       "      <td>50</td>\n",
       "      <td>1010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>264519</th>\n",
       "      <td>u_7946115</td>\n",
       "      <td>s_4009954</td>\n",
       "      <td>2017-08-24 10:50</td>\n",
       "      <td>117.255888</td>\n",
       "      <td>36.899730</td>\n",
       "      <td>b_40209888|-63|false;b_3862606|-73|false;b_40059953|-90|false;b_28660854|-80|false;b_38092938|-69|false;b_40362685|-64|false;b_44415092|-69|false;b_14580929|-63|false;b_52048607|-65|false;b_51726487|-56|false</td>\n",
       "      <td>c_38</td>\n",
       "      <td>117.255892</td>\n",
       "      <td>36.899805</td>\n",
       "      <td>43</td>\n",
       "      <td>...</td>\n",
       "      <td>(10, [], [(b_40209888, -63), (b_3862606, -73), (b_40059953, -90), (b_28660854, -80), (b_38092938, -69), (b_40362685, -64), (b_44415092, -69), (b_14580929, -63), (b_52048607, -65), (b_51726487, -56)])</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3882</td>\n",
       "      <td>236</td>\n",
       "      <td>50</td>\n",
       "      <td>650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>273378</th>\n",
       "      <td>u_10802645</td>\n",
       "      <td>s_4009954</td>\n",
       "      <td>2017-08-24 12:10</td>\n",
       "      <td>117.255933</td>\n",
       "      <td>36.899710</td>\n",
       "      <td>b_40362685|-63|false;b_22472878|-68|false;b_51048975|-70|false;b_55312507|-77|false;b_51696063|-83|false;b_44317506|-79|false;b_51726487|-54|false;b_40209888|-73|false;b_49278725|-78|false;b_20201918|-78|false</td>\n",
       "      <td>c_38</td>\n",
       "      <td>117.255892</td>\n",
       "      <td>36.899805</td>\n",
       "      <td>43</td>\n",
       "      <td>...</td>\n",
       "      <td>(10, [], [(b_40362685, -63), (b_22472878, -68), (b_51048975, -70), (b_55312507, -77), (b_51696063, -83), (b_44317506, -79), (b_51726487, -54), (b_40209888, -73), (b_49278725, -78), (b_20201918, -78)])</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4004</td>\n",
       "      <td>236</td>\n",
       "      <td>10</td>\n",
       "      <td>730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>898354</th>\n",
       "      <td>u_59878859</td>\n",
       "      <td>s_4009954</td>\n",
       "      <td>2017-08-20 13:20</td>\n",
       "      <td>117.255881</td>\n",
       "      <td>36.899761</td>\n",
       "      <td>b_52048607|-69|false;b_51726487|-56|false;b_30379445|-88|false;b_48793119|-81|false;b_49278725|-81|false;b_44415092|-79|false;b_24072852|-57|false;b_40209888|-60|false;b_22472878|-71|false;b_40059952|-79|false</td>\n",
       "      <td>c_38</td>\n",
       "      <td>117.255892</td>\n",
       "      <td>36.899805</td>\n",
       "      <td>43</td>\n",
       "      <td>...</td>\n",
       "      <td>(10, [], [(b_52048607, -69), (b_51726487, -56), (b_30379445, -88), (b_48793119, -81), (b_49278725, -81), (b_44415092, -79), (b_24072852, -57), (b_40209888, -60), (b_22472878, -71), (b_40059952, -79)])</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>13079</td>\n",
       "      <td>232</td>\n",
       "      <td>20</td>\n",
       "      <td>800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>942190</th>\n",
       "      <td>u_6317864</td>\n",
       "      <td>s_4009954</td>\n",
       "      <td>2017-08-19 18:00</td>\n",
       "      <td>117.255987</td>\n",
       "      <td>36.899761</td>\n",
       "      <td>b_51726487|-57|false;b_53667051|-91|false;b_53667061|-89|false;b_40362685|-84|false;b_52048607|-77|false;b_52048604|-86|false;b_40209888|-63|false;b_40059953|-82|false;b_28660854|-89|false;b_22472878|-77|false</td>\n",
       "      <td>c_38</td>\n",
       "      <td>117.255892</td>\n",
       "      <td>36.899805</td>\n",
       "      <td>43</td>\n",
       "      <td>...</td>\n",
       "      <td>(10, [], [(b_51726487, -57), (b_53667051, -91), (b_53667061, -89), (b_40362685, -84), (b_52048607, -77), (b_52048604, -86), (b_40209888, -63), (b_40059953, -82), (b_28660854, -89), (b_22472878, -77)])</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>13634</td>\n",
       "      <td>231</td>\n",
       "      <td>0</td>\n",
       "      <td>1080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>953208</th>\n",
       "      <td>u_9946560</td>\n",
       "      <td>s_4009954</td>\n",
       "      <td>2017-08-21 22:00</td>\n",
       "      <td>117.255917</td>\n",
       "      <td>36.899752</td>\n",
       "      <td>b_44322551|-88|false;b_30379445|-88|false;b_55289296|-87|false;b_44415092|-80|false;b_40059953|-84|false;b_40209888|-60|false;b_40362685|-74|false;b_20201918|-87|false;b_38366281|-87|false;b_52048604|-86|false</td>\n",
       "      <td>c_38</td>\n",
       "      <td>117.255892</td>\n",
       "      <td>36.899805</td>\n",
       "      <td>43</td>\n",
       "      <td>...</td>\n",
       "      <td>(10, [], [(b_44322551, -88), (b_30379445, -88), (b_55289296, -87), (b_44415092, -80), (b_40059953, -84), (b_40209888, -60), (b_40362685, -74), (b_20201918, -87), (b_38366281, -87), (b_52048604, -86)])</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>13783</td>\n",
       "      <td>233</td>\n",
       "      <td>0</td>\n",
       "      <td>1320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>967429</th>\n",
       "      <td>u_13402239</td>\n",
       "      <td>s_4009954</td>\n",
       "      <td>2017-08-20 16:10</td>\n",
       "      <td>117.255889</td>\n",
       "      <td>36.899757</td>\n",
       "      <td>b_52048607|-73|false;b_48793119|-84|false;b_40362685|-79|false;b_55312507|-84|false;b_44415092|-79|false;b_51048975|-74|false;b_40059952|-83|false;b_40209888|-74|false;b_51726487|-58|false;b_52025240|-77|false</td>\n",
       "      <td>c_38</td>\n",
       "      <td>117.255892</td>\n",
       "      <td>36.899805</td>\n",
       "      <td>43</td>\n",
       "      <td>...</td>\n",
       "      <td>(10, [], [(b_52048607, -73), (b_48793119, -84), (b_40362685, -79), (b_55312507, -84), (b_44415092, -79), (b_51048975, -74), (b_40059952, -83), (b_40209888, -74), (b_51726487, -58), (b_52025240, -77)])</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>13977</td>\n",
       "      <td>232</td>\n",
       "      <td>10</td>\n",
       "      <td>970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>972530</th>\n",
       "      <td>u_14914560</td>\n",
       "      <td>s_4009954</td>\n",
       "      <td>2017-08-24 11:30</td>\n",
       "      <td>117.255868</td>\n",
       "      <td>36.899711</td>\n",
       "      <td>b_40362685|-71|false;b_40209888|-68|false;b_52048607|-75|false;b_44434668|-75|false;b_51048975|-78|false;b_52048604|-84|false;b_44317506|-78|false;b_52025240|-77|false;b_44415092|-80|false;b_51726487|-63|false</td>\n",
       "      <td>c_38</td>\n",
       "      <td>117.255892</td>\n",
       "      <td>36.899805</td>\n",
       "      <td>43</td>\n",
       "      <td>...</td>\n",
       "      <td>(10, [], [(b_40362685, -71), (b_40209888, -68), (b_52048607, -75), (b_44434668, -75), (b_51048975, -78), (b_52048604, -84), (b_44317506, -78), (b_52025240, -77), (b_44415092, -80), (b_51726487, -63)])</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>14049</td>\n",
       "      <td>236</td>\n",
       "      <td>30</td>\n",
       "      <td>690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>986260</th>\n",
       "      <td>u_19071632</td>\n",
       "      <td>s_4009954</td>\n",
       "      <td>2017-08-24 17:40</td>\n",
       "      <td>117.255887</td>\n",
       "      <td>36.899723</td>\n",
       "      <td>b_51726487|-49|false;b_27468209|-62|false;b_52048607|-70|false;b_38753526|-83|false;b_50640451|-71|false;b_52025240|-68|false;b_55312507|-72|false;b_40209888|-59|false;b_40362685|-82|false;b_51145806|-89|false</td>\n",
       "      <td>c_38</td>\n",
       "      <td>117.255892</td>\n",
       "      <td>36.899805</td>\n",
       "      <td>43</td>\n",
       "      <td>...</td>\n",
       "      <td>(10, [], [(b_51726487, -49), (b_27468209, -62), (b_52048607, -70), (b_38753526, -83), (b_50640451, -71), (b_52025240, -68), (b_55312507, -72), (b_40209888, -59), (b_40362685, -82), (b_51145806, -89)])</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>14226</td>\n",
       "      <td>236</td>\n",
       "      <td>40</td>\n",
       "      <td>1060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>991288</th>\n",
       "      <td>u_20383163</td>\n",
       "      <td>s_4009954</td>\n",
       "      <td>2017-08-20 18:40</td>\n",
       "      <td>117.255902</td>\n",
       "      <td>36.899739</td>\n",
       "      <td>b_52025240|-87|false;b_40209888|-71|false;b_52048607|-83|false;b_20874926|-86|false;b_51048975|-80|false;b_49111853|-89|false;b_44415092|-84|false;b_44200910|-88|false;b_51726487|-69|false;b_40362685|-82|false</td>\n",
       "      <td>c_38</td>\n",
       "      <td>117.255892</td>\n",
       "      <td>36.899805</td>\n",
       "      <td>43</td>\n",
       "      <td>...</td>\n",
       "      <td>(10, [], [(b_52025240, -87), (b_40209888, -71), (b_52048607, -83), (b_20874926, -86), (b_51048975, -80), (b_49111853, -89), (b_44415092, -84), (b_44200910, -88), (b_51726487, -69), (b_40362685, -82)])</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>14306</td>\n",
       "      <td>232</td>\n",
       "      <td>40</td>\n",
       "      <td>1120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>992531</th>\n",
       "      <td>u_20742137</td>\n",
       "      <td>s_4009954</td>\n",
       "      <td>2017-08-24 14:30</td>\n",
       "      <td>117.255865</td>\n",
       "      <td>36.899727</td>\n",
       "      <td>b_50640451|-73|false;b_22472878|-76|false;b_26613990|-70|false;b_44434668|-72|false;b_51726487|-59|false;b_44415092|-82|false;b_51048975|-68|false;b_40209888|-66|false;b_38092938|-75|false;b_8262964|-79|false</td>\n",
       "      <td>c_38</td>\n",
       "      <td>117.255892</td>\n",
       "      <td>36.899805</td>\n",
       "      <td>43</td>\n",
       "      <td>...</td>\n",
       "      <td>(10, [], [(b_50640451, -73), (b_22472878, -76), (b_26613990, -70), (b_44434668, -72), (b_51726487, -59), (b_44415092, -82), (b_51048975, -68), (b_40209888, -66), (b_38092938, -75), (b_8262964, -79)])</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>14346</td>\n",
       "      <td>236</td>\n",
       "      <td>30</td>\n",
       "      <td>870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1009472</th>\n",
       "      <td>u_25538133</td>\n",
       "      <td>s_4009954</td>\n",
       "      <td>2017-08-22 19:50</td>\n",
       "      <td>117.255910</td>\n",
       "      <td>36.899720</td>\n",
       "      <td>b_26467980|-86|false;b_44415092|-80|false;b_40059953|-82|false;b_51726487|-64|false;b_44317488|-89|false;b_28660854|-87|false;b_40209888|-67|false;b_49111853|-82|false;b_40362685|-71|false;b_34972750|-62|false</td>\n",
       "      <td>c_38</td>\n",
       "      <td>117.255892</td>\n",
       "      <td>36.899805</td>\n",
       "      <td>43</td>\n",
       "      <td>...</td>\n",
       "      <td>(10, [], [(b_26467980, -86), (b_44415092, -80), (b_40059953, -82), (b_51726487, -64), (b_44317488, -89), (b_28660854, -87), (b_40209888, -67), (b_49111853, -82), (b_40362685, -71), (b_34972750, -62)])</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>14568</td>\n",
       "      <td>234</td>\n",
       "      <td>50</td>\n",
       "      <td>1190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1009474</th>\n",
       "      <td>u_25538133</td>\n",
       "      <td>s_4009954</td>\n",
       "      <td>2017-08-23 12:20</td>\n",
       "      <td>117.256001</td>\n",
       "      <td>36.899691</td>\n",
       "      <td>b_22472878|-73|false;b_40059952|-82|false;b_19884119|-84|false;b_44317488|-76|false;b_44415092|-80|false;b_40059953|-77|false;b_40209888|-61|false;b_20874926|-84|false;b_52048607|-76|false;b_26467949|-82|false</td>\n",
       "      <td>c_38</td>\n",
       "      <td>117.255892</td>\n",
       "      <td>36.899805</td>\n",
       "      <td>43</td>\n",
       "      <td>...</td>\n",
       "      <td>(10, [], [(b_22472878, -73), (b_40059952, -82), (b_19884119, -84), (b_44317488, -76), (b_44415092, -80), (b_40059953, -77), (b_40209888, -61), (b_20874926, -84), (b_52048607, -76), (b_26467949, -82)])</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>14570</td>\n",
       "      <td>235</td>\n",
       "      <td>20</td>\n",
       "      <td>740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1013690</th>\n",
       "      <td>u_26648199</td>\n",
       "      <td>s_4009954</td>\n",
       "      <td>2017-08-21 13:00</td>\n",
       "      <td>117.256045</td>\n",
       "      <td>36.899738</td>\n",
       "      <td>b_52048607|-76|false;b_44415092|-83|false;b_22472878|-83|false;b_55312507|-75|false;b_12695816|-80|false;b_40059952|-80|false;b_40059953|-86|false;b_30379445|-83|false;b_38366281|-86|false;b_51726487|-62|false</td>\n",
       "      <td>c_38</td>\n",
       "      <td>117.255892</td>\n",
       "      <td>36.899805</td>\n",
       "      <td>43</td>\n",
       "      <td>...</td>\n",
       "      <td>(10, [], [(b_52048607, -76), (b_44415092, -83), (b_22472878, -83), (b_55312507, -75), (b_12695816, -80), (b_40059952, -80), (b_40059953, -86), (b_30379445, -83), (b_38366281, -86), (b_51726487, -62)])</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>14640</td>\n",
       "      <td>233</td>\n",
       "      <td>0</td>\n",
       "      <td>780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1013782</th>\n",
       "      <td>u_26680888</td>\n",
       "      <td>s_4009954</td>\n",
       "      <td>2017-08-24 20:10</td>\n",
       "      <td>117.256033</td>\n",
       "      <td>36.899698</td>\n",
       "      <td>b_22472878|-79|false;b_40209888|-64|false;b_49278725|-84|false;b_52048604|-78|false;b_40059953|-87|false;b_51048975|-68|false;b_45161221|-73|false;b_44415092|-74|false;b_48793119|-81|false</td>\n",
       "      <td>c_38</td>\n",
       "      <td>117.255892</td>\n",
       "      <td>36.899805</td>\n",
       "      <td>43</td>\n",
       "      <td>...</td>\n",
       "      <td>(9, [], [(b_22472878, -79), (b_40209888, -64), (b_49278725, -84), (b_52048604, -78), (b_40059953, -87), (b_51048975, -68), (b_45161221, -73), (b_44415092, -74), (b_48793119, -81)])</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>14650</td>\n",
       "      <td>236</td>\n",
       "      <td>10</td>\n",
       "      <td>1210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1015808</th>\n",
       "      <td>u_27283044</td>\n",
       "      <td>s_4009954</td>\n",
       "      <td>2017-08-24 15:50</td>\n",
       "      <td>117.255916</td>\n",
       "      <td>36.899710</td>\n",
       "      <td>b_40209888|-67|false;b_38366281|-80|false;b_44434668|-82|false;b_52048607|-72|false;b_51726487|-61|false;b_20874926|-81|false;b_50640451|-78|false;b_40059952|-84|false;b_22472878|-76|false;b_44499866|-80|false</td>\n",
       "      <td>c_38</td>\n",
       "      <td>117.255892</td>\n",
       "      <td>36.899805</td>\n",
       "      <td>43</td>\n",
       "      <td>...</td>\n",
       "      <td>(10, [], [(b_40209888, -67), (b_38366281, -80), (b_44434668, -82), (b_52048607, -72), (b_51726487, -61), (b_20874926, -81), (b_50640451, -78), (b_40059952, -84), (b_22472878, -76), (b_44499866, -80)])</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>14698</td>\n",
       "      <td>236</td>\n",
       "      <td>50</td>\n",
       "      <td>950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1015821</th>\n",
       "      <td>u_27283044</td>\n",
       "      <td>s_4009954</td>\n",
       "      <td>2017-08-22 15:00</td>\n",
       "      <td>117.255957</td>\n",
       "      <td>36.899674</td>\n",
       "      <td>b_40362685|-75|false;b_52048607|-62|false;b_38366281|-80|false;b_40209888|-61|false;b_20874926|-75|false;b_51726487|-57|false;b_30379445|-78|false;b_28660854|-78|false;b_40059953|-78|false;b_44499866|-82|false</td>\n",
       "      <td>c_38</td>\n",
       "      <td>117.255892</td>\n",
       "      <td>36.899805</td>\n",
       "      <td>43</td>\n",
       "      <td>...</td>\n",
       "      <td>(10, [], [(b_40362685, -75), (b_52048607, -62), (b_38366281, -80), (b_40209888, -61), (b_20874926, -75), (b_51726487, -57), (b_30379445, -78), (b_28660854, -78), (b_40059953, -78), (b_44499866, -82)])</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>14711</td>\n",
       "      <td>234</td>\n",
       "      <td>0</td>\n",
       "      <td>900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1020622</th>\n",
       "      <td>u_28542247</td>\n",
       "      <td>s_4009954</td>\n",
       "      <td>2017-08-24 13:20</td>\n",
       "      <td>117.255838</td>\n",
       "      <td>36.899733</td>\n",
       "      <td>b_22472878|-83|false;b_40059953|-88|false;b_40362685|-61|false;b_51048975|-75|false;b_52048607|-64|false;b_51726487|-58|false;b_53667061|-88|false;b_44415092|-77|false;b_48793119|-80|false;b_40209888|-67|false</td>\n",
       "      <td>c_38</td>\n",
       "      <td>117.255892</td>\n",
       "      <td>36.899805</td>\n",
       "      <td>43</td>\n",
       "      <td>...</td>\n",
       "      <td>(10, [], [(b_22472878, -83), (b_40059953, -88), (b_40362685, -61), (b_51048975, -75), (b_52048607, -64), (b_51726487, -58), (b_53667061, -88), (b_44415092, -77), (b_48793119, -80), (b_40209888, -67)])</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>14787</td>\n",
       "      <td>236</td>\n",
       "      <td>20</td>\n",
       "      <td>800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1030037</th>\n",
       "      <td>u_31723548</td>\n",
       "      <td>s_4009954</td>\n",
       "      <td>2017-08-23 09:00</td>\n",
       "      <td>117.256018</td>\n",
       "      <td>36.899713</td>\n",
       "      <td>b_52048607|-73|false;b_55289296|-74|false;b_38092938|-90|false;b_40209888|-63|false;b_52048604|-78|false;b_52025241|-80|false;b_52025240|-77|false;b_40059953|-80|false;b_40059952|-78|false;b_55312507|-79|false</td>\n",
       "      <td>c_38</td>\n",
       "      <td>117.255892</td>\n",
       "      <td>36.899805</td>\n",
       "      <td>43</td>\n",
       "      <td>...</td>\n",
       "      <td>(10, [], [(b_52048607, -73), (b_55289296, -74), (b_38092938, -90), (b_40209888, -63), (b_52048604, -78), (b_52025241, -80), (b_52025240, -77), (b_40059953, -80), (b_40059952, -78), (b_55312507, -79)])</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>14923</td>\n",
       "      <td>235</td>\n",
       "      <td>0</td>\n",
       "      <td>540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1031767</th>\n",
       "      <td>u_32376834</td>\n",
       "      <td>s_4009954</td>\n",
       "      <td>2017-08-24 16:20</td>\n",
       "      <td>117.255981</td>\n",
       "      <td>36.899714</td>\n",
       "      <td>b_50640451|-80|false;b_38366281|-81|false;b_52048604|-86|false;b_40059953|-79|false;b_38362969|-83|false;b_49111854|-88|false;b_40209888|-63|false;b_51726487|-54|false;b_44415092|-77|false;b_52048607|-72|false</td>\n",
       "      <td>c_38</td>\n",
       "      <td>117.255892</td>\n",
       "      <td>36.899805</td>\n",
       "      <td>43</td>\n",
       "      <td>...</td>\n",
       "      <td>(10, [], [(b_50640451, -80), (b_38366281, -81), (b_52048604, -86), (b_40059953, -79), (b_38362969, -83), (b_49111854, -88), (b_40209888, -63), (b_51726487, -54), (b_44415092, -77), (b_52048607, -72)])</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>14952</td>\n",
       "      <td>236</td>\n",
       "      <td>20</td>\n",
       "      <td>980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1058050</th>\n",
       "      <td>u_40091449</td>\n",
       "      <td>s_4009954</td>\n",
       "      <td>2017-08-23 13:30</td>\n",
       "      <td>117.255999</td>\n",
       "      <td>36.899710</td>\n",
       "      <td>b_17079184|-79|false;b_22472878|-74|false;b_40209888|-61|false;b_44415092|-80|false;b_52048607|-73|false;b_40059953|-83|false;b_17079183|-83|false;b_40059952|-79|false;b_40362685|-72|false;b_38366281|-82|false</td>\n",
       "      <td>c_38</td>\n",
       "      <td>117.255892</td>\n",
       "      <td>36.899805</td>\n",
       "      <td>43</td>\n",
       "      <td>...</td>\n",
       "      <td>(10, [], [(b_17079184, -79), (b_22472878, -74), (b_40209888, -61), (b_44415092, -80), (b_52048607, -73), (b_40059953, -83), (b_17079183, -83), (b_40059952, -79), (b_40362685, -72), (b_38366281, -82)])</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>15324</td>\n",
       "      <td>235</td>\n",
       "      <td>30</td>\n",
       "      <td>810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1093068</th>\n",
       "      <td>u_50405034</td>\n",
       "      <td>s_4009954</td>\n",
       "      <td>2017-08-24 12:40</td>\n",
       "      <td>117.256006</td>\n",
       "      <td>36.899706</td>\n",
       "      <td>b_51726487|-58|false;b_33962055|-92|false;b_40209888|-66|false;b_22472878|-78|false;b_44415092|-76|false;b_49278725|-90|false;b_52025240|-74|false;b_51048975|-75|false;b_40362685|-72|false;b_48249102|-80|false</td>\n",
       "      <td>c_38</td>\n",
       "      <td>117.255892</td>\n",
       "      <td>36.899805</td>\n",
       "      <td>43</td>\n",
       "      <td>...</td>\n",
       "      <td>(10, [], [(b_51726487, -58), (b_33962055, -92), (b_40209888, -66), (b_22472878, -78), (b_44415092, -76), (b_49278725, -90), (b_52025240, -74), (b_51048975, -75), (b_40362685, -72), (b_48249102, -80)])</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>15837</td>\n",
       "      <td>236</td>\n",
       "      <td>40</td>\n",
       "      <td>760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1098199</th>\n",
       "      <td>u_52133123</td>\n",
       "      <td>s_4009954</td>\n",
       "      <td>2017-08-24 09:20</td>\n",
       "      <td>117.255867</td>\n",
       "      <td>36.899761</td>\n",
       "      <td>b_55312507|-81|false;b_44499866|-84|false;b_52025240|-74|false;b_51726487|-71|false;b_40209888|-69|false;b_50640451|-83|false;b_49278725|-85|false;b_44415092|-85|false;b_52048607|-65|false;b_20874926|-77|false</td>\n",
       "      <td>c_38</td>\n",
       "      <td>117.255892</td>\n",
       "      <td>36.899805</td>\n",
       "      <td>43</td>\n",
       "      <td>...</td>\n",
       "      <td>(10, [], [(b_55312507, -81), (b_44499866, -84), (b_52025240, -74), (b_51726487, -71), (b_40209888, -69), (b_50640451, -83), (b_49278725, -85), (b_44415092, -85), (b_52048607, -65), (b_20874926, -77)])</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>15879</td>\n",
       "      <td>236</td>\n",
       "      <td>20</td>\n",
       "      <td>560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1100396</th>\n",
       "      <td>u_52866059</td>\n",
       "      <td>s_4009954</td>\n",
       "      <td>2017-08-24 16:40</td>\n",
       "      <td>117.255925</td>\n",
       "      <td>36.899727</td>\n",
       "      <td>b_50640451|-76|false;b_55312507|-78|false;b_51726487|-54|false;b_22472878|-79|false;b_44322551|-82|false;b_48793119|-79|false;b_40362685|-73|false;b_40209888|-69|false;b_40059952|-84|false;b_52048607|-66|false</td>\n",
       "      <td>c_38</td>\n",
       "      <td>117.255892</td>\n",
       "      <td>36.899805</td>\n",
       "      <td>43</td>\n",
       "      <td>...</td>\n",
       "      <td>(10, [], [(b_50640451, -76), (b_55312507, -78), (b_51726487, -54), (b_22472878, -79), (b_44322551, -82), (b_48793119, -79), (b_40362685, -73), (b_40209888, -69), (b_40059952, -84), (b_52048607, -66)])</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>15904</td>\n",
       "      <td>236</td>\n",
       "      <td>40</td>\n",
       "      <td>1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1111138</th>\n",
       "      <td>u_55812548</td>\n",
       "      <td>s_4009954</td>\n",
       "      <td>2017-08-21 20:40</td>\n",
       "      <td>117.255920</td>\n",
       "      <td>36.899739</td>\n",
       "      <td>b_44322551|-80|false;b_51726487|-54|false;b_40209888|-70|false;b_40059953|-78|false;b_20874926|-78|false;b_14580929|-82|false;b_52048607|-71|false;b_44415092|-82|false;b_22472878|-73|false;b_40362685|-72|false</td>\n",
       "      <td>c_38</td>\n",
       "      <td>117.255892</td>\n",
       "      <td>36.899805</td>\n",
       "      <td>43</td>\n",
       "      <td>...</td>\n",
       "      <td>(10, [], [(b_44322551, -80), (b_51726487, -54), (b_40209888, -70), (b_40059953, -78), (b_20874926, -78), (b_14580929, -82), (b_52048607, -71), (b_44415092, -82), (b_22472878, -73), (b_40362685, -72)])</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>16121</td>\n",
       "      <td>233</td>\n",
       "      <td>40</td>\n",
       "      <td>1240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1112946</th>\n",
       "      <td>u_56334272</td>\n",
       "      <td>s_4009954</td>\n",
       "      <td>2017-08-24 12:10</td>\n",
       "      <td>117.255888</td>\n",
       "      <td>36.899708</td>\n",
       "      <td>b_40362685|-72|false;b_14580929|-74|false;b_22472878|-74|false;b_44317506|-67|false;b_44415092|-70|false;b_44499866|-74|false;b_20201918|-81|false;b_40209888|-58|false;b_51726487|-55|false;b_52048607|-61|false</td>\n",
       "      <td>c_38</td>\n",
       "      <td>117.255892</td>\n",
       "      <td>36.899805</td>\n",
       "      <td>43</td>\n",
       "      <td>...</td>\n",
       "      <td>(10, [], [(b_40362685, -72), (b_14580929, -74), (b_22472878, -74), (b_44317506, -67), (b_44415092, -70), (b_44499866, -74), (b_20201918, -81), (b_40209888, -58), (b_51726487, -55), (b_52048607, -61)])</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>16149</td>\n",
       "      <td>236</td>\n",
       "      <td>10</td>\n",
       "      <td>730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1119336</th>\n",
       "      <td>u_58219457</td>\n",
       "      <td>s_4009954</td>\n",
       "      <td>2017-08-20 17:20</td>\n",
       "      <td>117.255925</td>\n",
       "      <td>36.899777</td>\n",
       "      <td>b_40059953|-80|false;b_52048607|-74|false;b_44415092|-81|false;b_40362685|-74|false;b_51048975|-73|false;b_40209888|-66|false;b_51726487|-58|false;b_52025240|-75|false;b_22472878|-76|false;b_44200910|-81|false</td>\n",
       "      <td>c_38</td>\n",
       "      <td>117.255892</td>\n",
       "      <td>36.899805</td>\n",
       "      <td>43</td>\n",
       "      <td>...</td>\n",
       "      <td>(10, [], [(b_40059953, -80), (b_52048607, -74), (b_44415092, -81), (b_40362685, -74), (b_51048975, -73), (b_40209888, -66), (b_51726487, -58), (b_52025240, -75), (b_22472878, -76), (b_44200910, -81)])</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>16194</td>\n",
       "      <td>232</td>\n",
       "      <td>20</td>\n",
       "      <td>1040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1119337</th>\n",
       "      <td>u_58219457</td>\n",
       "      <td>s_4009954</td>\n",
       "      <td>2017-08-24 19:10</td>\n",
       "      <td>117.255952</td>\n",
       "      <td>36.899673</td>\n",
       "      <td>b_44322551|-80|false;b_51726487|-59|false;b_28660854|-77|false;b_20874926|-76|false;b_22472878|-70|false;b_40059953|-85|false;b_40362685|-72|false;b_55312507|-77|false;b_40209888|-68|false;b_50640451|-77|false</td>\n",
       "      <td>c_38</td>\n",
       "      <td>117.255892</td>\n",
       "      <td>36.899805</td>\n",
       "      <td>43</td>\n",
       "      <td>...</td>\n",
       "      <td>(10, [], [(b_44322551, -80), (b_51726487, -59), (b_28660854, -77), (b_20874926, -76), (b_22472878, -70), (b_40059953, -85), (b_40362685, -72), (b_55312507, -77), (b_40209888, -68), (b_50640451, -77)])</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>16195</td>\n",
       "      <td>236</td>\n",
       "      <td>10</td>\n",
       "      <td>1150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1119559</th>\n",
       "      <td>u_58401125</td>\n",
       "      <td>s_4009954</td>\n",
       "      <td>2017-08-24 20:50</td>\n",
       "      <td>117.255885</td>\n",
       "      <td>36.899714</td>\n",
       "      <td>b_51726487|-47|false;b_44415092|-79|false;b_50640451|-78|false;b_40362685|-77|false;b_40209888|-66|false;b_52025240|-79|false;b_52048607|-73|false;b_22472878|-80|false;b_49111853|-79|false;b_40059953|-88|false</td>\n",
       "      <td>c_38</td>\n",
       "      <td>117.255892</td>\n",
       "      <td>36.899805</td>\n",
       "      <td>43</td>\n",
       "      <td>...</td>\n",
       "      <td>(10, [], [(b_51726487, -47), (b_44415092, -79), (b_50640451, -78), (b_40362685, -77), (b_40209888, -66), (b_52025240, -79), (b_52048607, -73), (b_22472878, -80), (b_49111853, -79), (b_40059953, -88)])</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>16200</td>\n",
       "      <td>236</td>\n",
       "      <td>50</td>\n",
       "      <td>1250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1119905</th>\n",
       "      <td>u_58504303</td>\n",
       "      <td>s_4009954</td>\n",
       "      <td>2017-08-19 18:30</td>\n",
       "      <td>117.255943</td>\n",
       "      <td>36.899753</td>\n",
       "      <td>b_52048607|-83|false;b_40059952|-90|false;b_40362685|-81|false;b_40209888|-81|false;b_48249102|-93|false;b_51048975|-92|false;b_51726487|-71|false;b_22472878|-86|false;b_40059953|-79|false;b_20874926|-86|false</td>\n",
       "      <td>c_38</td>\n",
       "      <td>117.255892</td>\n",
       "      <td>36.899805</td>\n",
       "      <td>43</td>\n",
       "      <td>...</td>\n",
       "      <td>(10, [], [(b_52048607, -83), (b_40059952, -90), (b_40362685, -81), (b_40209888, -81), (b_48249102, -93), (b_51048975, -92), (b_51726487, -71), (b_22472878, -86), (b_40059953, -79), (b_20874926, -86)])</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>16206</td>\n",
       "      <td>231</td>\n",
       "      <td>30</td>\n",
       "      <td>1110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1131290</th>\n",
       "      <td>u_61734111</td>\n",
       "      <td>s_4009954</td>\n",
       "      <td>2017-08-20 18:10</td>\n",
       "      <td>117.255973</td>\n",
       "      <td>36.899728</td>\n",
       "      <td>b_51726487|-58|false;b_44415092|-85|false;b_49278725|-86|false;b_52048607|-78|false;b_32072203|-90|false;b_40209888|-68|false;b_40362685|-82|false;b_38366281|-84|false;b_54149190|-87|false;b_51048975|-78|false</td>\n",
       "      <td>c_38</td>\n",
       "      <td>117.255892</td>\n",
       "      <td>36.899805</td>\n",
       "      <td>43</td>\n",
       "      <td>...</td>\n",
       "      <td>(10, [], [(b_51726487, -58), (b_44415092, -85), (b_49278725, -86), (b_52048607, -78), (b_32072203, -90), (b_40209888, -68), (b_40362685, -82), (b_38366281, -84), (b_54149190, -87), (b_51048975, -78)])</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>16377</td>\n",
       "      <td>232</td>\n",
       "      <td>10</td>\n",
       "      <td>1090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1136315</th>\n",
       "      <td>u_63394759</td>\n",
       "      <td>s_4009954</td>\n",
       "      <td>2017-08-23 16:40</td>\n",
       "      <td>117.255851</td>\n",
       "      <td>36.899727</td>\n",
       "      <td>b_12695817|-91|false;b_40059952|-84|false;b_38366281|-79|false;b_51726487|-68|false;b_55312507|-74|false;b_44200910|-90|false;b_49148817|-88|false;b_40059953|-82|false;b_52025240|-79|false;b_44415092|-82|false</td>\n",
       "      <td>c_38</td>\n",
       "      <td>117.255892</td>\n",
       "      <td>36.899805</td>\n",
       "      <td>43</td>\n",
       "      <td>...</td>\n",
       "      <td>(10, [], [(b_12695817, -91), (b_40059952, -84), (b_38366281, -79), (b_51726487, -68), (b_55312507, -74), (b_44200910, -90), (b_49148817, -88), (b_40059953, -82), (b_52025240, -79), (b_44415092, -82)])</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>16460</td>\n",
       "      <td>235</td>\n",
       "      <td>40</td>\n",
       "      <td>1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1137692</th>\n",
       "      <td>u_63813847</td>\n",
       "      <td>s_4009954</td>\n",
       "      <td>2017-08-20 16:50</td>\n",
       "      <td>117.255988</td>\n",
       "      <td>36.899709</td>\n",
       "      <td>b_40059953|-87|false;b_44415092|-74|false;b_40362685|-79|false;b_40209888|-58|false;b_52048607|-75|false;b_44322551|-80|false;b_55312507|-81|false;b_52025240|-80|false;b_48249102|-87|false;b_51726487|-52|false</td>\n",
       "      <td>c_38</td>\n",
       "      <td>117.255892</td>\n",
       "      <td>36.899805</td>\n",
       "      <td>43</td>\n",
       "      <td>...</td>\n",
       "      <td>(10, [], [(b_40059953, -87), (b_44415092, -74), (b_40362685, -79), (b_40209888, -58), (b_52048607, -75), (b_44322551, -80), (b_55312507, -81), (b_52025240, -80), (b_48249102, -87), (b_51726487, -52)])</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>16478</td>\n",
       "      <td>232</td>\n",
       "      <td>50</td>\n",
       "      <td>1010</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>133 rows × 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            user_id    shop_id        time_stamp   longitude   latitude  \\\n",
       "3634       u_584348  s_4009954  2017-08-20 12:10  117.255877  36.899749   \n",
       "12035     u_2013372  s_4009954  2017-08-23 20:50  117.256029  36.899706   \n",
       "39001     u_8563349  s_4009954  2017-08-20 13:00  117.256001  36.899697   \n",
       "55657    u_12867996  s_4009954  2017-08-22 20:00  117.255868  36.899704   \n",
       "64179    u_15312645  s_4009954  2017-08-24 18:40  117.255924  36.899711   \n",
       "69768    u_17144306  s_4009954  2017-08-22 18:20  117.255994  36.899715   \n",
       "83995    u_20963816  s_4009954  2017-08-20 18:40  117.255865  36.899757   \n",
       "84239    u_21031202  s_4009954  2017-08-20 09:50  117.256053  36.899723   \n",
       "103309   u_26386978  s_4009954  2017-08-22 17:50  117.256019  36.899684   \n",
       "103398   u_26415883  s_4009954  2017-08-24 12:10  117.255861  36.899714   \n",
       "110388   u_28212891  s_4009954  2017-08-24 18:00  117.255951  36.899715   \n",
       "117931   u_30471722  s_4009954  2017-08-22 20:20  117.256009  36.899705   \n",
       "133574   u_35191941  s_4009954  2017-08-19 17:30  117.256008  36.899742   \n",
       "138148   u_36631548  s_4009954  2017-08-22 22:10  117.255997  36.899714   \n",
       "149329   u_40215514  s_4009954  2017-08-24 18:30  117.255945  36.899710   \n",
       "149331   u_40215514  s_4009954  2017-08-23 19:10  117.255963  36.899730   \n",
       "156834   u_42572916  s_4009954  2017-08-21 14:20  117.255992  36.899728   \n",
       "188551   u_52175426  s_4009954  2017-08-24 20:00  117.255914  36.899698   \n",
       "189523   u_52552188  s_4009954  2017-08-24 18:20  117.255852  36.899730   \n",
       "191582   u_53100482  s_4009954  2017-08-24 20:10  117.255827  36.899701   \n",
       "191793   u_53159700  s_4009954  2017-08-21 21:40  117.255956  36.899725   \n",
       "196151   u_54322990  s_4009954  2017-08-20 14:20  117.255897  36.899724   \n",
       "196710   u_54485550  s_4009954  2017-08-22 17:50  117.256012  36.899711   \n",
       "197301   u_54640711  s_4009954  2017-08-21 18:20  117.256013  36.899744   \n",
       "199140   u_55119625  s_4009954  2017-08-24 17:40  117.255962  36.899663   \n",
       "212969   u_59184843  s_4009954  2017-08-20 15:40  117.255882  36.899734   \n",
       "245974    u_3182669  s_4009954  2017-08-22 09:20  117.255856  36.899739   \n",
       "262181    u_7131012  s_4009954  2017-08-22 16:50  117.255864  36.899752   \n",
       "264519    u_7946115  s_4009954  2017-08-24 10:50  117.255888  36.899730   \n",
       "273378   u_10802645  s_4009954  2017-08-24 12:10  117.255933  36.899710   \n",
       "...             ...        ...               ...         ...        ...   \n",
       "898354   u_59878859  s_4009954  2017-08-20 13:20  117.255881  36.899761   \n",
       "942190    u_6317864  s_4009954  2017-08-19 18:00  117.255987  36.899761   \n",
       "953208    u_9946560  s_4009954  2017-08-21 22:00  117.255917  36.899752   \n",
       "967429   u_13402239  s_4009954  2017-08-20 16:10  117.255889  36.899757   \n",
       "972530   u_14914560  s_4009954  2017-08-24 11:30  117.255868  36.899711   \n",
       "986260   u_19071632  s_4009954  2017-08-24 17:40  117.255887  36.899723   \n",
       "991288   u_20383163  s_4009954  2017-08-20 18:40  117.255902  36.899739   \n",
       "992531   u_20742137  s_4009954  2017-08-24 14:30  117.255865  36.899727   \n",
       "1009472  u_25538133  s_4009954  2017-08-22 19:50  117.255910  36.899720   \n",
       "1009474  u_25538133  s_4009954  2017-08-23 12:20  117.256001  36.899691   \n",
       "1013690  u_26648199  s_4009954  2017-08-21 13:00  117.256045  36.899738   \n",
       "1013782  u_26680888  s_4009954  2017-08-24 20:10  117.256033  36.899698   \n",
       "1015808  u_27283044  s_4009954  2017-08-24 15:50  117.255916  36.899710   \n",
       "1015821  u_27283044  s_4009954  2017-08-22 15:00  117.255957  36.899674   \n",
       "1020622  u_28542247  s_4009954  2017-08-24 13:20  117.255838  36.899733   \n",
       "1030037  u_31723548  s_4009954  2017-08-23 09:00  117.256018  36.899713   \n",
       "1031767  u_32376834  s_4009954  2017-08-24 16:20  117.255981  36.899714   \n",
       "1058050  u_40091449  s_4009954  2017-08-23 13:30  117.255999  36.899710   \n",
       "1093068  u_50405034  s_4009954  2017-08-24 12:40  117.256006  36.899706   \n",
       "1098199  u_52133123  s_4009954  2017-08-24 09:20  117.255867  36.899761   \n",
       "1100396  u_52866059  s_4009954  2017-08-24 16:40  117.255925  36.899727   \n",
       "1111138  u_55812548  s_4009954  2017-08-21 20:40  117.255920  36.899739   \n",
       "1112946  u_56334272  s_4009954  2017-08-24 12:10  117.255888  36.899708   \n",
       "1119336  u_58219457  s_4009954  2017-08-20 17:20  117.255925  36.899777   \n",
       "1119337  u_58219457  s_4009954  2017-08-24 19:10  117.255952  36.899673   \n",
       "1119559  u_58401125  s_4009954  2017-08-24 20:50  117.255885  36.899714   \n",
       "1119905  u_58504303  s_4009954  2017-08-19 18:30  117.255943  36.899753   \n",
       "1131290  u_61734111  s_4009954  2017-08-20 18:10  117.255973  36.899728   \n",
       "1136315  u_63394759  s_4009954  2017-08-23 16:40  117.255851  36.899727   \n",
       "1137692  u_63813847  s_4009954  2017-08-20 16:50  117.255988  36.899709   \n",
       "\n",
       "                                                                                                                                                                                                                wifi_infos  \\\n",
       "3634     b_51726487|-56|false;b_44415092|-76|false;b_55312507|-80|false;b_28660854|-84|false;b_40362685|-77|false;b_52025240|-71|false;b_40209888|-64|false;b_22472878|-76|false;b_44317506|-82|false;b_52048607|-74|false   \n",
       "12035    b_55312507|-75|false;b_20874926|-73|false;b_44322551|-76|false;b_22472878|-79|false;b_40209888|-71|false;b_51726487|-68|false;b_40362685|-71|false;b_40059952|-79|false;b_50640451|-79|false;b_52048607|-71|false   \n",
       "39001      b_48793119|-76|false;b_51726487|-60|false;b_52025240|-82|false;b_19691207|-73|false;b_44415092|-72|false;b_40209888|-65|false;b_49278725|-77|false;b_22472878|-79|false;b_696895|-70|false;b_40362685|-78|false   \n",
       "55657    b_44322551|-76|false;b_40362685|-72|false;b_51726487|-53|false;b_50640451|-69|false;b_22472878|-78|false;b_40209888|-56|false;b_16118920|-69|false;b_52048607|-64|false;b_38366281|-84|false;b_44415092|-80|false   \n",
       "64179    b_40059953|-88|false;b_52048607|-65|false;b_22472878|-73|false;b_44317488|-81|false;b_44499866|-80|false;b_48793119|-79|false;b_40209888|-60|false;b_20874926|-69|false;b_15638004|-93|false;b_31517286|-69|false   \n",
       "69768    b_22472878|-65|false;b_40059953|-81|false;b_51048975|-66|false;b_49278725|-76|false;b_44415092|-73|false;b_40209888|-62|false;b_30379445|-77|false;b_44200910|-73|false;b_25339934|-69|false;b_40362685|-71|false   \n",
       "83995    b_40209888|-71|false;b_51048975|-80|false;b_52025240|-82|false;b_52048607|-79|false;b_51726487|-51|false;b_53667061|-90|false;b_40362685|-75|false;b_44415092|-81|false;b_30118357|-82|false;b_54134135|-84|false   \n",
       "84239    b_12705141|-76|false;b_38366281|-85|false;b_49148816|-78|false;b_12695816|-76|false;b_49148817|-86|false;b_38300990|-83|false;b_12695817|-86|false;b_49109253|-81|false;b_26464864|-79|false;b_44200910|-82|false   \n",
       "103309   b_40059953|-78|false;b_51726487|-71|false;b_52048607|-77|false;b_44415092|-86|false;b_26613990|-87|false;b_50274240|-78|false;b_22472878|-76|false;b_15638003|-81|false;b_48249102|-87|false;b_28733484|-79|false   \n",
       "103398   b_52025240|-63|false;b_22472878|-72|false;b_38362969|-82|false;b_40362685|-58|false;b_40059953|-75|false;b_26613990|-77|false;b_44434668|-69|false;b_51726487|-51|false;b_40209888|-57|false;b_44415092|-71|false   \n",
       "110388   b_49278725|-86|false;b_51726487|-53|false;b_51048975|-71|false;b_40209888|-75|false;b_28660854|-81|false;b_39574079|-85|false;b_34039495|-88|false;b_22472878|-83|false;b_52048607|-71|false;b_52048604|-77|false   \n",
       "117931   b_55289296|-83|false;b_22472878|-79|false;b_40059953|-84|false;b_52048607|-74|false;b_40362685|-80|false;b_44415092|-80|false;b_30379445|-82|false;b_40209888|-64|false;b_16118920|-80|false;b_51726487|-63|false   \n",
       "133574   b_52048607|-76|false;b_40362685|-80|false;b_51726487|-60|false;b_40059953|-79|false;b_51048975|-79|false;b_44415092|-80|false;b_44317488|-86|false;b_40209888|-75|false;b_55289296|-89|false;b_52025240|-87|false   \n",
       "138148   b_49111853|-89|false;b_40059953|-84|false;b_40209888|-80|false;b_38753526|-93|false;b_52025241|-94|false;b_52048607|-80|false;b_40362685|-89|false;b_26467980|-90|false;b_52025240|-91|false;b_50640451|-93|false   \n",
       "149329   b_51048975|-66|false;b_51726487|-55|false;b_30379445|-67|false;b_26613990|-69|false;b_49278725|-74|false;b_20874926|-66|false;b_52048604|-75|false;b_40209888|-57|false;b_48793119|-76|false;b_22472878|-75|false   \n",
       "149331   b_22472878|-67|false;b_52048607|-62|false;b_20874926|-69|false;b_40362685|-67|false;b_50640451|-64|false;b_44317506|-74|false;b_55312507|-69|false;b_51726487|-60|false;b_44520897|-69|false;b_40209888|-54|false   \n",
       "156834   b_48238061|-76|false;b_44322551|-80|false;b_26740399|-77|false;b_52048607|-70|false;b_55312507|-74|false;b_49111853|-80|false;b_40059953|-78|false;b_44434668|-75|false;b_50640451|-69|false;b_40362685|-73|false   \n",
       "188551   b_20201918|-83|false;b_26467949|-75|false;b_51726487|-56|false;b_40059953|-89|false;b_40362685|-71|false;b_52048607|-67|false;b_40209888|-59|false;b_20874926|-71|false;b_44415092|-83|false;b_22472878|-78|false   \n",
       "189523   b_22472878|-78|false;b_48793119|-80|false;b_44322551|-76|false;b_55312507|-77|false;b_52025240|-74|false;b_40362685|-63|false;b_51726487|-60|false;b_20874926|-76|false;b_52048607|-64|false;b_40209888|-63|false   \n",
       "191582   b_51726487|-54|false;b_40209888|-67|false;b_40059953|-87|false;b_20874926|-79|false;b_52048607|-69|false;b_44322551|-78|false;b_22472878|-74|false;b_40059952|-84|false;b_40362685|-68|false;b_19555539|-70|false   \n",
       "191793   b_40059952|-88|false;b_51726487|-64|false;b_40362685|-76|false;b_40059953|-84|false;b_53667045|-88|false;b_44499866|-80|false;b_40209888|-65|false;b_50640451|-81|false;b_44415092|-83|false;b_52048607|-74|false   \n",
       "196151   b_40209888|-60|false;b_51726487|-57|false;b_52048607|-71|false;b_40362685|-73|false;b_52025240|-77|false;b_49111853|-79|false;b_44322551|-82|false;b_22472878|-75|false;b_40059953|-90|false;b_26467949|-80|false   \n",
       "196710   b_51726487|-62|false;b_51048975|-77|false;b_55289296|-79|false;b_40209888|-72|false;b_33837518|-63|false;b_22472878|-75|false;b_40059953|-87|false;b_40362685|-81|false;b_52048607|-70|false;b_30379445|-80|false   \n",
       "197301   b_44415092|-79|false;b_52048607|-74|false;b_49278725|-77|false;b_40209888|-65|false;b_22472878|-71|false;b_26467980|-75|false;b_48793119|-77|false;b_44434668|-76|false;b_40059953|-81|false;b_51726487|-65|false   \n",
       "199140   b_52048604|-77|false;b_38092938|-90|false;b_40362685|-73|false;b_51726487|-69|false;b_40209888|-64|false;b_14580929|-80|false;b_40059953|-86|false;b_22472878|-84|false;b_12705141|-87|false;b_44197194|-83|false   \n",
       "212969   b_52025240|-69|false;b_12695817|-87|false;b_52048607|-72|false;b_52025241|-75|false;b_40209888|-60|false;b_40362685|-75|false;b_40059953|-80|false;b_51726487|-62|false;b_44415092|-80|false;b_22472878|-76|false   \n",
       "245974   b_51726487|-57|false;b_55312507|-75|false;b_52048604|-84|false;b_52048607|-60|false;b_40209888|-60|false;b_44499866|-78|false;b_44415092|-80|false;b_44434668|-80|false;b_55289296|-77|false;b_40362640|-89|false   \n",
       "262181   b_40209888|-64|false;b_52048604|-76|false;b_22472878|-76|false;b_48793119|-81|false;b_30379445|-85|false;b_26467980|-74|false;b_51048975|-73|false;b_40059952|-76|false;b_49278725|-79|false;b_44415092|-83|false   \n",
       "264519    b_40209888|-63|false;b_3862606|-73|false;b_40059953|-90|false;b_28660854|-80|false;b_38092938|-69|false;b_40362685|-64|false;b_44415092|-69|false;b_14580929|-63|false;b_52048607|-65|false;b_51726487|-56|false   \n",
       "273378   b_40362685|-63|false;b_22472878|-68|false;b_51048975|-70|false;b_55312507|-77|false;b_51696063|-83|false;b_44317506|-79|false;b_51726487|-54|false;b_40209888|-73|false;b_49278725|-78|false;b_20201918|-78|false   \n",
       "...                                                                                                                                                                                                                    ...   \n",
       "898354   b_52048607|-69|false;b_51726487|-56|false;b_30379445|-88|false;b_48793119|-81|false;b_49278725|-81|false;b_44415092|-79|false;b_24072852|-57|false;b_40209888|-60|false;b_22472878|-71|false;b_40059952|-79|false   \n",
       "942190   b_51726487|-57|false;b_53667051|-91|false;b_53667061|-89|false;b_40362685|-84|false;b_52048607|-77|false;b_52048604|-86|false;b_40209888|-63|false;b_40059953|-82|false;b_28660854|-89|false;b_22472878|-77|false   \n",
       "953208   b_44322551|-88|false;b_30379445|-88|false;b_55289296|-87|false;b_44415092|-80|false;b_40059953|-84|false;b_40209888|-60|false;b_40362685|-74|false;b_20201918|-87|false;b_38366281|-87|false;b_52048604|-86|false   \n",
       "967429   b_52048607|-73|false;b_48793119|-84|false;b_40362685|-79|false;b_55312507|-84|false;b_44415092|-79|false;b_51048975|-74|false;b_40059952|-83|false;b_40209888|-74|false;b_51726487|-58|false;b_52025240|-77|false   \n",
       "972530   b_40362685|-71|false;b_40209888|-68|false;b_52048607|-75|false;b_44434668|-75|false;b_51048975|-78|false;b_52048604|-84|false;b_44317506|-78|false;b_52025240|-77|false;b_44415092|-80|false;b_51726487|-63|false   \n",
       "986260   b_51726487|-49|false;b_27468209|-62|false;b_52048607|-70|false;b_38753526|-83|false;b_50640451|-71|false;b_52025240|-68|false;b_55312507|-72|false;b_40209888|-59|false;b_40362685|-82|false;b_51145806|-89|false   \n",
       "991288   b_52025240|-87|false;b_40209888|-71|false;b_52048607|-83|false;b_20874926|-86|false;b_51048975|-80|false;b_49111853|-89|false;b_44415092|-84|false;b_44200910|-88|false;b_51726487|-69|false;b_40362685|-82|false   \n",
       "992531    b_50640451|-73|false;b_22472878|-76|false;b_26613990|-70|false;b_44434668|-72|false;b_51726487|-59|false;b_44415092|-82|false;b_51048975|-68|false;b_40209888|-66|false;b_38092938|-75|false;b_8262964|-79|false   \n",
       "1009472  b_26467980|-86|false;b_44415092|-80|false;b_40059953|-82|false;b_51726487|-64|false;b_44317488|-89|false;b_28660854|-87|false;b_40209888|-67|false;b_49111853|-82|false;b_40362685|-71|false;b_34972750|-62|false   \n",
       "1009474  b_22472878|-73|false;b_40059952|-82|false;b_19884119|-84|false;b_44317488|-76|false;b_44415092|-80|false;b_40059953|-77|false;b_40209888|-61|false;b_20874926|-84|false;b_52048607|-76|false;b_26467949|-82|false   \n",
       "1013690  b_52048607|-76|false;b_44415092|-83|false;b_22472878|-83|false;b_55312507|-75|false;b_12695816|-80|false;b_40059952|-80|false;b_40059953|-86|false;b_30379445|-83|false;b_38366281|-86|false;b_51726487|-62|false   \n",
       "1013782                       b_22472878|-79|false;b_40209888|-64|false;b_49278725|-84|false;b_52048604|-78|false;b_40059953|-87|false;b_51048975|-68|false;b_45161221|-73|false;b_44415092|-74|false;b_48793119|-81|false   \n",
       "1015808  b_40209888|-67|false;b_38366281|-80|false;b_44434668|-82|false;b_52048607|-72|false;b_51726487|-61|false;b_20874926|-81|false;b_50640451|-78|false;b_40059952|-84|false;b_22472878|-76|false;b_44499866|-80|false   \n",
       "1015821  b_40362685|-75|false;b_52048607|-62|false;b_38366281|-80|false;b_40209888|-61|false;b_20874926|-75|false;b_51726487|-57|false;b_30379445|-78|false;b_28660854|-78|false;b_40059953|-78|false;b_44499866|-82|false   \n",
       "1020622  b_22472878|-83|false;b_40059953|-88|false;b_40362685|-61|false;b_51048975|-75|false;b_52048607|-64|false;b_51726487|-58|false;b_53667061|-88|false;b_44415092|-77|false;b_48793119|-80|false;b_40209888|-67|false   \n",
       "1030037  b_52048607|-73|false;b_55289296|-74|false;b_38092938|-90|false;b_40209888|-63|false;b_52048604|-78|false;b_52025241|-80|false;b_52025240|-77|false;b_40059953|-80|false;b_40059952|-78|false;b_55312507|-79|false   \n",
       "1031767  b_50640451|-80|false;b_38366281|-81|false;b_52048604|-86|false;b_40059953|-79|false;b_38362969|-83|false;b_49111854|-88|false;b_40209888|-63|false;b_51726487|-54|false;b_44415092|-77|false;b_52048607|-72|false   \n",
       "1058050  b_17079184|-79|false;b_22472878|-74|false;b_40209888|-61|false;b_44415092|-80|false;b_52048607|-73|false;b_40059953|-83|false;b_17079183|-83|false;b_40059952|-79|false;b_40362685|-72|false;b_38366281|-82|false   \n",
       "1093068  b_51726487|-58|false;b_33962055|-92|false;b_40209888|-66|false;b_22472878|-78|false;b_44415092|-76|false;b_49278725|-90|false;b_52025240|-74|false;b_51048975|-75|false;b_40362685|-72|false;b_48249102|-80|false   \n",
       "1098199  b_55312507|-81|false;b_44499866|-84|false;b_52025240|-74|false;b_51726487|-71|false;b_40209888|-69|false;b_50640451|-83|false;b_49278725|-85|false;b_44415092|-85|false;b_52048607|-65|false;b_20874926|-77|false   \n",
       "1100396  b_50640451|-76|false;b_55312507|-78|false;b_51726487|-54|false;b_22472878|-79|false;b_44322551|-82|false;b_48793119|-79|false;b_40362685|-73|false;b_40209888|-69|false;b_40059952|-84|false;b_52048607|-66|false   \n",
       "1111138  b_44322551|-80|false;b_51726487|-54|false;b_40209888|-70|false;b_40059953|-78|false;b_20874926|-78|false;b_14580929|-82|false;b_52048607|-71|false;b_44415092|-82|false;b_22472878|-73|false;b_40362685|-72|false   \n",
       "1112946  b_40362685|-72|false;b_14580929|-74|false;b_22472878|-74|false;b_44317506|-67|false;b_44415092|-70|false;b_44499866|-74|false;b_20201918|-81|false;b_40209888|-58|false;b_51726487|-55|false;b_52048607|-61|false   \n",
       "1119336  b_40059953|-80|false;b_52048607|-74|false;b_44415092|-81|false;b_40362685|-74|false;b_51048975|-73|false;b_40209888|-66|false;b_51726487|-58|false;b_52025240|-75|false;b_22472878|-76|false;b_44200910|-81|false   \n",
       "1119337  b_44322551|-80|false;b_51726487|-59|false;b_28660854|-77|false;b_20874926|-76|false;b_22472878|-70|false;b_40059953|-85|false;b_40362685|-72|false;b_55312507|-77|false;b_40209888|-68|false;b_50640451|-77|false   \n",
       "1119559  b_51726487|-47|false;b_44415092|-79|false;b_50640451|-78|false;b_40362685|-77|false;b_40209888|-66|false;b_52025240|-79|false;b_52048607|-73|false;b_22472878|-80|false;b_49111853|-79|false;b_40059953|-88|false   \n",
       "1119905  b_52048607|-83|false;b_40059952|-90|false;b_40362685|-81|false;b_40209888|-81|false;b_48249102|-93|false;b_51048975|-92|false;b_51726487|-71|false;b_22472878|-86|false;b_40059953|-79|false;b_20874926|-86|false   \n",
       "1131290  b_51726487|-58|false;b_44415092|-85|false;b_49278725|-86|false;b_52048607|-78|false;b_32072203|-90|false;b_40209888|-68|false;b_40362685|-82|false;b_38366281|-84|false;b_54149190|-87|false;b_51048975|-78|false   \n",
       "1136315  b_12695817|-91|false;b_40059952|-84|false;b_38366281|-79|false;b_51726487|-68|false;b_55312507|-74|false;b_44200910|-90|false;b_49148817|-88|false;b_40059953|-82|false;b_52025240|-79|false;b_44415092|-82|false   \n",
       "1137692  b_40059953|-87|false;b_44415092|-74|false;b_40362685|-79|false;b_40209888|-58|false;b_52048607|-75|false;b_44322551|-80|false;b_55312507|-81|false;b_52025240|-80|false;b_48249102|-87|false;b_51726487|-52|false   \n",
       "\n",
       "        category_id  shop_longitude  shop_latitude  price     ...       \\\n",
       "3634           c_38      117.255892      36.899805     43     ...        \n",
       "12035          c_38      117.255892      36.899805     43     ...        \n",
       "39001          c_38      117.255892      36.899805     43     ...        \n",
       "55657          c_38      117.255892      36.899805     43     ...        \n",
       "64179          c_38      117.255892      36.899805     43     ...        \n",
       "69768          c_38      117.255892      36.899805     43     ...        \n",
       "83995          c_38      117.255892      36.899805     43     ...        \n",
       "84239          c_38      117.255892      36.899805     43     ...        \n",
       "103309         c_38      117.255892      36.899805     43     ...        \n",
       "103398         c_38      117.255892      36.899805     43     ...        \n",
       "110388         c_38      117.255892      36.899805     43     ...        \n",
       "117931         c_38      117.255892      36.899805     43     ...        \n",
       "133574         c_38      117.255892      36.899805     43     ...        \n",
       "138148         c_38      117.255892      36.899805     43     ...        \n",
       "149329         c_38      117.255892      36.899805     43     ...        \n",
       "149331         c_38      117.255892      36.899805     43     ...        \n",
       "156834         c_38      117.255892      36.899805     43     ...        \n",
       "188551         c_38      117.255892      36.899805     43     ...        \n",
       "189523         c_38      117.255892      36.899805     43     ...        \n",
       "191582         c_38      117.255892      36.899805     43     ...        \n",
       "191793         c_38      117.255892      36.899805     43     ...        \n",
       "196151         c_38      117.255892      36.899805     43     ...        \n",
       "196710         c_38      117.255892      36.899805     43     ...        \n",
       "197301         c_38      117.255892      36.899805     43     ...        \n",
       "199140         c_38      117.255892      36.899805     43     ...        \n",
       "212969         c_38      117.255892      36.899805     43     ...        \n",
       "245974         c_38      117.255892      36.899805     43     ...        \n",
       "262181         c_38      117.255892      36.899805     43     ...        \n",
       "264519         c_38      117.255892      36.899805     43     ...        \n",
       "273378         c_38      117.255892      36.899805     43     ...        \n",
       "...             ...             ...            ...    ...     ...        \n",
       "898354         c_38      117.255892      36.899805     43     ...        \n",
       "942190         c_38      117.255892      36.899805     43     ...        \n",
       "953208         c_38      117.255892      36.899805     43     ...        \n",
       "967429         c_38      117.255892      36.899805     43     ...        \n",
       "972530         c_38      117.255892      36.899805     43     ...        \n",
       "986260         c_38      117.255892      36.899805     43     ...        \n",
       "991288         c_38      117.255892      36.899805     43     ...        \n",
       "992531         c_38      117.255892      36.899805     43     ...        \n",
       "1009472        c_38      117.255892      36.899805     43     ...        \n",
       "1009474        c_38      117.255892      36.899805     43     ...        \n",
       "1013690        c_38      117.255892      36.899805     43     ...        \n",
       "1013782        c_38      117.255892      36.899805     43     ...        \n",
       "1015808        c_38      117.255892      36.899805     43     ...        \n",
       "1015821        c_38      117.255892      36.899805     43     ...        \n",
       "1020622        c_38      117.255892      36.899805     43     ...        \n",
       "1030037        c_38      117.255892      36.899805     43     ...        \n",
       "1031767        c_38      117.255892      36.899805     43     ...        \n",
       "1058050        c_38      117.255892      36.899805     43     ...        \n",
       "1093068        c_38      117.255892      36.899805     43     ...        \n",
       "1098199        c_38      117.255892      36.899805     43     ...        \n",
       "1100396        c_38      117.255892      36.899805     43     ...        \n",
       "1111138        c_38      117.255892      36.899805     43     ...        \n",
       "1112946        c_38      117.255892      36.899805     43     ...        \n",
       "1119336        c_38      117.255892      36.899805     43     ...        \n",
       "1119337        c_38      117.255892      36.899805     43     ...        \n",
       "1119559        c_38      117.255892      36.899805     43     ...        \n",
       "1119905        c_38      117.255892      36.899805     43     ...        \n",
       "1131290        c_38      117.255892      36.899805     43     ...        \n",
       "1136315        c_38      117.255892      36.899805     43     ...        \n",
       "1137692        c_38      117.255892      36.899805     43     ...        \n",
       "\n",
       "                                                                                                                                                                                                  basic_wifi_info  \\\n",
       "3634     (10, [], [(b_51726487, -56), (b_44415092, -76), (b_55312507, -80), (b_28660854, -84), (b_40362685, -77), (b_52025240, -71), (b_40209888, -64), (b_22472878, -76), (b_44317506, -82), (b_52048607, -74)])   \n",
       "12035    (10, [], [(b_55312507, -75), (b_20874926, -73), (b_44322551, -76), (b_22472878, -79), (b_40209888, -71), (b_51726487, -68), (b_40362685, -71), (b_40059952, -79), (b_50640451, -79), (b_52048607, -71)])   \n",
       "39001      (10, [], [(b_48793119, -76), (b_51726487, -60), (b_52025240, -82), (b_19691207, -73), (b_44415092, -72), (b_40209888, -65), (b_49278725, -77), (b_22472878, -79), (b_696895, -70), (b_40362685, -78)])   \n",
       "55657    (10, [], [(b_44322551, -76), (b_40362685, -72), (b_51726487, -53), (b_50640451, -69), (b_22472878, -78), (b_40209888, -56), (b_16118920, -69), (b_52048607, -64), (b_38366281, -84), (b_44415092, -80)])   \n",
       "64179    (10, [], [(b_40059953, -88), (b_52048607, -65), (b_22472878, -73), (b_44317488, -81), (b_44499866, -80), (b_48793119, -79), (b_40209888, -60), (b_20874926, -69), (b_15638004, -93), (b_31517286, -69)])   \n",
       "69768    (10, [], [(b_22472878, -65), (b_40059953, -81), (b_51048975, -66), (b_49278725, -76), (b_44415092, -73), (b_40209888, -62), (b_30379445, -77), (b_44200910, -73), (b_25339934, -69), (b_40362685, -71)])   \n",
       "83995    (10, [], [(b_40209888, -71), (b_51048975, -80), (b_52025240, -82), (b_52048607, -79), (b_51726487, -51), (b_53667061, -90), (b_40362685, -75), (b_44415092, -81), (b_30118357, -82), (b_54134135, -84)])   \n",
       "84239    (10, [], [(b_12705141, -76), (b_38366281, -85), (b_49148816, -78), (b_12695816, -76), (b_49148817, -86), (b_38300990, -83), (b_12695817, -86), (b_49109253, -81), (b_26464864, -79), (b_44200910, -82)])   \n",
       "103309   (10, [], [(b_40059953, -78), (b_51726487, -71), (b_52048607, -77), (b_44415092, -86), (b_26613990, -87), (b_50274240, -78), (b_22472878, -76), (b_15638003, -81), (b_48249102, -87), (b_28733484, -79)])   \n",
       "103398   (10, [], [(b_52025240, -63), (b_22472878, -72), (b_38362969, -82), (b_40362685, -58), (b_40059953, -75), (b_26613990, -77), (b_44434668, -69), (b_51726487, -51), (b_40209888, -57), (b_44415092, -71)])   \n",
       "110388   (10, [], [(b_49278725, -86), (b_51726487, -53), (b_51048975, -71), (b_40209888, -75), (b_28660854, -81), (b_39574079, -85), (b_34039495, -88), (b_22472878, -83), (b_52048607, -71), (b_52048604, -77)])   \n",
       "117931   (10, [], [(b_55289296, -83), (b_22472878, -79), (b_40059953, -84), (b_52048607, -74), (b_40362685, -80), (b_44415092, -80), (b_30379445, -82), (b_40209888, -64), (b_16118920, -80), (b_51726487, -63)])   \n",
       "133574   (10, [], [(b_52048607, -76), (b_40362685, -80), (b_51726487, -60), (b_40059953, -79), (b_51048975, -79), (b_44415092, -80), (b_44317488, -86), (b_40209888, -75), (b_55289296, -89), (b_52025240, -87)])   \n",
       "138148   (10, [], [(b_49111853, -89), (b_40059953, -84), (b_40209888, -80), (b_38753526, -93), (b_52025241, -94), (b_52048607, -80), (b_40362685, -89), (b_26467980, -90), (b_52025240, -91), (b_50640451, -93)])   \n",
       "149329   (10, [], [(b_51048975, -66), (b_51726487, -55), (b_30379445, -67), (b_26613990, -69), (b_49278725, -74), (b_20874926, -66), (b_52048604, -75), (b_40209888, -57), (b_48793119, -76), (b_22472878, -75)])   \n",
       "149331   (10, [], [(b_22472878, -67), (b_52048607, -62), (b_20874926, -69), (b_40362685, -67), (b_50640451, -64), (b_44317506, -74), (b_55312507, -69), (b_51726487, -60), (b_44520897, -69), (b_40209888, -54)])   \n",
       "156834   (10, [], [(b_48238061, -76), (b_44322551, -80), (b_26740399, -77), (b_52048607, -70), (b_55312507, -74), (b_49111853, -80), (b_40059953, -78), (b_44434668, -75), (b_50640451, -69), (b_40362685, -73)])   \n",
       "188551   (10, [], [(b_20201918, -83), (b_26467949, -75), (b_51726487, -56), (b_40059953, -89), (b_40362685, -71), (b_52048607, -67), (b_40209888, -59), (b_20874926, -71), (b_44415092, -83), (b_22472878, -78)])   \n",
       "189523   (10, [], [(b_22472878, -78), (b_48793119, -80), (b_44322551, -76), (b_55312507, -77), (b_52025240, -74), (b_40362685, -63), (b_51726487, -60), (b_20874926, -76), (b_52048607, -64), (b_40209888, -63)])   \n",
       "191582   (10, [], [(b_51726487, -54), (b_40209888, -67), (b_40059953, -87), (b_20874926, -79), (b_52048607, -69), (b_44322551, -78), (b_22472878, -74), (b_40059952, -84), (b_40362685, -68), (b_19555539, -70)])   \n",
       "191793   (10, [], [(b_40059952, -88), (b_51726487, -64), (b_40362685, -76), (b_40059953, -84), (b_53667045, -88), (b_44499866, -80), (b_40209888, -65), (b_50640451, -81), (b_44415092, -83), (b_52048607, -74)])   \n",
       "196151   (10, [], [(b_40209888, -60), (b_51726487, -57), (b_52048607, -71), (b_40362685, -73), (b_52025240, -77), (b_49111853, -79), (b_44322551, -82), (b_22472878, -75), (b_40059953, -90), (b_26467949, -80)])   \n",
       "196710   (10, [], [(b_51726487, -62), (b_51048975, -77), (b_55289296, -79), (b_40209888, -72), (b_33837518, -63), (b_22472878, -75), (b_40059953, -87), (b_40362685, -81), (b_52048607, -70), (b_30379445, -80)])   \n",
       "197301   (10, [], [(b_44415092, -79), (b_52048607, -74), (b_49278725, -77), (b_40209888, -65), (b_22472878, -71), (b_26467980, -75), (b_48793119, -77), (b_44434668, -76), (b_40059953, -81), (b_51726487, -65)])   \n",
       "199140   (10, [], [(b_52048604, -77), (b_38092938, -90), (b_40362685, -73), (b_51726487, -69), (b_40209888, -64), (b_14580929, -80), (b_40059953, -86), (b_22472878, -84), (b_12705141, -87), (b_44197194, -83)])   \n",
       "212969   (10, [], [(b_52025240, -69), (b_12695817, -87), (b_52048607, -72), (b_52025241, -75), (b_40209888, -60), (b_40362685, -75), (b_40059953, -80), (b_51726487, -62), (b_44415092, -80), (b_22472878, -76)])   \n",
       "245974   (10, [], [(b_51726487, -57), (b_55312507, -75), (b_52048604, -84), (b_52048607, -60), (b_40209888, -60), (b_44499866, -78), (b_44415092, -80), (b_44434668, -80), (b_55289296, -77), (b_40362640, -89)])   \n",
       "262181   (10, [], [(b_40209888, -64), (b_52048604, -76), (b_22472878, -76), (b_48793119, -81), (b_30379445, -85), (b_26467980, -74), (b_51048975, -73), (b_40059952, -76), (b_49278725, -79), (b_44415092, -83)])   \n",
       "264519    (10, [], [(b_40209888, -63), (b_3862606, -73), (b_40059953, -90), (b_28660854, -80), (b_38092938, -69), (b_40362685, -64), (b_44415092, -69), (b_14580929, -63), (b_52048607, -65), (b_51726487, -56)])   \n",
       "273378   (10, [], [(b_40362685, -63), (b_22472878, -68), (b_51048975, -70), (b_55312507, -77), (b_51696063, -83), (b_44317506, -79), (b_51726487, -54), (b_40209888, -73), (b_49278725, -78), (b_20201918, -78)])   \n",
       "...                                                                                                                                                                                                           ...   \n",
       "898354   (10, [], [(b_52048607, -69), (b_51726487, -56), (b_30379445, -88), (b_48793119, -81), (b_49278725, -81), (b_44415092, -79), (b_24072852, -57), (b_40209888, -60), (b_22472878, -71), (b_40059952, -79)])   \n",
       "942190   (10, [], [(b_51726487, -57), (b_53667051, -91), (b_53667061, -89), (b_40362685, -84), (b_52048607, -77), (b_52048604, -86), (b_40209888, -63), (b_40059953, -82), (b_28660854, -89), (b_22472878, -77)])   \n",
       "953208   (10, [], [(b_44322551, -88), (b_30379445, -88), (b_55289296, -87), (b_44415092, -80), (b_40059953, -84), (b_40209888, -60), (b_40362685, -74), (b_20201918, -87), (b_38366281, -87), (b_52048604, -86)])   \n",
       "967429   (10, [], [(b_52048607, -73), (b_48793119, -84), (b_40362685, -79), (b_55312507, -84), (b_44415092, -79), (b_51048975, -74), (b_40059952, -83), (b_40209888, -74), (b_51726487, -58), (b_52025240, -77)])   \n",
       "972530   (10, [], [(b_40362685, -71), (b_40209888, -68), (b_52048607, -75), (b_44434668, -75), (b_51048975, -78), (b_52048604, -84), (b_44317506, -78), (b_52025240, -77), (b_44415092, -80), (b_51726487, -63)])   \n",
       "986260   (10, [], [(b_51726487, -49), (b_27468209, -62), (b_52048607, -70), (b_38753526, -83), (b_50640451, -71), (b_52025240, -68), (b_55312507, -72), (b_40209888, -59), (b_40362685, -82), (b_51145806, -89)])   \n",
       "991288   (10, [], [(b_52025240, -87), (b_40209888, -71), (b_52048607, -83), (b_20874926, -86), (b_51048975, -80), (b_49111853, -89), (b_44415092, -84), (b_44200910, -88), (b_51726487, -69), (b_40362685, -82)])   \n",
       "992531    (10, [], [(b_50640451, -73), (b_22472878, -76), (b_26613990, -70), (b_44434668, -72), (b_51726487, -59), (b_44415092, -82), (b_51048975, -68), (b_40209888, -66), (b_38092938, -75), (b_8262964, -79)])   \n",
       "1009472  (10, [], [(b_26467980, -86), (b_44415092, -80), (b_40059953, -82), (b_51726487, -64), (b_44317488, -89), (b_28660854, -87), (b_40209888, -67), (b_49111853, -82), (b_40362685, -71), (b_34972750, -62)])   \n",
       "1009474  (10, [], [(b_22472878, -73), (b_40059952, -82), (b_19884119, -84), (b_44317488, -76), (b_44415092, -80), (b_40059953, -77), (b_40209888, -61), (b_20874926, -84), (b_52048607, -76), (b_26467949, -82)])   \n",
       "1013690  (10, [], [(b_52048607, -76), (b_44415092, -83), (b_22472878, -83), (b_55312507, -75), (b_12695816, -80), (b_40059952, -80), (b_40059953, -86), (b_30379445, -83), (b_38366281, -86), (b_51726487, -62)])   \n",
       "1013782                      (9, [], [(b_22472878, -79), (b_40209888, -64), (b_49278725, -84), (b_52048604, -78), (b_40059953, -87), (b_51048975, -68), (b_45161221, -73), (b_44415092, -74), (b_48793119, -81)])   \n",
       "1015808  (10, [], [(b_40209888, -67), (b_38366281, -80), (b_44434668, -82), (b_52048607, -72), (b_51726487, -61), (b_20874926, -81), (b_50640451, -78), (b_40059952, -84), (b_22472878, -76), (b_44499866, -80)])   \n",
       "1015821  (10, [], [(b_40362685, -75), (b_52048607, -62), (b_38366281, -80), (b_40209888, -61), (b_20874926, -75), (b_51726487, -57), (b_30379445, -78), (b_28660854, -78), (b_40059953, -78), (b_44499866, -82)])   \n",
       "1020622  (10, [], [(b_22472878, -83), (b_40059953, -88), (b_40362685, -61), (b_51048975, -75), (b_52048607, -64), (b_51726487, -58), (b_53667061, -88), (b_44415092, -77), (b_48793119, -80), (b_40209888, -67)])   \n",
       "1030037  (10, [], [(b_52048607, -73), (b_55289296, -74), (b_38092938, -90), (b_40209888, -63), (b_52048604, -78), (b_52025241, -80), (b_52025240, -77), (b_40059953, -80), (b_40059952, -78), (b_55312507, -79)])   \n",
       "1031767  (10, [], [(b_50640451, -80), (b_38366281, -81), (b_52048604, -86), (b_40059953, -79), (b_38362969, -83), (b_49111854, -88), (b_40209888, -63), (b_51726487, -54), (b_44415092, -77), (b_52048607, -72)])   \n",
       "1058050  (10, [], [(b_17079184, -79), (b_22472878, -74), (b_40209888, -61), (b_44415092, -80), (b_52048607, -73), (b_40059953, -83), (b_17079183, -83), (b_40059952, -79), (b_40362685, -72), (b_38366281, -82)])   \n",
       "1093068  (10, [], [(b_51726487, -58), (b_33962055, -92), (b_40209888, -66), (b_22472878, -78), (b_44415092, -76), (b_49278725, -90), (b_52025240, -74), (b_51048975, -75), (b_40362685, -72), (b_48249102, -80)])   \n",
       "1098199  (10, [], [(b_55312507, -81), (b_44499866, -84), (b_52025240, -74), (b_51726487, -71), (b_40209888, -69), (b_50640451, -83), (b_49278725, -85), (b_44415092, -85), (b_52048607, -65), (b_20874926, -77)])   \n",
       "1100396  (10, [], [(b_50640451, -76), (b_55312507, -78), (b_51726487, -54), (b_22472878, -79), (b_44322551, -82), (b_48793119, -79), (b_40362685, -73), (b_40209888, -69), (b_40059952, -84), (b_52048607, -66)])   \n",
       "1111138  (10, [], [(b_44322551, -80), (b_51726487, -54), (b_40209888, -70), (b_40059953, -78), (b_20874926, -78), (b_14580929, -82), (b_52048607, -71), (b_44415092, -82), (b_22472878, -73), (b_40362685, -72)])   \n",
       "1112946  (10, [], [(b_40362685, -72), (b_14580929, -74), (b_22472878, -74), (b_44317506, -67), (b_44415092, -70), (b_44499866, -74), (b_20201918, -81), (b_40209888, -58), (b_51726487, -55), (b_52048607, -61)])   \n",
       "1119336  (10, [], [(b_40059953, -80), (b_52048607, -74), (b_44415092, -81), (b_40362685, -74), (b_51048975, -73), (b_40209888, -66), (b_51726487, -58), (b_52025240, -75), (b_22472878, -76), (b_44200910, -81)])   \n",
       "1119337  (10, [], [(b_44322551, -80), (b_51726487, -59), (b_28660854, -77), (b_20874926, -76), (b_22472878, -70), (b_40059953, -85), (b_40362685, -72), (b_55312507, -77), (b_40209888, -68), (b_50640451, -77)])   \n",
       "1119559  (10, [], [(b_51726487, -47), (b_44415092, -79), (b_50640451, -78), (b_40362685, -77), (b_40209888, -66), (b_52025240, -79), (b_52048607, -73), (b_22472878, -80), (b_49111853, -79), (b_40059953, -88)])   \n",
       "1119905  (10, [], [(b_52048607, -83), (b_40059952, -90), (b_40362685, -81), (b_40209888, -81), (b_48249102, -93), (b_51048975, -92), (b_51726487, -71), (b_22472878, -86), (b_40059953, -79), (b_20874926, -86)])   \n",
       "1131290  (10, [], [(b_51726487, -58), (b_44415092, -85), (b_49278725, -86), (b_52048607, -78), (b_32072203, -90), (b_40209888, -68), (b_40362685, -82), (b_38366281, -84), (b_54149190, -87), (b_51048975, -78)])   \n",
       "1136315  (10, [], [(b_12695817, -91), (b_40059952, -84), (b_38366281, -79), (b_51726487, -68), (b_55312507, -74), (b_44200910, -90), (b_49148817, -88), (b_40059953, -82), (b_52025240, -79), (b_44415092, -82)])   \n",
       "1137692  (10, [], [(b_40059953, -87), (b_44415092, -74), (b_40362685, -79), (b_40209888, -58), (b_52048607, -75), (b_44322551, -80), (b_55312507, -81), (b_52025240, -80), (b_48249102, -87), (b_51726487, -52)])   \n",
       "\n",
       "        wifi_size  use_wifi_size  no_use_wifi_size  use_wifi_freq  \\\n",
       "3634           10              0                10            0.0   \n",
       "12035          10              0                10            0.0   \n",
       "39001          10              0                10            0.0   \n",
       "55657          10              0                10            0.0   \n",
       "64179          10              0                10            0.0   \n",
       "69768          10              0                10            0.0   \n",
       "83995          10              0                10            0.0   \n",
       "84239          10              0                10            0.0   \n",
       "103309         10              0                10            0.0   \n",
       "103398         10              0                10            0.0   \n",
       "110388         10              0                10            0.0   \n",
       "117931         10              0                10            0.0   \n",
       "133574         10              0                10            0.0   \n",
       "138148         10              0                10            0.0   \n",
       "149329         10              0                10            0.0   \n",
       "149331         10              0                10            0.0   \n",
       "156834         10              0                10            0.0   \n",
       "188551         10              0                10            0.0   \n",
       "189523         10              0                10            0.0   \n",
       "191582         10              0                10            0.0   \n",
       "191793         10              0                10            0.0   \n",
       "196151         10              0                10            0.0   \n",
       "196710         10              0                10            0.0   \n",
       "197301         10              0                10            0.0   \n",
       "199140         10              0                10            0.0   \n",
       "212969         10              0                10            0.0   \n",
       "245974         10              0                10            0.0   \n",
       "262181         10              0                10            0.0   \n",
       "264519         10              0                10            0.0   \n",
       "273378         10              0                10            0.0   \n",
       "...           ...            ...               ...            ...   \n",
       "898354         10              0                10            0.0   \n",
       "942190         10              0                10            0.0   \n",
       "953208         10              0                10            0.0   \n",
       "967429         10              0                10            0.0   \n",
       "972530         10              0                10            0.0   \n",
       "986260         10              0                10            0.0   \n",
       "991288         10              0                10            0.0   \n",
       "992531         10              0                10            0.0   \n",
       "1009472        10              0                10            0.0   \n",
       "1009474        10              0                10            0.0   \n",
       "1013690        10              0                10            0.0   \n",
       "1013782         9              0                 9            0.0   \n",
       "1015808        10              0                10            0.0   \n",
       "1015821        10              0                10            0.0   \n",
       "1020622        10              0                10            0.0   \n",
       "1030037        10              0                10            0.0   \n",
       "1031767        10              0                10            0.0   \n",
       "1058050        10              0                10            0.0   \n",
       "1093068        10              0                10            0.0   \n",
       "1098199        10              0                10            0.0   \n",
       "1100396        10              0                10            0.0   \n",
       "1111138        10              0                10            0.0   \n",
       "1112946        10              0                10            0.0   \n",
       "1119336        10              0                10            0.0   \n",
       "1119337        10              0                10            0.0   \n",
       "1119559        10              0                10            0.0   \n",
       "1119905        10              0                10            0.0   \n",
       "1131290        10              0                10            0.0   \n",
       "1136315        10              0                10            0.0   \n",
       "1137692        10              0                10            0.0   \n",
       "\n",
       "        no_use_wifi_freq  i_loc  dayofyear  minute  hour_minute  \n",
       "3634                 1.0     45        232      10          730  \n",
       "12035                1.0    149        235      50         1250  \n",
       "39001                1.0    475        232       0          780  \n",
       "55657                1.0    833        234       0         1200  \n",
       "64179                1.0    960        236      40         1120  \n",
       "69768                1.0   1027        234      20         1100  \n",
       "83995                1.0   1231        232      40         1120  \n",
       "84239                1.0   1238        232      50          590  \n",
       "103309               1.0   1480        234      50         1070  \n",
       "103398               1.0   1491        236      10          730  \n",
       "110388               1.0   1606        236       0         1080  \n",
       "117931               1.0   1724        234      20         1220  \n",
       "133574               1.0   1966        231      30         1050  \n",
       "138148               1.0   2022        234      10         1330  \n",
       "149329               1.0   2203        236      30         1110  \n",
       "149331               1.0   2205        235      10         1150  \n",
       "156834               1.0   2357        233      20          860  \n",
       "188551               1.0   2806        236       0         1200  \n",
       "189523               1.0   2821        236      20         1100  \n",
       "191582               1.0   2861        236      10         1210  \n",
       "191793               1.0   2862        233      40         1300  \n",
       "196151               1.0   2922        232      20          860  \n",
       "196710               1.0   2932        234      50         1070  \n",
       "197301               1.0   2939        233      20         1100  \n",
       "199140               1.0   2987        236      40         1060  \n",
       "212969               1.0   3202        232      40          940  \n",
       "245974               1.0   3633        234      20          560  \n",
       "262181               1.0   3854        234      50         1010  \n",
       "264519               1.0   3882        236      50          650  \n",
       "273378               1.0   4004        236      10          730  \n",
       "...                  ...    ...        ...     ...          ...  \n",
       "898354               1.0  13079        232      20          800  \n",
       "942190               1.0  13634        231       0         1080  \n",
       "953208               1.0  13783        233       0         1320  \n",
       "967429               1.0  13977        232      10          970  \n",
       "972530               1.0  14049        236      30          690  \n",
       "986260               1.0  14226        236      40         1060  \n",
       "991288               1.0  14306        232      40         1120  \n",
       "992531               1.0  14346        236      30          870  \n",
       "1009472              1.0  14568        234      50         1190  \n",
       "1009474              1.0  14570        235      20          740  \n",
       "1013690              1.0  14640        233       0          780  \n",
       "1013782              1.0  14650        236      10         1210  \n",
       "1015808              1.0  14698        236      50          950  \n",
       "1015821              1.0  14711        234       0          900  \n",
       "1020622              1.0  14787        236      20          800  \n",
       "1030037              1.0  14923        235       0          540  \n",
       "1031767              1.0  14952        236      20          980  \n",
       "1058050              1.0  15324        235      30          810  \n",
       "1093068              1.0  15837        236      40          760  \n",
       "1098199              1.0  15879        236      20          560  \n",
       "1100396              1.0  15904        236      40         1000  \n",
       "1111138              1.0  16121        233      40         1240  \n",
       "1112946              1.0  16149        236      10          730  \n",
       "1119336              1.0  16194        232      20         1040  \n",
       "1119337              1.0  16195        236      10         1150  \n",
       "1119559              1.0  16200        236      50         1250  \n",
       "1119905              1.0  16206        231      30         1110  \n",
       "1131290              1.0  16377        232      10         1090  \n",
       "1136315              1.0  16460        235      40         1000  \n",
       "1137692              1.0  16478        232      50         1010  \n",
       "\n",
       "[133 rows x 25 columns]"
      ]
     },
     "execution_count": 332,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s2_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('b_40209888', 163),\n",
       " ('b_51726487', 139),\n",
       " ('b_22472878', 117),\n",
       " ('b_44415092', 108),\n",
       " ('b_52048604', 95),\n",
       " ('b_14580929', 66),\n",
       " ('b_52048607', 61),\n",
       " ('b_20874926', 59),\n",
       " ('b_39574079', 59),\n",
       " ('b_40059953', 59),\n",
       " ('b_26467980', 56),\n",
       " ('b_30379445', 48),\n",
       " ('b_48793119', 48),\n",
       " ('b_40362685', 44),\n",
       " ('b_40059952', 44),\n",
       " ('b_51048975', 41),\n",
       " ('b_55289296', 40),\n",
       " ('b_49278725', 34),\n",
       " ('b_15638003', 32),\n",
       " ('b_44200910', 25),\n",
       " ('b_38366281', 24),\n",
       " ('b_13292390', 23),\n",
       " ('b_53667045', 23),\n",
       " ('b_28660854', 19),\n",
       " ('b_22962825', 19),\n",
       " ('b_12705141', 14),\n",
       " ('b_17079182', 12),\n",
       " ('b_53667047', 11),\n",
       " ('b_49148817', 10),\n",
       " ('b_44197194', 10),\n",
       " ('b_48249102', 9),\n",
       " ('b_44322551', 9),\n",
       " ('b_19884119', 7),\n",
       " ('b_30559295', 7),\n",
       " ('b_17079183', 6),\n",
       " ('b_26467949', 6),\n",
       " ('b_51696066', 5),\n",
       " ('b_34039495', 5),\n",
       " ('b_44434668', 5),\n",
       " ('b_13292391', 5),\n",
       " ('b_55016265', 5),\n",
       " ('b_55312507', 4),\n",
       " ('b_12695816', 4),\n",
       " ('b_17079184', 4),\n",
       " ('b_52025240', 4),\n",
       " ('b_12695817', 3),\n",
       " ('b_44317506', 3),\n",
       " ('b_10341739', 3),\n",
       " ('b_24668187', 2),\n",
       " ('b_2775240', 2),\n",
       " ('b_15638004', 2),\n",
       " ('b_31112294', 2),\n",
       " ('b_26467981', 2),\n",
       " ('b_53667051', 2),\n",
       " ('b_31179915', 2),\n",
       " ('b_35942302', 2),\n",
       " ('b_53667061', 2),\n",
       " ('b_3435358', 2),\n",
       " ('b_9025795', 2),\n",
       " ('b_23080718', 2),\n",
       " ('b_3736362', 2),\n",
       " ('b_12054771', 2),\n",
       " ('b_50640451', 2),\n",
       " ('b_44317488', 2),\n",
       " ('b_16678040', 2),\n",
       " ('b_24047025', 2),\n",
       " ('b_33540403', 2),\n",
       " ('b_38960167', 2),\n",
       " ('b_49148816', 2),\n",
       " ('b_34857982', 2),\n",
       " ('b_23895926', 2),\n",
       " ('b_23992896', 1),\n",
       " ('b_26466541', 1),\n",
       " ('b_51696063', 1),\n",
       " ('b_40570751', 1),\n",
       " ('b_54134135', 1),\n",
       " ('b_49911242', 1),\n",
       " ('b_15622062', 1),\n",
       " ('b_7364846', 1),\n",
       " ('b_48928225', 1),\n",
       " ('b_24007687', 1),\n",
       " ('b_21970625', 1),\n",
       " ('b_40448659', 1),\n",
       " ('b_21186692', 1),\n",
       " ('b_4829576', 1),\n",
       " ('b_33273993', 1),\n",
       " ('b_35070371', 1),\n",
       " ('b_42265626', 1),\n",
       " ('b_3183397', 1),\n",
       " ('b_44922785', 1),\n",
       " ('b_30346945', 1),\n",
       " ('b_32072203', 1),\n",
       " ('b_38753526', 1),\n",
       " ('b_10133085', 1),\n",
       " ('b_22990325', 1),\n",
       " ('b_52025241', 1),\n",
       " ('b_31055260', 1),\n",
       " ('b_54261628', 1),\n",
       " ('b_32813393', 1),\n",
       " ('b_8262965', 1),\n",
       " ('b_41705925', 1),\n",
       " ('b_13222120', 1),\n",
       " ('b_47225278', 1),\n",
       " ('b_3379241', 1),\n",
       " ('b_49278922', 1),\n",
       " ('b_36948388', 1),\n",
       " ('b_3284086', 1),\n",
       " ('b_38300990', 1),\n",
       " ('b_19153094', 1),\n",
       " ('b_33406552', 1),\n",
       " ('b_26145380', 1),\n",
       " ('b_24701122', 1),\n",
       " ('b_24128118', 1),\n",
       " ('b_46036898', 1),\n",
       " ('b_49153490', 1),\n",
       " ('b_37937122', 1),\n",
       " ('b_2959829', 1),\n",
       " ('b_13541112', 1),\n",
       " ('b_20184916', 1),\n",
       " ('b_12121854', 1),\n",
       " ('b_13667719', 1),\n",
       " ('b_17271990', 1),\n",
       " ('b_29967291', 1),\n",
       " ('b_3492645', 1),\n",
       " ('b_50187089', 1),\n",
       " ('b_9748693', 1),\n",
       " ('b_3074325', 1),\n",
       " ('b_33673627', 1),\n",
       " ('b_51145806', 1),\n",
       " ('b_50721297', 1),\n",
       " ('b_3521282', 1),\n",
       " ('b_27756859', 1),\n",
       " ('b_25931619', 1),\n",
       " ('b_8562607', 1),\n",
       " ('b_44520897', 1),\n",
       " ('b_15790994', 1),\n",
       " ('b_44408384', 1),\n",
       " ('b_20261524', 1),\n",
       " ('b_27444521', 1),\n",
       " ('b_52048605', 1),\n",
       " ('b_53520069', 1),\n",
       " ('b_19770055', 1),\n",
       " ('b_34814192', 1),\n",
       " ('b_44520233', 1),\n",
       " ('b_53008771', 1),\n",
       " ('b_57238370', 1),\n",
       " ('b_22497927', 1),\n",
       " ('b_25968676', 1),\n",
       " ('b_14148793', 1),\n",
       " ('b_19150109', 1),\n",
       " ('b_55016266', 1)]"
      ]
     },
     "execution_count": 358,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_sorted_wifi([s1_valid])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('b_40209888', 273),\n",
       " ('b_51726487', 268),\n",
       " ('b_52048607', 242),\n",
       " ('b_40362685', 218),\n",
       " ('b_20874926', 189),\n",
       " ('b_22472878', 155),\n",
       " ('b_44415092', 154),\n",
       " ('b_51048975', 101),\n",
       " ('b_14580929', 74),\n",
       " ('b_44322551', 59),\n",
       " ('b_40059953', 55),\n",
       " ('b_22962825', 55),\n",
       " ('b_26467980', 50),\n",
       " ('b_28660854', 50),\n",
       " ('b_55312507', 44),\n",
       " ('b_50640451', 44),\n",
       " ('b_53667045', 44),\n",
       " ('b_44317506', 43),\n",
       " ('b_52025240', 36),\n",
       " ('b_38366281', 34),\n",
       " ('b_53667047', 34),\n",
       " ('b_39574079', 33),\n",
       " ('b_40059952', 32),\n",
       " ('b_55289296', 31),\n",
       " ('b_44434668', 29),\n",
       " ('b_30379445', 26),\n",
       " ('b_52048604', 26),\n",
       " ('b_44499866', 25),\n",
       " ('b_26467949', 25),\n",
       " ('b_12054771', 23),\n",
       " ('b_13292390', 23),\n",
       " ('b_44520897', 21),\n",
       " ('b_48793119', 19),\n",
       " ('b_38753526', 18),\n",
       " ('b_49148817', 18),\n",
       " ('b_38092938', 17),\n",
       " ('b_40570751', 15),\n",
       " ('b_49278725', 15),\n",
       " ('b_52025241', 12),\n",
       " ('b_53667061', 12),\n",
       " ('b_44200910', 12),\n",
       " ('b_48249102', 10),\n",
       " ('b_49111853', 10),\n",
       " ('b_49148816', 10),\n",
       " ('b_15638003', 9),\n",
       " ('b_37937122', 9),\n",
       " ('b_17079184', 8),\n",
       " ('b_13292391', 8),\n",
       " ('b_54149190', 7),\n",
       " ('b_24701139', 7),\n",
       " ('b_54134135', 6),\n",
       " ('b_38300990', 6),\n",
       " ('b_17079183', 6),\n",
       " ('b_26613990', 5),\n",
       " ('b_12705141', 5),\n",
       " ('b_44317488', 5),\n",
       " ('b_52048599', 5),\n",
       " ('b_33540403', 5),\n",
       " ('b_49111854', 4),\n",
       " ('b_19222639', 4),\n",
       " ('b_34039495', 4),\n",
       " ('b_24701122', 4),\n",
       " ('b_18641816', 3),\n",
       " ('b_3971163', 3),\n",
       " ('b_53667051', 3),\n",
       " ('b_45935700', 3),\n",
       " ('b_19770055', 3),\n",
       " ('b_53117086', 3),\n",
       " ('b_51696066', 3),\n",
       " ('b_20201918', 3),\n",
       " ('b_8816160', 3),\n",
       " ('b_47930210', 3),\n",
       " ('b_12695816', 3),\n",
       " ('b_51145806', 3),\n",
       " ('b_36014818', 3),\n",
       " ('b_44197194', 3),\n",
       " ('b_27444521', 3),\n",
       " ('b_4300392', 3),\n",
       " ('b_12455408', 3),\n",
       " ('b_24668187', 2),\n",
       " ('b_2816034', 2),\n",
       " ('b_19884119', 2),\n",
       " ('b_30118357', 2),\n",
       " ('b_2329270', 2),\n",
       " ('b_11160740', 2),\n",
       " ('b_43851042', 2),\n",
       " ('b_24007687', 2),\n",
       " ('b_35012948', 2),\n",
       " ('b_43207008', 2),\n",
       " ('b_3183397', 2),\n",
       " ('b_24256770', 2),\n",
       " ('b_12695817', 2),\n",
       " ('b_33963836', 2),\n",
       " ('b_22990325', 2),\n",
       " ('b_54743298', 2),\n",
       " ('b_26019453', 2),\n",
       " ('b_44579536', 2),\n",
       " ('b_8262964', 2),\n",
       " ('b_30559295', 2),\n",
       " ('b_45431388', 2),\n",
       " ('b_44933868', 2),\n",
       " ('b_9747565', 2),\n",
       " ('b_50721297', 2),\n",
       " ('b_23895926', 2),\n",
       " ('b_53685122', 2),\n",
       " ('b_40362640', 2),\n",
       " ('b_38005842', 2),\n",
       " ('b_53685183', 2),\n",
       " ('b_17271990', 2),\n",
       " ('b_38212564', 2),\n",
       " ('b_53685173', 2),\n",
       " ('b_53685178', 2),\n",
       " ('b_48877097', 2),\n",
       " ('b_54242386', 2),\n",
       " ('b_20244187', 2),\n",
       " ('b_23992896', 2),\n",
       " ('b_10097644', 2),\n",
       " ('b_53685184', 2),\n",
       " ('b_44520893', 2),\n",
       " ('b_53685174', 2),\n",
       " ('b_53520069', 2),\n",
       " ('b_22497927', 2),\n",
       " ('b_25968676', 2),\n",
       " ('b_53667121', 2),\n",
       " ('b_53667044', 2),\n",
       " ('b_54703161', 1),\n",
       " ('b_33962055', 1),\n",
       " ('b_31112294', 1),\n",
       " ('b_16678040', 1),\n",
       " ('b_24142710', 1),\n",
       " ('b_22754003', 1),\n",
       " ('b_47046843', 1),\n",
       " ('b_36094802', 1),\n",
       " ('b_48793275', 1),\n",
       " ('b_35892680', 1),\n",
       " ('b_46616349', 1),\n",
       " ('b_36092183', 1),\n",
       " ('b_4512854', 1),\n",
       " ('b_22377128', 1),\n",
       " ('b_19161766', 1),\n",
       " ('b_19581200', 1),\n",
       " ('b_4829576', 1),\n",
       " ('b_3461714', 1),\n",
       " ('b_54258526', 1),\n",
       " ('b_56553431', 1),\n",
       " ('b_53683081', 1),\n",
       " ('b_55016265', 1),\n",
       " ('b_46039708', 1),\n",
       " ('b_47072332', 1),\n",
       " ('b_37896874', 1),\n",
       " ('b_44922785', 1),\n",
       " ('b_39471916', 1),\n",
       " ('b_8335824', 1),\n",
       " ('b_3730076', 1),\n",
       " ('b_26312646', 1),\n",
       " ('b_53683014', 1),\n",
       " ('b_44690582', 1),\n",
       " ('b_25988144', 1),\n",
       " ('b_24951683', 1),\n",
       " ('b_49153489', 1),\n",
       " ('b_55869370', 1),\n",
       " ('b_27470438', 1),\n",
       " ('b_16450886', 1),\n",
       " ('b_2420853', 1),\n",
       " ('b_28659514', 1),\n",
       " ('b_49109253', 1),\n",
       " ('b_14753472', 1),\n",
       " ('b_53667050', 1),\n",
       " ('b_40345721', 1),\n",
       " ('b_26613991', 1),\n",
       " ('b_11914474', 1),\n",
       " ('b_49278922', 1),\n",
       " ('b_23767014', 1),\n",
       " ('b_42420419', 1),\n",
       " ('b_40671669', 1),\n",
       " ('b_37474049', 1),\n",
       " ('b_3053007', 1),\n",
       " ('b_53683449', 1),\n",
       " ('b_57436030', 1),\n",
       " ('b_17370240', 1),\n",
       " ('b_26145380', 1),\n",
       " ('b_11291311', 1),\n",
       " ('b_11387619', 1),\n",
       " ('b_39628717', 1),\n",
       " ('b_7659578', 1),\n",
       " ('b_38362968', 1),\n",
       " ('b_48238061', 1),\n",
       " ('b_2959829', 1),\n",
       " ('b_13541112', 1),\n",
       " ('b_53683451', 1),\n",
       " ('b_47225278', 1),\n",
       " ('b_28733484', 1),\n",
       " ('b_12121854', 1),\n",
       " ('b_35307237', 1),\n",
       " ('b_54237030', 1),\n",
       " ('b_8210864', 1),\n",
       " ('b_49093679', 1),\n",
       " ('b_26467504', 1),\n",
       " ('b_3492645', 1),\n",
       " ('b_50187089', 1),\n",
       " ('b_5839026', 1),\n",
       " ('b_44410358', 1),\n",
       " ('b_2781585', 1),\n",
       " ('b_21120267', 1),\n",
       " ('b_48932556', 1),\n",
       " ('b_3521282', 1),\n",
       " ('b_10341739', 1),\n",
       " ('b_27756859', 1),\n",
       " ('b_25931619', 1),\n",
       " ('b_8562607', 1),\n",
       " ('b_34814192', 1),\n",
       " ('b_42565616', 1),\n",
       " ('b_44317423', 1),\n",
       " ('b_33457813', 1),\n",
       " ('b_33846633', 1),\n",
       " ('b_43639732', 1),\n",
       " ('b_3881970', 1),\n",
       " ('b_42807897', 1),\n",
       " ('b_38362969', 1),\n",
       " ('b_19002286', 1),\n",
       " ('b_53683013', 1),\n",
       " ('b_3902209', 1),\n",
       " ('b_10455641', 1),\n",
       " ('b_10370524', 1),\n",
       " ('b_50263204', 1),\n",
       " ('b_21186692', 1),\n",
       " ('b_39962021', 1),\n",
       " ('b_53667040', 1),\n",
       " ('b_53667041', 1),\n",
       " ('b_53667046', 1)]"
      ]
     },
     "execution_count": 359,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_sorted_wifi([s2_valid])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('b_40209888', 386),\n",
       " ('b_51726487', 341),\n",
       " ('b_22472878', 247),\n",
       " ('b_52048604', 219),\n",
       " ('b_44415092', 195),\n",
       " ('b_52048607', 173),\n",
       " ('b_26467980', 152),\n",
       " ('b_49278725', 145),\n",
       " ('b_30379445', 143),\n",
       " ('b_40362685', 136),\n",
       " ('b_55289296', 135),\n",
       " ('b_48793119', 131),\n",
       " ('b_12705141', 111),\n",
       " ('b_40059953', 101),\n",
       " ('b_39574079', 97),\n",
       " ('b_40059952', 96),\n",
       " ('b_51048975', 94),\n",
       " ('b_28660854', 84),\n",
       " ('b_51271474', 63),\n",
       " ('b_53667045', 61),\n",
       " ('b_44200910', 58),\n",
       " ('b_26467949', 41),\n",
       " ('b_15638003', 33),\n",
       " ('b_17079183', 32),\n",
       " ('b_31112294', 30),\n",
       " ('b_52025240', 30),\n",
       " ('b_17079182', 29),\n",
       " ('b_17079184', 28),\n",
       " ('b_55016265', 25),\n",
       " ('b_34039495', 23),\n",
       " ('b_20874926', 23),\n",
       " ('b_49148817', 23),\n",
       " ('b_44434668', 22),\n",
       " ('b_38366281', 22),\n",
       " ('b_53667047', 22),\n",
       " ('b_48249102', 21),\n",
       " ('b_14580929', 20),\n",
       " ('b_15638004', 15),\n",
       " ('b_30559295', 15),\n",
       " ('b_53667051', 13),\n",
       " ('b_44322551', 11),\n",
       " ('b_55312507', 11),\n",
       " ('b_12695816', 9),\n",
       " ('b_38300990', 9),\n",
       " ('b_49148816', 9),\n",
       " ('b_50640451', 8),\n",
       " ('b_14753472', 7),\n",
       " ('b_19884119', 7),\n",
       " ('b_44317506', 7),\n",
       " ('b_51145806', 7),\n",
       " ('b_44197194', 7),\n",
       " ('b_53497184', 7),\n",
       " ('b_26613990', 6),\n",
       " ('b_26467981', 6),\n",
       " ('b_20201918', 6),\n",
       " ('b_55606879', 6),\n",
       " ('b_52025241', 5),\n",
       " ('b_54149190', 5),\n",
       " ('b_53667061', 5),\n",
       " ('b_12695817', 5),\n",
       " ('b_44317488', 5),\n",
       " ('b_42415389', 5),\n",
       " ('b_18721144', 4),\n",
       " ('b_9025795', 4),\n",
       " ('b_30118357', 3),\n",
       " ('b_29648562', 3),\n",
       " ('b_44499866', 3),\n",
       " ('b_51696066', 3),\n",
       " ('b_49153489', 3),\n",
       " ('b_4509235', 3),\n",
       " ('b_8262964', 3),\n",
       " ('b_23895926', 3),\n",
       " ('b_43623124', 3),\n",
       " ('b_16118920', 3),\n",
       " ('b_52048599', 3),\n",
       " ('b_26468126', 3),\n",
       " ('b_35407655', 3),\n",
       " ('b_3046873', 3),\n",
       " ('b_16485102', 3),\n",
       " ('b_53667046', 3),\n",
       " ('b_33962055', 2),\n",
       " ('b_26613991', 2),\n",
       " ('b_52056979', 2),\n",
       " ('b_53685744', 2),\n",
       " ('b_37496457', 2),\n",
       " ('b_47082593', 2),\n",
       " ('b_19770055', 2),\n",
       " ('b_16679751', 2),\n",
       " ('b_31055260', 2),\n",
       " ('b_48943003', 2),\n",
       " ('b_2780556', 2),\n",
       " ('b_49305165', 2),\n",
       " ('b_30162176', 2),\n",
       " ('b_52048605', 2),\n",
       " ('b_26143618', 2),\n",
       " ('b_4309622', 2),\n",
       " ('b_16409372', 2),\n",
       " ('b_21514052', 2),\n",
       " ('b_17271990', 2),\n",
       " ('b_53667041', 2),\n",
       " ('b_2836238', 2),\n",
       " ('b_14932423', 2),\n",
       " ('b_41762103', 2),\n",
       " ('b_33546422', 2),\n",
       " ('b_53667121', 2),\n",
       " ('b_5143163', 1),\n",
       " ('b_44959978', 1),\n",
       " ('b_34256062', 1),\n",
       " ('b_3862606', 1),\n",
       " ('b_3912399', 1),\n",
       " ('b_38693357', 1),\n",
       " ('b_21214273', 1),\n",
       " ('b_56242405', 1),\n",
       " ('b_54134135', 1),\n",
       " ('b_50915214', 1),\n",
       " ('b_3108475', 1),\n",
       " ('b_3985094', 1),\n",
       " ('b_3933545', 1),\n",
       " ('b_49111853', 1),\n",
       " ('b_11167568', 1),\n",
       " ('b_51696725', 1),\n",
       " ('b_21186692', 1),\n",
       " ('b_42449948', 1),\n",
       " ('b_12795629', 1),\n",
       " ('b_44843671', 1),\n",
       " ('b_3417117', 1),\n",
       " ('b_17093607', 1),\n",
       " ('b_34794085', 1),\n",
       " ('b_37356551', 1),\n",
       " ('b_35012948', 1),\n",
       " ('b_3918679', 1),\n",
       " ('b_14432887', 1),\n",
       " ('b_21546018', 1),\n",
       " ('b_47050476', 1),\n",
       " ('b_3268742', 1),\n",
       " ('b_24061238', 1),\n",
       " ('b_21571555', 1),\n",
       " ('b_53682941', 1),\n",
       " ('b_24674874', 1),\n",
       " ('b_24256770', 1),\n",
       " ('b_3191744', 1),\n",
       " ('b_34037674', 1),\n",
       " ('b_42706451', 1),\n",
       " ('b_43226695', 1),\n",
       " ('b_19328889', 1),\n",
       " ('b_38753526', 1),\n",
       " ('b_4530602', 1),\n",
       " ('b_28462560', 1),\n",
       " ('b_20307113', 1),\n",
       " ('b_13489161', 1),\n",
       " ('b_8997205', 1),\n",
       " ('b_28992015', 1),\n",
       " ('b_34893889', 1),\n",
       " ('b_53682898', 1),\n",
       " ('b_33406551', 1),\n",
       " ('b_52168829', 1),\n",
       " ('b_39590725', 1),\n",
       " ('b_25885475', 1),\n",
       " ('b_28733484', 1),\n",
       " ('b_4476797', 1),\n",
       " ('b_2420853', 1),\n",
       " ('b_33546735', 1),\n",
       " ('b_39756001', 1),\n",
       " ('b_53340470', 1),\n",
       " ('b_5645833', 1),\n",
       " ('b_30325836', 1),\n",
       " ('b_17847075', 1),\n",
       " ('b_49109253', 1),\n",
       " ('b_42762602', 1),\n",
       " ('b_53667050', 1),\n",
       " ('b_22484630', 1),\n",
       " ('b_39607761', 1),\n",
       " ('b_45431388', 1),\n",
       " ('b_32318224', 1),\n",
       " ('b_42762549', 1),\n",
       " ('b_34067745', 1),\n",
       " ('b_17305527', 1),\n",
       " ('b_30308069', 1),\n",
       " ('b_50741213', 1),\n",
       " ('b_31219751', 1),\n",
       " ('b_57223785', 1),\n",
       " ('b_39630003', 1),\n",
       " ('b_36045195', 1),\n",
       " ('b_48929567', 1),\n",
       " ('b_27439799', 1),\n",
       " ('b_8233648', 1),\n",
       " ('b_28139112', 1),\n",
       " ('b_20243751', 1),\n",
       " ('b_7679420', 1),\n",
       " ('b_47930210', 1),\n",
       " ('b_39665221', 1),\n",
       " ('b_44874709', 1),\n",
       " ('b_43742357', 1),\n",
       " ('b_50843475', 1),\n",
       " ('b_38005842', 1),\n",
       " ('b_4530990', 1),\n",
       " ('b_48238061', 1),\n",
       " ('b_53682915', 1),\n",
       " ('b_57363996', 1),\n",
       " ('b_7359035', 1),\n",
       " ('b_3930723', 1),\n",
       " ('b_45802322', 1),\n",
       " ('b_44408384', 1),\n",
       " ('b_21049577', 1),\n",
       " ('b_23729161', 1),\n",
       " ('b_40362640', 1),\n",
       " ('b_42611391', 1),\n",
       " ('b_50752714', 1),\n",
       " ('b_50742599', 1),\n",
       " ('b_39587497', 1),\n",
       " ('b_34972750', 1),\n",
       " ('b_30842774', 1),\n",
       " ('b_53008771', 1),\n",
       " ('b_27756859', 1),\n",
       " ('b_7663220', 1),\n",
       " ('b_34084076', 1),\n",
       " ('b_11334559', 1),\n",
       " ('b_7301633', 1),\n",
       " ('b_55874439', 1),\n",
       " ('b_49111854', 1),\n",
       " ('b_33271771', 1),\n",
       " ('b_17098624', 1),\n",
       " ('b_3508368', 1),\n",
       " ('b_49411116', 1),\n",
       " ('b_3114529', 1),\n",
       " ('b_4053619', 1),\n",
       " ('b_13145099', 1),\n",
       " ('b_6711232', 1),\n",
       " ('b_31054265', 1),\n",
       " ('b_44826589', 1)]"
      ]
     },
     "execution_count": 356,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_sorted_wifi([s1_train])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('b_40209888', 120),\n",
       " ('b_51726487', 118),\n",
       " ('b_52048607', 107),\n",
       " ('b_40362685', 90),\n",
       " ('b_44415092', 85),\n",
       " ('b_22472878', 85),\n",
       " ('b_40059953', 72),\n",
       " ('b_52025240', 45),\n",
       " ('b_51048975', 37),\n",
       " ('b_40059952', 34),\n",
       " ('b_20874926', 31),\n",
       " ('b_48793119', 30),\n",
       " ('b_55312507', 28),\n",
       " ('b_49278725', 28),\n",
       " ('b_28660854', 25),\n",
       " ('b_52048604', 24),\n",
       " ('b_44322551', 21),\n",
       " ('b_50640451', 21),\n",
       " ('b_30379445', 17),\n",
       " ('b_26467980', 17),\n",
       " ('b_55289296', 16),\n",
       " ('b_38366281', 14),\n",
       " ('b_44434668', 13),\n",
       " ('b_49111853', 11),\n",
       " ('b_44317506', 10),\n",
       " ('b_44317488', 10),\n",
       " ('b_14580929', 10),\n",
       " ('b_44499866', 9),\n",
       " ('b_52025241', 9),\n",
       " ('b_48249102', 8),\n",
       " ('b_38092938', 8),\n",
       " ('b_39574079', 8),\n",
       " ('b_49153489', 7),\n",
       " ('b_26467949', 7),\n",
       " ('b_44200910', 7),\n",
       " ('b_53667045', 7),\n",
       " ('b_26613990', 6),\n",
       " ('b_53667061', 6),\n",
       " ('b_44197194', 6),\n",
       " ('b_53667051', 5),\n",
       " ('b_38753526', 5),\n",
       " ('b_20201918', 5),\n",
       " ('b_51696063', 4),\n",
       " ('b_12695817', 4),\n",
       " ('b_12705141', 4),\n",
       " ('b_15638003', 3),\n",
       " ('b_12695816', 3),\n",
       " ('b_8262964', 3),\n",
       " ('b_38300990', 3),\n",
       " ('b_40362640', 3),\n",
       " ('b_49111854', 3),\n",
       " ('b_49148817', 3),\n",
       " ('b_45161221', 3),\n",
       " ('b_19884119', 2),\n",
       " ('b_54149190', 2),\n",
       " ('b_34039495', 2),\n",
       " ('b_3201662', 2),\n",
       " ('b_17079183', 2),\n",
       " ('b_16118920', 2),\n",
       " ('b_38362969', 2),\n",
       " ('b_44520897', 2),\n",
       " ('b_49148816', 2),\n",
       " ('b_21412250', 1),\n",
       " ('b_56612552', 1),\n",
       " ('b_33962055', 1),\n",
       " ('b_3862606', 1),\n",
       " ('b_30118357', 1),\n",
       " ('b_15638004', 1),\n",
       " ('b_50274240', 1),\n",
       " ('b_55592358', 1),\n",
       " ('b_51696066', 1),\n",
       " ('b_54134135', 1),\n",
       " ('b_19691207', 1),\n",
       " ('b_13057106', 1),\n",
       " ('b_39607761', 1),\n",
       " ('b_27468209', 1),\n",
       " ('b_31513905', 1),\n",
       " ('b_32072203', 1),\n",
       " ('b_48793318', 1),\n",
       " ('b_3601306', 1),\n",
       " ('b_55980402', 1),\n",
       " ('b_39756001', 1),\n",
       " ('b_49109253', 1),\n",
       " ('b_30559295', 1),\n",
       " ('b_26740399', 1),\n",
       " ('b_19555539', 1),\n",
       " ('b_31517286', 1),\n",
       " ('b_17079184', 1),\n",
       " ('b_14753472', 1),\n",
       " ('b_33837518', 1),\n",
       " ('b_28733484', 1),\n",
       " ('b_53486613', 1),\n",
       " ('b_37508136', 1),\n",
       " ('b_46048117', 1),\n",
       " ('b_25339934', 1),\n",
       " ('b_39587497', 1),\n",
       " ('b_51145806', 1),\n",
       " ('b_34972750', 1),\n",
       " ('b_2781585', 1),\n",
       " ('b_26464864', 1),\n",
       " ('b_31056817', 1),\n",
       " ('b_53667047', 1),\n",
       " ('b_48238061', 1),\n",
       " ('b_24072852', 1),\n",
       " ('b_15383492', 1),\n",
       " ('b_49305165', 1),\n",
       " ('b_696895', 1),\n",
       " ('b_53667044', 1)]"
      ]
     },
     "execution_count": 357,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_sorted_wifi([s2_train])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('b_40209888', 549),\n",
       " ('b_51726487', 480),\n",
       " ('b_22472878', 364),\n",
       " ('b_52048604', 314),\n",
       " ('b_44415092', 303),\n",
       " ('b_52048607', 234),\n",
       " ('b_26467980', 208),\n",
       " ('b_30379445', 191),\n",
       " ('b_40362685', 180),\n",
       " ('b_49278725', 179),\n",
       " ('b_48793119', 179),\n",
       " ('b_55289296', 175),\n",
       " ('b_40059953', 160),\n",
       " ('b_39574079', 156),\n",
       " ('b_40059952', 140),\n",
       " ('b_51048975', 135),\n",
       " ('b_12705141', 125),\n",
       " ('b_28660854', 103),\n",
       " ('b_14580929', 86),\n",
       " ('b_53667045', 84),\n",
       " ('b_44200910', 83),\n",
       " ('b_20874926', 82),\n",
       " ('b_15638003', 65),\n",
       " ('b_51271474', 63),\n",
       " ('b_26467949', 47),\n",
       " ('b_38366281', 46),\n",
       " ('b_17079182', 41),\n",
       " ('b_17079183', 38),\n",
       " ('b_52025240', 34),\n",
       " ('b_49148817', 33),\n",
       " ('b_53667047', 33),\n",
       " ('b_31112294', 32),\n",
       " ('b_17079184', 32),\n",
       " ('b_48249102', 30),\n",
       " ('b_55016265', 30),\n",
       " ('b_34039495', 28),\n",
       " ('b_44434668', 27),\n",
       " ('b_30559295', 22),\n",
       " ('b_44322551', 20),\n",
       " ('b_15638004', 17),\n",
       " ('b_44197194', 17),\n",
       " ('b_53667051', 15),\n",
       " ('b_55312507', 15),\n",
       " ('b_19884119', 14),\n",
       " ('b_12695816', 13),\n",
       " ('b_49148816', 11),\n",
       " ('b_50640451', 10),\n",
       " ('b_44317506', 10),\n",
       " ('b_38300990', 10),\n",
       " ('b_51696066', 8),\n",
       " ('b_12695817', 8),\n",
       " ('b_26467981', 8),\n",
       " ('b_51145806', 8),\n",
       " ('b_14753472', 7),\n",
       " ('b_53667061', 7),\n",
       " ('b_44317488', 7),\n",
       " ('b_53497184', 7),\n",
       " ('b_26613990', 6),\n",
       " ('b_52025241', 6),\n",
       " ('b_9025795', 6),\n",
       " ('b_20201918', 6),\n",
       " ('b_55606879', 6),\n",
       " ('b_54149190', 5),\n",
       " ('b_23895926', 5),\n",
       " ('b_42415389', 5),\n",
       " ('b_18721144', 4),\n",
       " ('b_30118357', 3),\n",
       " ('b_29648562', 3),\n",
       " ('b_44499866', 3),\n",
       " ('b_19770055', 3),\n",
       " ('b_31055260', 3),\n",
       " ('b_49153489', 3),\n",
       " ('b_4509235', 3),\n",
       " ('b_8262964', 3),\n",
       " ('b_52048605', 3),\n",
       " ('b_43623124', 3),\n",
       " ('b_16118920', 3),\n",
       " ('b_52048599', 3),\n",
       " ('b_17271990', 3),\n",
       " ('b_26468126', 3),\n",
       " ('b_35407655', 3),\n",
       " ('b_3046873', 3),\n",
       " ('b_16485102', 3),\n",
       " ('b_53667046', 3),\n",
       " ('b_33962055', 2),\n",
       " ('b_26613991', 2),\n",
       " ('b_52056979', 2),\n",
       " ('b_54134135', 2),\n",
       " ('b_53685744', 2),\n",
       " ('b_21186692', 2),\n",
       " ('b_37496457', 2),\n",
       " ('b_47082593', 2),\n",
       " ('b_16679751', 2),\n",
       " ('b_48943003', 2),\n",
       " ('b_38753526', 2),\n",
       " ('b_2780556', 2),\n",
       " ('b_49305165', 2),\n",
       " ('b_30162176', 2),\n",
       " ('b_26143618', 2),\n",
       " ('b_4309622', 2),\n",
       " ('b_16409372', 2),\n",
       " ('b_21514052', 2),\n",
       " ('b_44408384', 2),\n",
       " ('b_53667041', 2),\n",
       " ('b_2836238', 2),\n",
       " ('b_53008771', 2),\n",
       " ('b_27756859', 2),\n",
       " ('b_14932423', 2),\n",
       " ('b_41762103', 2),\n",
       " ('b_33546422', 2),\n",
       " ('b_53667121', 2),\n",
       " ('b_5143163', 1),\n",
       " ('b_44959978', 1),\n",
       " ('b_34256062', 1),\n",
       " ('b_3862606', 1),\n",
       " ('b_3912399', 1),\n",
       " ('b_38693357', 1),\n",
       " ('b_21214273', 1),\n",
       " ('b_56242405', 1),\n",
       " ('b_50915214', 1),\n",
       " ('b_3108475', 1),\n",
       " ('b_3985094', 1),\n",
       " ('b_3933545', 1),\n",
       " ('b_49111853', 1),\n",
       " ('b_11167568', 1),\n",
       " ('b_51696725', 1),\n",
       " ('b_42449948', 1),\n",
       " ('b_12795629', 1),\n",
       " ('b_44843671', 1),\n",
       " ('b_3417117', 1),\n",
       " ('b_17093607', 1),\n",
       " ('b_34794085', 1),\n",
       " ('b_37356551', 1),\n",
       " ('b_35012948', 1),\n",
       " ('b_3918679', 1),\n",
       " ('b_14432887', 1),\n",
       " ('b_21546018', 1),\n",
       " ('b_47050476', 1),\n",
       " ('b_3268742', 1),\n",
       " ('b_24061238', 1),\n",
       " ('b_21571555', 1),\n",
       " ('b_53682941', 1),\n",
       " ('b_24674874', 1),\n",
       " ('b_24256770', 1),\n",
       " ('b_3191744', 1),\n",
       " ('b_34037674', 1),\n",
       " ('b_42706451', 1),\n",
       " ('b_43226695', 1),\n",
       " ('b_19328889', 1),\n",
       " ('b_4530602', 1),\n",
       " ('b_28462560', 1),\n",
       " ('b_20307113', 1),\n",
       " ('b_13489161', 1),\n",
       " ('b_8997205', 1),\n",
       " ('b_28992015', 1),\n",
       " ('b_34893889', 1),\n",
       " ('b_53682898', 1),\n",
       " ('b_33406551', 1),\n",
       " ('b_52168829', 1),\n",
       " ('b_39590725', 1),\n",
       " ('b_25885475', 1),\n",
       " ('b_28733484', 1),\n",
       " ('b_4476797', 1),\n",
       " ('b_2420853', 1),\n",
       " ('b_33546735', 1),\n",
       " ('b_39756001', 1),\n",
       " ('b_53340470', 1),\n",
       " ('b_5645833', 1),\n",
       " ('b_30325836', 1),\n",
       " ('b_17847075', 1),\n",
       " ('b_49109253', 1),\n",
       " ('b_42762602', 1),\n",
       " ('b_53667050', 1),\n",
       " ('b_22484630', 1),\n",
       " ('b_39607761', 1),\n",
       " ('b_45431388', 1),\n",
       " ('b_32318224', 1),\n",
       " ('b_42762549', 1),\n",
       " ('b_34067745', 1),\n",
       " ('b_17305527', 1),\n",
       " ('b_30308069', 1),\n",
       " ('b_50741213', 1),\n",
       " ('b_31219751', 1),\n",
       " ('b_57223785', 1),\n",
       " ('b_39630003', 1),\n",
       " ('b_36045195', 1),\n",
       " ('b_48929567', 1),\n",
       " ('b_27439799', 1),\n",
       " ('b_8233648', 1),\n",
       " ('b_28139112', 1),\n",
       " ('b_20243751', 1),\n",
       " ('b_7679420', 1),\n",
       " ('b_47930210', 1),\n",
       " ('b_39665221', 1),\n",
       " ('b_44874709', 1),\n",
       " ('b_43742357', 1),\n",
       " ('b_50843475', 1),\n",
       " ('b_38005842', 1),\n",
       " ('b_4530990', 1),\n",
       " ('b_48238061', 1),\n",
       " ('b_53682915', 1),\n",
       " ('b_57363996', 1),\n",
       " ('b_7359035', 1),\n",
       " ('b_3930723', 1),\n",
       " ('b_45802322', 1),\n",
       " ('b_21049577', 1),\n",
       " ('b_23729161', 1),\n",
       " ('b_40362640', 1),\n",
       " ('b_42611391', 1),\n",
       " ('b_50752714', 1),\n",
       " ('b_50742599', 1),\n",
       " ('b_39587497', 1),\n",
       " ('b_34972750', 1),\n",
       " ('b_30842774', 1),\n",
       " ('b_7663220', 1),\n",
       " ('b_34084076', 1),\n",
       " ('b_11334559', 1),\n",
       " ('b_7301633', 1),\n",
       " ('b_55874439', 1),\n",
       " ('b_49111854', 1),\n",
       " ('b_33271771', 1),\n",
       " ('b_17098624', 1),\n",
       " ('b_3508368', 1),\n",
       " ('b_49411116', 1),\n",
       " ('b_3114529', 1),\n",
       " ('b_4053619', 1),\n",
       " ('b_13145099', 1),\n",
       " ('b_6711232', 1),\n",
       " ('b_31054265', 1),\n",
       " ('b_44826589', 1)]"
      ]
     },
     "execution_count": 329,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_sorted_wifi_just_train(s1_train,s1_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('b_40209888', 393),\n",
       " ('b_51726487', 386),\n",
       " ('b_52048607', 349),\n",
       " ('b_40362685', 308),\n",
       " ('b_22472878', 240),\n",
       " ('b_44415092', 239),\n",
       " ('b_20874926', 220),\n",
       " ('b_51048975', 138),\n",
       " ('b_40059953', 127),\n",
       " ('b_14580929', 84),\n",
       " ('b_52025240', 81),\n",
       " ('b_44322551', 80),\n",
       " ('b_28660854', 75),\n",
       " ('b_55312507', 72),\n",
       " ('b_26467980', 67),\n",
       " ('b_40059952', 66),\n",
       " ('b_50640451', 65),\n",
       " ('b_44317506', 53),\n",
       " ('b_53667045', 51),\n",
       " ('b_52048604', 50),\n",
       " ('b_48793119', 49),\n",
       " ('b_38366281', 48),\n",
       " ('b_55289296', 47),\n",
       " ('b_30379445', 43),\n",
       " ('b_49278725', 43),\n",
       " ('b_44434668', 42),\n",
       " ('b_39574079', 41),\n",
       " ('b_53667047', 35),\n",
       " ('b_44499866', 34),\n",
       " ('b_26467949', 32),\n",
       " ('b_38092938', 25),\n",
       " ('b_38753526', 23),\n",
       " ('b_44520897', 23),\n",
       " ('b_52025241', 21),\n",
       " ('b_49111853', 21),\n",
       " ('b_49148817', 21),\n",
       " ('b_44200910', 19),\n",
       " ('b_48249102', 18),\n",
       " ('b_53667061', 18),\n",
       " ('b_44317488', 15),\n",
       " ('b_15638003', 12),\n",
       " ('b_49148816', 12),\n",
       " ('b_26613990', 11),\n",
       " ('b_54149190', 9),\n",
       " ('b_12705141', 9),\n",
       " ('b_38300990', 9),\n",
       " ('b_17079184', 9),\n",
       " ('b_44197194', 9),\n",
       " ('b_53667051', 8),\n",
       " ('b_49153489', 8),\n",
       " ('b_20201918', 8),\n",
       " ('b_17079183', 8),\n",
       " ('b_54134135', 7),\n",
       " ('b_49111854', 7),\n",
       " ('b_34039495', 6),\n",
       " ('b_12695817', 6),\n",
       " ('b_12695816', 6),\n",
       " ('b_8262964', 5),\n",
       " ('b_40362640', 5),\n",
       " ('b_19884119', 4),\n",
       " ('b_51696066', 4),\n",
       " ('b_51696063', 4),\n",
       " ('b_51145806', 4),\n",
       " ('b_30118357', 3),\n",
       " ('b_30559295', 3),\n",
       " ('b_38362969', 3),\n",
       " ('b_45161221', 3),\n",
       " ('b_53667044', 3),\n",
       " ('b_33962055', 2),\n",
       " ('b_3201662', 2),\n",
       " ('b_49109253', 2),\n",
       " ('b_16118920', 2),\n",
       " ('b_14753472', 2),\n",
       " ('b_28733484', 2),\n",
       " ('b_2781585', 2),\n",
       " ('b_48238061', 2),\n",
       " ('b_21412250', 1),\n",
       " ('b_56612552', 1),\n",
       " ('b_3862606', 1),\n",
       " ('b_15638004', 1),\n",
       " ('b_50274240', 1),\n",
       " ('b_55592358', 1),\n",
       " ('b_19691207', 1),\n",
       " ('b_13057106', 1),\n",
       " ('b_39607761', 1),\n",
       " ('b_27468209', 1),\n",
       " ('b_31513905', 1),\n",
       " ('b_32072203', 1),\n",
       " ('b_48793318', 1),\n",
       " ('b_3601306', 1),\n",
       " ('b_55980402', 1),\n",
       " ('b_39756001', 1),\n",
       " ('b_26740399', 1),\n",
       " ('b_19555539', 1),\n",
       " ('b_31517286', 1),\n",
       " ('b_33837518', 1),\n",
       " ('b_53486613', 1),\n",
       " ('b_37508136', 1),\n",
       " ('b_46048117', 1),\n",
       " ('b_25339934', 1),\n",
       " ('b_39587497', 1),\n",
       " ('b_34972750', 1),\n",
       " ('b_26464864', 1),\n",
       " ('b_31056817', 1),\n",
       " ('b_24072852', 1),\n",
       " ('b_15383492', 1),\n",
       " ('b_49305165', 1),\n",
       " ('b_696895', 1)]"
      ]
     },
     "execution_count": 355,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_sorted_wifi_just_train(s2_train,s2_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lon lat\n",
    "# ptrain_lonlat = train_lonlats[(_train_b_y[:,33] == 1) | (_train_b_y[:,61]==1)]\n",
    "# pvalid_lonlat = valid_lonlats[(_valid_b_y[:,33] == 1) | (_valid_b_y[:,61]==1)]\n",
    "# ptrain_x = np.concatenate([ptrain_x,ptrain_lonlat],axis=1)\n",
    "# pvalid_x = np.concatenate([pvalid_x,pvalid_lonlat],axis=1)\n",
    "\n",
    "\n",
    "# def rank_wifi(x):\n",
    "#     def _rank(x,m):\n",
    "#         sig_site = zip(list(x),range(m))\n",
    "#         sig_site = sorted(sig_site,key=lambda x: -x[0])\n",
    "#         rs = []\n",
    "#         for _r, (_sig,_site) in enumerate(sig_site):\n",
    "#             if _sig == -115:\n",
    "#                 rs.append((_site,m))\n",
    "#             else:\n",
    "#                 rs.append((_site,_r))\n",
    "#         rs = sorted(rs,key=lambda x:x[0])\n",
    "#         rs = [_r[1] for _r in rs]\n",
    "#         return rs\n",
    "            \n",
    "#     m = x.shape[1]\n",
    "#     return np.vstack(map( lambda a:_rank(a,m), x))\n",
    "    \n",
    "# ptrain_x_rank = rank_wifi(ptrain_x)\n",
    "# pvalid_x_rank = rank_wifi(pvalid_x)\n",
    "# ptrain_x = np.concatenate([ptrain_x,ptrain_x_rank],axis=1)\n",
    "# pvalid_x = np.concatenate([pvalid_x,pvalid_x_rank],axis=1)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "[2, 3, 4, 5, 6, 7, 9, 13, 21, 27, 29, 37, 48, 49, 60, 61, 71, 79, 94]\n",
      "Fitting 1 classifiers...\n",
      "Fitting classifier1: randomforestclassifier (1/1)\n",
      "Training and fitting fold 1 of 3...\n",
      "Training and fitting fold 2 of 3...\n",
      "Training and fitting fold 3 of 3...\n",
      "origin 0.999551569507\n",
      "error shape, 2\n",
      "[1 1]\n",
      "error pos shape 2\n",
      "error index\n",
      "[1931 1932]\n",
      "s_1346456\n",
      "\n",
      "1\n",
      "[24, 32, 41, 50, 53, 64, 66, 88, 92, 98, 101, 223, 236, 280, 292, 300, 334, 337, 387, 484, 487, 488, 523]\n",
      "Fitting 1 classifiers...\n",
      "Fitting classifier1: randomforestclassifier (1/1)\n",
      "Training and fitting fold 1 of 3...\n",
      "Training and fitting fold 2 of 3...\n",
      "Training and fitting fold 3 of 3...\n",
      "origin 0.999551569507\n",
      "error shape, 2\n",
      "[1 1]\n",
      "error pos shape 2\n",
      "error index\n",
      "[1193 1551]\n",
      "s_1392063\n",
      "\n",
      "2\n",
      "[23, 26, 34, 65, 123, 133, 169, 171, 230, 255, 302, 373, 405, 408, 540, 774, 776, 783, 800, 801, 810, 837, 924, 932, 974, 1304, 1584, 1969]\n",
      "Fitting 1 classifiers...\n",
      "Fitting classifier1: randomforestclassifier (1/1)\n",
      "Training and fitting fold 1 of 3...\n",
      "Training and fitting fold 2 of 3...\n",
      "Training and fitting fold 3 of 3...\n",
      "origin 1.0\n",
      "error shape, 0\n",
      "[]\n",
      "error pos shape 0\n",
      "error index\n",
      "[]\n",
      "s_1452940\n",
      "\n",
      "3\n",
      "[56, 267, 369, 438, 521, 531, 611, 619, 681, 687, 705, 718, 848, 1004, 1744, 4479]\n",
      "Fitting 1 classifiers...\n",
      "Fitting classifier1: randomforestclassifier (1/1)\n",
      "Training and fitting fold 1 of 3...\n",
      "Training and fitting fold 2 of 3...\n",
      "Training and fitting fold 3 of 3...\n",
      "origin 1.0\n",
      "error shape, 0\n",
      "[]\n",
      "error pos shape 0\n",
      "error index\n",
      "[]\n",
      "s_1671756\n",
      "\n",
      "4\n",
      "[258, 376, 390, 424, 427, 921]\n",
      "Fitting 1 classifiers...\n",
      "Fitting classifier1: randomforestclassifier (1/1)\n",
      "Training and fitting fold 1 of 3...\n",
      "Training and fitting fold 2 of 3...\n",
      "Training and fitting fold 3 of 3...\n",
      "origin 1.0\n",
      "error shape, 0\n",
      "[]\n",
      "error pos shape 0\n",
      "error index\n",
      "[]\n",
      "s_1882105\n",
      "\n",
      "5\n",
      "[20, 31, 34, 55, 57, 58, 62, 63, 117, 119, 139, 152, 244, 260, 282, 412, 423]\n",
      "Fitting 1 classifiers...\n",
      "Fitting classifier1: randomforestclassifier (1/1)\n",
      "Training and fitting fold 1 of 3...\n",
      "Training and fitting fold 2 of 3...\n",
      "Training and fitting fold 3 of 3...\n",
      "origin 0.998430493274\n",
      "error shape, 7\n",
      "[0 0 0 0 1 0 0]\n",
      "error pos shape 1\n",
      "error index\n",
      "[ 668 1468 1784 1810 2323 2522 3234]\n",
      "s_1944816\n",
      "\n",
      "6\n",
      "[58, 96, 106, 128, 130, 150, 184, 248, 258, 310, 322, 355, 384, 410, 426]\n",
      "Fitting 1 classifiers...\n",
      "Fitting classifier1: randomforestclassifier (1/1)\n",
      "Training and fitting fold 1 of 3...\n",
      "Training and fitting fold 2 of 3...\n",
      "Training and fitting fold 3 of 3...\n",
      "origin 0.996188340807\n",
      "error shape, 17\n",
      "[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "error pos shape 17\n",
      "error index\n",
      "[ 750  939 1287 1303 1444 1667 2171 2316 2548 2792 3934 4406 4425 4426 4427\n",
      " 4428 4429]\n",
      "s_1997293\n",
      "\n",
      "7\n",
      "[58, 83, 160, 192, 197, 206, 207, 211, 219, 224, 237, 331, 385, 526, 570, 628]\n",
      "Fitting 1 classifiers...\n",
      "Fitting classifier1: randomforestclassifier (1/1)\n",
      "Training and fitting fold 1 of 3...\n",
      "Training and fitting fold 2 of 3...\n",
      "Training and fitting fold 3 of 3...\n",
      "origin 0.999103139013\n",
      "error shape, 4\n",
      "[0 1 1 1]\n",
      "error pos shape 3\n",
      "error index\n",
      "[ 408 1143 2503 3862]\n",
      "s_2257337\n",
      "\n",
      "8\n",
      "[2, 3, 4, 6, 7, 13, 21, 27, 29, 37, 49, 60, 61, 71, 79, 91, 94]\n",
      "Fitting 1 classifiers...\n",
      "Fitting classifier1: randomforestclassifier (1/1)\n",
      "Training and fitting fold 1 of 3...\n",
      "Training and fitting fold 2 of 3...\n",
      "Training and fitting fold 3 of 3...\n",
      "origin 0.998878923767\n",
      "error shape, 5\n",
      "[1 1 1 1 1]\n",
      "error pos shape 5\n",
      "error index\n",
      "[1572 1945 2050 4151 4152]\n",
      "s_2423948\n",
      "\n",
      "9\n",
      "[269, 424, 427, 3756]\n",
      "Fitting 1 classifiers...\n",
      "Fitting classifier1: randomforestclassifier (1/1)\n",
      "Training and fitting fold 1 of 3...\n",
      "Training and fitting fold 2 of 3...\n",
      "Training and fitting fold 3 of 3...\n",
      "origin 1.0\n",
      "error shape, 0\n",
      "[]\n",
      "error pos shape 0\n",
      "error index\n",
      "[]\n",
      "s_2631847\n",
      "\n",
      "10\n",
      "[10, 112, 168, 175]\n",
      "Fitting 1 classifiers...\n",
      "Fitting classifier1: randomforestclassifier (1/1)\n",
      "Training and fitting fold 1 of 3...\n",
      "Training and fitting fold 2 of 3...\n",
      "Training and fitting fold 3 of 3...\n",
      "origin 1.0\n",
      "error shape, 0\n",
      "[]\n",
      "error pos shape 0\n",
      "error index\n",
      "[]\n",
      "s_2651838\n",
      "\n",
      "11\n",
      "[20, 31, 34, 55, 57, 62, 63, 90, 104, 113, 137, 311]\n",
      "Fitting 1 classifiers...\n",
      "Fitting classifier1: randomforestclassifier (1/1)\n",
      "Training and fitting fold 1 of 3...\n",
      "Training and fitting fold 2 of 3...\n",
      "Training and fitting fold 3 of 3...\n",
      "origin 0.999775784753\n",
      "error shape, 1\n",
      "[1]\n",
      "error pos shape 1\n",
      "error index\n",
      "[2936]\n",
      "s_2691361\n",
      "\n",
      "12\n",
      "[58, 128, 130, 150, 248, 258, 269, 322, 355, 368, 376, 403, 410, 532, 767]\n",
      "Fitting 1 classifiers...\n",
      "Fitting classifier1: randomforestclassifier (1/1)\n",
      "Training and fitting fold 1 of 3...\n",
      "Training and fitting fold 2 of 3...\n",
      "Training and fitting fold 3 of 3...\n",
      "origin 1.0\n",
      "error shape, 0\n",
      "[]\n",
      "error pos shape 0\n",
      "error index\n",
      "[]\n",
      "s_2741830\n",
      "\n",
      "13\n",
      "[58, 96, 106, 128, 134, 150, 184, 212, 248, 258, 267, 269, 322, 369, 376, 390, 407, 424, 427, 521, 531, 616, 718, 1392, 1630, 4588]\n",
      "Fitting 1 classifiers...\n",
      "Fitting classifier1: randomforestclassifier (1/1)\n",
      "Training and fitting fold 1 of 3...\n",
      "Training and fitting fold 2 of 3...\n",
      "Training and fitting fold 3 of 3...\n",
      "origin 1.0\n",
      "error shape, 0\n",
      "[]\n",
      "error pos shape 0\n",
      "error index\n",
      "[]\n",
      "s_2795244\n",
      "\n",
      "14\n",
      "[0, 1, 12, 17, 19, 28, 36, 56]\n",
      "Fitting 1 classifiers...\n",
      "Fitting classifier1: randomforestclassifier (1/1)\n",
      "Training and fitting fold 1 of 3...\n",
      "Training and fitting fold 2 of 3...\n",
      "Training and fitting fold 3 of 3...\n",
      "origin 0.9966367713\n",
      "error shape, 15\n",
      "[0 0 0 1 0 0 0 1 1 0 1 1 1 1 0]\n",
      "error pos shape 7\n",
      "error index\n",
      "[ 429  626  627  639 2248 2249 2251 2565 2566 3080 3097 3172 3461 3462 3934]\n",
      "s_2939231\n",
      "\n",
      "15\n",
      "[258, 267, 269, 322, 376, 390, 521, 531, 818, 916, 921, 2621, 3650]\n",
      "Fitting 1 classifiers...\n",
      "Fitting classifier1: randomforestclassifier (1/1)\n",
      "Training and fitting fold 1 of 3...\n",
      "Training and fitting fold 2 of 3...\n",
      "Training and fitting fold 3 of 3...\n",
      "origin 1.0\n",
      "error shape, 0\n",
      "[]\n",
      "error pos shape 0\n",
      "error index\n",
      "[]\n",
      "s_2956845\n",
      "\n",
      "16\n",
      "[852]\n",
      "Fitting 1 classifiers...\n",
      "Fitting classifier1: randomforestclassifier (1/1)\n",
      "Training and fitting fold 1 of 3...\n",
      "Training and fitting fold 2 of 3...\n",
      "Training and fitting fold 3 of 3...\n",
      "origin 1.0\n",
      "error shape, 0\n",
      "[]\n",
      "error pos shape 0\n",
      "error index\n",
      "[]\n",
      "s_3031981\n",
      "\n",
      "17\n",
      "[41, 53, 88, 92, 98, 101, 236, 292, 300, 352, 383, 387, 404, 414, 447, 467, 500, 686]\n",
      "Fitting 1 classifiers...\n",
      "Fitting classifier1: randomforestclassifier (1/1)\n",
      "Training and fitting fold 1 of 3...\n",
      "Training and fitting fold 2 of 3...\n",
      "Training and fitting fold 3 of 3...\n",
      "origin 0.999775784753\n",
      "error shape, 1\n",
      "[1]\n",
      "error pos shape 1\n",
      "error index\n",
      "[3974]\n",
      "s_3036726\n",
      "\n",
      "18\n",
      "[3183, 3194, 3274, 3339, 3364, 3563, 3648, 3708, 3818, 3985, 3986, 4023, 4057, 4350, 4441, 4521, 4617]\n",
      "Fitting 1 classifiers...\n",
      "Fitting classifier1: randomforestclassifier (1/1)\n",
      "Training and fitting fold 1 of 3...\n",
      "Training and fitting fold 2 of 3...\n",
      "Training and fitting fold 3 of 3...\n",
      "origin 1.0\n",
      "error shape, 0\n",
      "[]\n",
      "error pos shape 0\n",
      "error index\n",
      "[]\n",
      "s_3081549\n",
      "\n",
      "19\n",
      "[20, 104, 240, 287, 478]\n",
      "Fitting 1 classifiers...\n",
      "Fitting classifier1: randomforestclassifier (1/1)\n",
      "Training and fitting fold 1 of 3...\n",
      "Training and fitting fold 2 of 3...\n",
      "Training and fitting fold 3 of 3...\n",
      "origin 1.0\n",
      "error shape, 0\n",
      "[]\n",
      "error pos shape 0\n",
      "error index\n",
      "[]\n",
      "s_3493162\n",
      "\n",
      "20\n",
      "[0, 1, 12, 28, 56, 148, 174, 267, 309, 333, 369, 438, 468, 499, 506, 511]\n",
      "Fitting 1 classifiers...\n",
      "Fitting classifier1: randomforestclassifier (1/1)\n",
      "Training and fitting fold 1 of 3...\n",
      "Training and fitting fold 2 of 3...\n",
      "Training and fitting fold 3 of 3...\n",
      "origin 1.0\n",
      "error shape, 0\n",
      "[]\n",
      "error pos shape 0\n",
      "error index\n",
      "[]\n",
      "s_3534278\n",
      "\n",
      "21\n",
      "[267, 438, 531, 611, 619, 681, 687, 705, 746, 852, 915]\n",
      "Fitting 1 classifiers...\n",
      "Fitting classifier1: randomforestclassifier (1/1)\n",
      "Training and fitting fold 1 of 3...\n",
      "Training and fitting fold 2 of 3...\n",
      "Training and fitting fold 3 of 3...\n",
      "origin 1.0\n",
      "error shape, 0\n",
      "[]\n",
      "error pos shape 0\n",
      "error index\n",
      "[]\n",
      "s_3542509\n",
      "\n",
      "22\n",
      "[15, 24, 32, 45, 47, 50, 52, 66, 70, 114, 146, 151, 153, 163, 186, 187, 234, 278]\n",
      "Fitting 1 classifiers...\n",
      "Fitting classifier1: randomforestclassifier (1/1)\n",
      "Training and fitting fold 1 of 3...\n",
      "Training and fitting fold 2 of 3...\n",
      "Training and fitting fold 3 of 3...\n",
      "origin 0.999551569507\n",
      "error shape, 2\n",
      "[1 1]\n",
      "error pos shape 2\n",
      "error index\n",
      "[2107 4227]\n",
      "s_3622929\n",
      "\n",
      "23\n",
      "[31, 55, 57, 62, 63, 83, 139, 160, 197, 207, 224, 289, 331, 412, 423, 1067, 1299, 1367, 2133]\n",
      "Fitting 1 classifiers...\n",
      "Fitting classifier1: randomforestclassifier (1/1)\n",
      "Training and fitting fold 1 of 3...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training and fitting fold 2 of 3...\n",
      "Training and fitting fold 3 of 3...\n",
      "origin 1.0\n",
      "error shape, 0\n",
      "[]\n",
      "error pos shape 0\n",
      "error index\n",
      "[]\n",
      "s_3643138\n",
      "\n",
      "24\n",
      "[59, 67, 72, 78, 80, 136, 154, 155, 275, 319, 356, 357, 374, 397, 406, 415, 451, 505, 507, 513, 547, 564, 720, 882]\n",
      "Fitting 1 classifiers...\n",
      "Fitting classifier1: randomforestclassifier (1/1)\n",
      "Training and fitting fold 1 of 3...\n",
      "Training and fitting fold 2 of 3...\n",
      "Training and fitting fold 3 of 3...\n",
      "origin 0.999775784753\n",
      "error shape, 1\n",
      "[1]\n",
      "error pos shape 1\n",
      "error index\n",
      "[906]\n",
      "s_3644057\n",
      "\n",
      "25\n",
      "[58, 96, 106, 128, 130, 150, 167, 184, 192, 206, 212, 310, 419, 426, 476]\n",
      "Fitting 1 classifiers...\n",
      "Fitting classifier1: randomforestclassifier (1/1)\n",
      "Training and fitting fold 1 of 3...\n",
      "Training and fitting fold 2 of 3...\n",
      "Training and fitting fold 3 of 3...\n",
      "origin 1.0\n",
      "error shape, 0\n",
      "[]\n",
      "error pos shape 0\n",
      "error index\n",
      "[]\n",
      "s_3658709\n",
      "\n",
      "26\n",
      "[8, 10, 20, 23, 26, 40, 65, 112, 123, 132, 133, 147, 168, 169, 171, 175, 180, 195, 202, 230, 302, 436]\n",
      "Fitting 1 classifiers...\n",
      "Fitting classifier1: randomforestclassifier (1/1)\n",
      "Training and fitting fold 1 of 3...\n",
      "Training and fitting fold 2 of 3...\n",
      "Training and fitting fold 3 of 3...\n",
      "origin 0.992600896861\n",
      "error shape, 33\n",
      "[0 0 1 1 0 1 1 0 0 0 0 0 0 0 0 1 1 0 1 0 0 0 0 0 1 0 1 1 1 0 0 0 0]\n",
      "error pos shape 11\n",
      "error index\n",
      "[ 241  493  728  799  927 1068 1135 1158 1493 1516 1553 1554 1590 1703 2094\n",
      " 2133 2240 2391 2601 2637 2734 2789 3144 3184 3498 3652 3711 4051 4057 4062\n",
      " 4117 4367 4435]\n",
      "s_3726779\n",
      "\n",
      "27\n",
      "[58, 128, 130, 150, 248, 258, 267, 269, 322, 368, 376, 390, 424, 427, 521, 532]\n",
      "Fitting 1 classifiers...\n",
      "Fitting classifier1: randomforestclassifier (1/1)\n",
      "Training and fitting fold 1 of 3...\n",
      "Training and fitting fold 2 of 3...\n",
      "Training and fitting fold 3 of 3...\n",
      "origin 0.999551569507\n",
      "error shape, 2\n",
      "[1 1]\n",
      "error pos shape 2\n",
      "error index\n",
      "[ 650 2672]\n",
      "s_3727061\n",
      "\n",
      "28\n",
      "[15, 24, 32, 41, 45, 50, 52, 53, 64, 66, 88, 92, 146, 153, 217, 221, 223, 229, 245, 278, 290, 317]\n",
      "Fitting 1 classifiers...\n",
      "Fitting classifier1: randomforestclassifier (1/1)\n",
      "Training and fitting fold 1 of 3...\n",
      "Training and fitting fold 2 of 3...\n",
      "Training and fitting fold 3 of 3...\n",
      "origin 0.99932735426\n",
      "error shape, 3\n",
      "[0 1 0]\n",
      "error pos shape 1\n",
      "error index\n",
      "[ 867  868 3620]\n",
      "s_3738103\n",
      "\n",
      "29\n",
      "[26, 65, 132, 133, 147, 171, 180, 230, 255, 257, 405, 408, 435, 452, 456, 492, 503]\n",
      "Fitting 1 classifiers...\n",
      "Fitting classifier1: randomforestclassifier (1/1)\n",
      "Training and fitting fold 1 of 3...\n",
      "Training and fitting fold 2 of 3...\n",
      "Training and fitting fold 3 of 3...\n",
      "origin 0.999551569507\n",
      "error shape, 2\n",
      "[1 1]\n",
      "error pos shape 2\n",
      "error index\n",
      "[ 704 3287]\n",
      "s_3743937\n",
      "\n",
      "30\n",
      "[130, 150, 248, 368, 403, 419, 470]\n",
      "Fitting 1 classifiers...\n",
      "Fitting classifier1: randomforestclassifier (1/1)\n",
      "Training and fitting fold 1 of 3...\n",
      "Training and fitting fold 2 of 3...\n",
      "Training and fitting fold 3 of 3...\n",
      "origin 1.0\n",
      "error shape, 0\n",
      "[]\n",
      "error pos shape 0\n",
      "error index\n",
      "[]\n",
      "s_3759545\n",
      "\n",
      "31\n",
      "[58, 96, 106, 167, 184, 192, 206, 211, 219, 237, 384, 517, 582]\n",
      "Fitting 1 classifiers...\n",
      "Fitting classifier1: randomforestclassifier (1/1)\n",
      "Training and fitting fold 1 of 3...\n",
      "Training and fitting fold 2 of 3...\n",
      "Training and fitting fold 3 of 3...\n",
      "origin 1.0\n",
      "error shape, 0\n",
      "[]\n",
      "error pos shape 0\n",
      "error index\n",
      "[]\n",
      "s_3779904\n",
      "\n",
      "32\n",
      "[16, 30, 35, 39, 46, 68, 73, 84, 126, 166, 199, 203, 274, 306, 353, 377, 378, 489, 591, 642, 684]\n",
      "Fitting 1 classifiers...\n",
      "Fitting classifier1: randomforestclassifier (1/1)\n",
      "Training and fitting fold 1 of 3...\n",
      "Training and fitting fold 2 of 3...\n",
      "Training and fitting fold 3 of 3...\n",
      "origin 0.998878923767\n",
      "error shape, 5\n",
      "[1 1 0 0 0]\n",
      "error pos shape 2\n",
      "error index\n",
      "[ 809 3379 4108 4110 4114]\n",
      "s_3818635\n",
      "\n",
      "33\n",
      "[2, 3, 4, 6, 7, 9, 13, 21, 27, 29, 37, 42, 49, 54, 60, 61, 71, 79, 91, 94, 125, 165]\n",
      "Fitting 1 classifiers...\n",
      "Fitting classifier1: randomforestclassifier (1/1)\n",
      "Training and fitting fold 1 of 3...\n",
      "Training and fitting fold 2 of 3...\n",
      "Training and fitting fold 3 of 3...\n",
      "origin 0.995291479821\n",
      "error shape, 21\n",
      "[1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 1]\n",
      "error pos shape 18\n",
      "error index\n",
      "[   8   94  104  468  606  720  863  989 1397 1484 1513 1524 2182 2374 2634\n",
      " 2964 3251 3860 4017 4219 4301]\n",
      "s_3827392\n",
      "\n",
      "34\n",
      "[10, 26, 40, 65, 112, 123, 133, 147, 168, 171, 175, 255, 257, 302, 393, 408, 436, 452, 466, 516, 566, 612, 614, 657]\n",
      "Fitting 1 classifiers...\n",
      "Fitting classifier1: randomforestclassifier (1/1)\n",
      "Training and fitting fold 1 of 3...\n",
      "Training and fitting fold 2 of 3...\n",
      "Training and fitting fold 3 of 3...\n",
      "origin 0.999775784753\n",
      "error shape, 1\n",
      "[0]\n",
      "error pos shape 0\n",
      "error index\n",
      "[3287]\n",
      "s_3833950\n",
      "\n",
      "35\n",
      "[31, 55, 57, 58, 62, 63, 83, 96, 139, 160, 224, 244, 266, 423, 540]\n",
      "Fitting 1 classifiers...\n",
      "Fitting classifier1: randomforestclassifier (1/1)\n",
      "Training and fitting fold 1 of 3...\n",
      "Training and fitting fold 2 of 3...\n",
      "Training and fitting fold 3 of 3...\n",
      "origin 0.99932735426\n",
      "error shape, 3\n",
      "[1 0 0]\n",
      "error pos shape 1\n",
      "error index\n",
      "[1469 2503 4025]\n",
      "s_3902873\n",
      "\n",
      "36\n",
      "37\n",
      "[130, 150, 248, 258, 267, 269, 322, 368, 369, 376, 390, 403, 410, 424, 427, 532, 1023]\n",
      "Fitting 1 classifiers...\n",
      "Fitting classifier1: randomforestclassifier (1/1)\n",
      "Training and fitting fold 1 of 3...\n",
      "Training and fitting fold 2 of 3...\n",
      "Training and fitting fold 3 of 3...\n",
      "origin 1.0\n",
      "error shape, 0\n",
      "[]\n",
      "error pos shape 0\n",
      "error index\n",
      "[]\n",
      "s_3907298\n",
      "\n",
      "38\n",
      "[31, 55, 62, 83, 244, 266, 289, 412, 423]\n",
      "Fitting 1 classifiers...\n",
      "Fitting classifier1: randomforestclassifier (1/1)\n",
      "Training and fitting fold 1 of 3...\n",
      "Training and fitting fold 2 of 3...\n",
      "Training and fitting fold 3 of 3...\n",
      "origin 1.0\n",
      "error shape, 0\n",
      "[]\n",
      "error pos shape 0\n",
      "error index\n",
      "[]\n",
      "s_3931209\n",
      "\n",
      "39\n",
      "[10, 20, 31, 34, 55, 57, 58, 63, 83, 96, 134, 139, 160, 197, 207, 211, 219, 224, 266, 289, 321, 331, 384, 526]\n",
      "Fitting 1 classifiers...\n",
      "Fitting classifier1: randomforestclassifier (1/1)\n",
      "Training and fitting fold 1 of 3...\n",
      "Training and fitting fold 2 of 3...\n",
      "Training and fitting fold 3 of 3...\n",
      "origin 1.0\n",
      "error shape, 0\n",
      "[]\n",
      "error pos shape 0\n",
      "error index\n",
      "[]\n",
      "s_3939053\n",
      "\n",
      "40\n",
      "[10, 20, 23, 34, 55, 57, 62, 63, 74, 90, 104, 134, 240, 287, 407, 478, 1747]\n",
      "Fitting 1 classifiers...\n",
      "Fitting classifier1: randomforestclassifier (1/1)\n",
      "Training and fitting fold 1 of 3...\n",
      "Training and fitting fold 2 of 3...\n",
      "Training and fitting fold 3 of 3...\n",
      "origin 0.999775784753\n",
      "error shape, 1\n",
      "[1]\n",
      "error pos shape 1\n",
      "error index\n",
      "[97]\n",
      "s_4005274\n",
      "\n",
      "41\n",
      "[8, 10, 20, 23, 26, 31, 34, 40, 57, 65, 74, 112, 123, 132, 133, 134, 147, 168, 175, 180, 195, 255]\n",
      "Fitting 1 classifiers...\n",
      "Fitting classifier1: randomforestclassifier (1/1)\n",
      "Training and fitting fold 1 of 3...\n",
      "Training and fitting fold 2 of 3...\n",
      "Training and fitting fold 3 of 3...\n",
      "origin 0.986547085202\n",
      "error shape, 60\n",
      "[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "error pos shape 58\n",
      "error index\n",
      "[  38  124  240  241  408  493  709  898  920  927 1114 1158 1394 1468 1516\n",
      " 1565 1590 1703 1704 1733 1783 1784 1785 1898 1921 1967 2014 2080 2094 2193\n",
      " 2463 2677 2734 2789 2821 2831 3080 3131 3144 3349 3354 3385 3400 3516 3517\n",
      " 3652 3711 3755 3757 4062 4094 4103 4117 4128 4129 4177 4355 4412 4435 4454]\n",
      "s_4009954\n",
      "\n",
      "42\n",
      "[107, 161, 173, 182, 208, 228, 231, 238, 242, 265, 268, 271, 285, 286, 293, 343, 349, 365, 401, 434, 437, 442, 450, 481]\n",
      "Fitting 1 classifiers...\n",
      "Fitting classifier1: randomforestclassifier (1/1)\n",
      "Training and fitting fold 1 of 3...\n",
      "Training and fitting fold 2 of 3...\n",
      "Training and fitting fold 3 of 3...\n",
      "origin 0.998206278027\n",
      "error shape, 8\n",
      "[1 1 1 1 1 1 1 1]\n",
      "error pos shape 8\n",
      "error index\n",
      "[ 290  415  471 1511 1589 1940 1941 4155]\n",
      "s_433337\n",
      "\n",
      "43\n",
      "[2, 3, 4, 5, 6, 7, 9, 13, 14, 33, 42, 48, 95, 213, 326, 433]\n",
      "Fitting 1 classifiers...\n",
      "Fitting classifier1: randomforestclassifier (1/1)\n",
      "Training and fitting fold 1 of 3...\n",
      "Training and fitting fold 2 of 3...\n",
      "Training and fitting fold 3 of 3...\n",
      "origin 0.99932735426\n",
      "error shape, 3\n",
      "[1 1 1]\n",
      "error pos shape 3\n",
      "error index\n",
      "[2355 3052 3247]\n",
      "s_490562\n",
      "\n",
      "44\n",
      "[59, 67, 72, 78, 80, 109, 136, 143, 154, 155, 239, 249, 253, 275, 301, 319, 336, 374, 400, 458, 569]\n",
      "Fitting 1 classifiers...\n",
      "Fitting classifier1: randomforestclassifier (1/1)\n",
      "Training and fitting fold 1 of 3...\n",
      "Training and fitting fold 2 of 3...\n",
      "Training and fitting fold 3 of 3...\n",
      "origin 0.999775784753\n",
      "error shape, 1\n",
      "[1]\n",
      "error pos shape 1\n",
      "error index\n",
      "[4021]\n",
      "s_490980\n",
      "\n",
      "45\n",
      "[59, 67, 72, 78, 80, 136, 143, 154, 155, 205, 239, 249, 275, 319, 356, 357, 367, 374, 397, 406, 415, 431, 445, 451, 465, 501]\n",
      "Fitting 1 classifiers...\n",
      "Fitting classifier1: randomforestclassifier (1/1)\n",
      "Training and fitting fold 1 of 3...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training and fitting fold 2 of 3...\n",
      "Training and fitting fold 3 of 3...\n",
      "origin 0.99730941704\n",
      "error shape, 12\n",
      "[0 1 1 1 0 0 0 1 0 0 1 1]\n",
      "error pos shape 6\n",
      "error index\n",
      "[ 311 1173 1174 1176 1279 2131 2646 2933 3200 3600 4054 4061]\n",
      "s_491277\n",
      "\n",
      "46\n",
      "[88, 92, 98, 101, 103, 116, 122, 233, 236, 254, 256, 295, 300, 330, 345, 349, 352, 383, 404, 422, 500, 556, 643, 696, 738]\n",
      "Fitting 1 classifiers...\n",
      "Fitting classifier1: randomforestclassifier (1/1)\n",
      "Training and fitting fold 1 of 3...\n",
      "Training and fitting fold 2 of 3...\n",
      "Training and fitting fold 3 of 3...\n",
      "origin 1.0\n",
      "error shape, 0\n",
      "[]\n",
      "error pos shape 0\n",
      "error index\n",
      "[]\n",
      "s_493201\n",
      "\n",
      "47\n",
      "[59, 67, 72, 80, 143, 155, 179, 235, 239, 253, 277, 283, 301, 336, 358, 429, 458, 874, 2263, 2547]\n",
      "Fitting 1 classifiers...\n",
      "Fitting classifier1: randomforestclassifier (1/1)\n",
      "Training and fitting fold 1 of 3...\n",
      "Training and fitting fold 2 of 3...\n",
      "Training and fitting fold 3 of 3...\n",
      "origin 1.0\n",
      "error shape, 0\n",
      "[]\n",
      "error pos shape 0\n",
      "error index\n",
      "[]\n",
      "s_496413\n",
      "\n",
      "48\n",
      "[2, 3, 4, 5, 7, 11, 13, 14, 16, 33, 38, 42, 82, 95, 99, 102, 118, 204]\n",
      "Fitting 1 classifiers...\n",
      "Fitting classifier1: randomforestclassifier (1/1)\n",
      "Training and fitting fold 1 of 3...\n",
      "Training and fitting fold 2 of 3...\n",
      "Training and fitting fold 3 of 3...\n",
      "origin 0.99932735426\n",
      "error shape, 3\n",
      "[1 1 1]\n",
      "error pos shape 3\n",
      "error index\n",
      "[ 343 3267 3712]\n",
      "s_497803\n",
      "\n",
      "49\n",
      "[2, 3, 4, 6, 7, 13, 21, 27, 29, 37, 49, 54, 60, 61, 71, 91, 94, 125, 165, 232, 394, 432]\n",
      "Fitting 1 classifiers...\n",
      "Fitting classifier1: randomforestclassifier (1/1)\n",
      "Training and fitting fold 1 of 3...\n",
      "Training and fitting fold 2 of 3...\n",
      "Training and fitting fold 3 of 3...\n",
      "origin 0.997533632287\n",
      "error shape, 11\n",
      "[0 1 0 0 1 1 1 1 1 0 1]\n",
      "error pos shape 7\n",
      "error index\n",
      "[  18   79  466  899 1399 1671 1672 3028 3029 3251 3253]\n",
      "s_501724\n",
      "\n",
      "50\n",
      "[15, 45, 47, 136, 186, 218, 251, 284, 318, 328, 338, 342, 344, 363, 391, 444]\n",
      "Fitting 1 classifiers...\n",
      "Fitting classifier1: randomforestclassifier (1/1)\n",
      "Training and fitting fold 1 of 3...\n",
      "Training and fitting fold 2 of 3...\n",
      "Training and fitting fold 3 of 3...\n",
      "origin 0.999551569507\n",
      "error shape, 2\n",
      "[1 0]\n",
      "error pos shape 1\n",
      "error index\n",
      "[ 867 3011]\n",
      "s_504009\n",
      "\n",
      "51\n",
      "[4, 5, 11, 13, 16, 18, 22, 25, 33, 35, 38, 43, 82, 99, 102, 105, 118, 194, 204]\n",
      "Fitting 1 classifiers...\n",
      "Fitting classifier1: randomforestclassifier (1/1)\n",
      "Training and fitting fold 1 of 3...\n",
      "Training and fitting fold 2 of 3...\n",
      "Training and fitting fold 3 of 3...\n",
      "origin 0.997757847534\n",
      "error shape, 10\n",
      "[1 1 1 1 1 1 1 1 1 1]\n",
      "error pos shape 10\n",
      "error index\n",
      "[ 145  146  151  165 1758 2159 2259 3960 4067 4068]\n",
      "s_506583\n",
      "\n",
      "52\n",
      "[2, 3, 4, 5, 6, 7, 9, 13, 14, 21, 27, 29, 37, 42, 48, 49, 60, 61, 71, 91, 232, 273]\n",
      "Fitting 1 classifiers...\n",
      "Fitting classifier1: randomforestclassifier (1/1)\n",
      "Training and fitting fold 1 of 3...\n",
      "Training and fitting fold 2 of 3...\n",
      "Training and fitting fold 3 of 3...\n",
      "origin 0.991479820628\n",
      "error shape, 38\n",
      "[1 1 1 0 1 1 1 0 0 1 1 0 1 1 1 0 1 0 0 0 1 1 1 1 0 0 0 0 1 1 1 0 1 0 1 0 1\n",
      " 0]\n",
      "error pos shape 22\n",
      "error index\n",
      "[  17  122  279  412  492  846 1099 1171 1272 1398 1566 1693 1752 1770 1826\n",
      " 1846 1885 1979 2048 2270 2403 2472 2474 2484 2552 2618 2717 2965 2973 3026\n",
      " 3157 3328 3413 3506 3689 4078 4305 4378]\n",
      "s_506597\n",
      "\n",
      "53\n",
      "[15, 24, 32, 41, 45, 50, 52, 53, 64, 66, 88, 92, 101, 217, 223, 272, 280, 294, 303, 347, 425]\n",
      "Fitting 1 classifiers...\n",
      "Fitting classifier1: randomforestclassifier (1/1)\n",
      "Training and fitting fold 1 of 3...\n",
      "Training and fitting fold 2 of 3...\n",
      "Training and fitting fold 3 of 3...\n",
      "origin 0.9966367713\n",
      "error shape, 15\n",
      "[1 1 0 1 1 0 1 1 0 0 1 1 0 1 0]\n",
      "error pos shape 9\n",
      "error index\n",
      "[ 504  636  868  993 1145 1918 1983 2784 2853 3261 3620 3809 4083 4093 4319]\n",
      "s_506646\n",
      "\n",
      "54\n",
      "[15, 24, 32, 41, 45, 50, 52, 53, 64, 66, 88, 92, 146, 153, 217, 221, 229, 245, 290, 308]\n",
      "Fitting 1 classifiers...\n",
      "Fitting classifier1: randomforestclassifier (1/1)\n",
      "Training and fitting fold 1 of 3...\n",
      "Training and fitting fold 2 of 3...\n",
      "Training and fitting fold 3 of 3...\n",
      "origin 0.999775784753\n",
      "error shape, 1\n",
      "[1]\n",
      "error pos shape 1\n",
      "error index\n",
      "[3927]\n",
      "s_507000\n",
      "\n",
      "55\n",
      "[2, 3, 4, 5, 6, 7, 9, 13, 14, 21, 27, 29, 33, 37, 42, 48, 49, 60, 71, 77, 86, 95, 193, 220]\n",
      "Fitting 1 classifiers...\n",
      "Fitting classifier1: randomforestclassifier (1/1)\n",
      "Training and fitting fold 1 of 3...\n",
      "Training and fitting fold 2 of 3...\n",
      "Training and fitting fold 3 of 3...\n",
      "origin 0.986771300448\n",
      "error shape, 59\n",
      "[1 1 0 1 1 0 1 1 1 1 1 0 0 1 1 0 0 1 1 1 0 1 1 0 1 0 1 1 1 1 1 1 1 0 0 1 1\n",
      " 1 1 0 0 0 1 1 1 1 1 0 0 1 1 1 1 1 1 1 1 1 1]\n",
      "error pos shape 43\n",
      "error index\n",
      "[ 244  250  279  431  432  474  587  588  590  591  630  753  781  958 1033\n",
      " 1097 1099 1230 1276 1395 1566 1693 1695 1752 1880 1885 1958 1979 2248 2267\n",
      " 2297 2354 2393 2403 2474 2623 2712 2824 2965 2973 3026 3027 3112 3115 3373\n",
      " 3378 3431 3534 3689 3849 4079 4108 4111 4113 4114 4115 4153 4192 4263]\n",
      "s_508287\n",
      "\n",
      "56\n",
      "[6, 54, 59, 67, 72, 78, 80, 109, 120, 121, 138, 140, 156, 239, 264, 301]\n",
      "Fitting 1 classifiers...\n",
      "Fitting classifier1: randomforestclassifier (1/1)\n",
      "Training and fitting fold 1 of 3...\n",
      "Training and fitting fold 2 of 3...\n",
      "Training and fitting fold 3 of 3...\n",
      "origin 0.99932735426\n",
      "error shape, 3\n",
      "[1 1 1]\n",
      "error pos shape 3\n",
      "error index\n",
      "[1156 1909 3805]\n",
      "s_509630\n",
      "\n",
      "57\n",
      "[15, 24, 45, 47, 52, 70, 131, 151, 164, 186, 187, 216, 243, 247, 472, 491, 549, 629, 631, 649]\n",
      "Fitting 1 classifiers...\n",
      "Fitting classifier1: randomforestclassifier (1/1)\n",
      "Training and fitting fold 1 of 3...\n",
      "Training and fitting fold 2 of 3...\n",
      "Training and fitting fold 3 of 3...\n",
      "origin 0.998878923767\n",
      "error shape, 5\n",
      "[1 0 0 1 1]\n",
      "error pos shape 3\n",
      "error index\n",
      "[ 458  939 1287 2620 3405]\n",
      "s_510225\n",
      "\n",
      "58\n",
      "[44, 51, 75, 76, 81, 85, 89, 98, 103, 115, 116, 122, 145, 198, 226, 254, 332, 360]\n",
      "Fitting 1 classifiers...\n",
      "Fitting classifier1: randomforestclassifier (1/1)\n",
      "Training and fitting fold 1 of 3...\n",
      "Training and fitting fold 2 of 3...\n",
      "Training and fitting fold 3 of 3...\n",
      "origin 0.998430493274\n",
      "error shape, 7\n",
      "[1 1 1 1 1 1 1]\n",
      "error pos shape 7\n",
      "error index\n",
      "[ 306  309 1684 1696 1897 3318 3319]\n",
      "s_510334\n",
      "\n",
      "59\n",
      "[2, 6, 27, 29, 37, 54, 59, 61, 67, 72, 78, 91, 109, 120, 121, 125, 138, 140, 156, 165, 264, 457]\n",
      "Fitting 1 classifiers...\n",
      "Fitting classifier1: randomforestclassifier (1/1)\n",
      "Training and fitting fold 1 of 3...\n",
      "Training and fitting fold 2 of 3...\n",
      "Training and fitting fold 3 of 3...\n",
      "origin 0.99932735426\n",
      "error shape, 3\n",
      "[1 1 0]\n",
      "error pos shape 2\n",
      "error index\n",
      "[ 386  388 1033]\n",
      "s_512700\n",
      "\n",
      "60\n",
      "[2, 3, 4, 6, 7, 13, 21, 27, 29, 37, 49, 54, 61, 91, 120, 121, 125, 140, 165, 605]\n",
      "Fitting 1 classifiers...\n",
      "Fitting classifier1: randomforestclassifier (1/1)\n",
      "Training and fitting fold 1 of 3...\n",
      "Training and fitting fold 2 of 3...\n",
      "Training and fitting fold 3 of 3...\n",
      "origin 0.99932735426\n",
      "error shape, 3\n",
      "[0 0 1]\n",
      "error pos shape 1\n",
      "error index\n",
      "[ 587  590 4235]\n",
      "s_514913\n",
      "\n",
      "61\n",
      "[2, 3, 4, 6, 7, 9, 13, 21, 27, 29, 37, 49, 54, 60, 61, 71, 79, 91, 94, 120, 125, 165, 232]\n",
      "Fitting 1 classifiers...\n",
      "Fitting classifier1: randomforestclassifier (1/1)\n",
      "Training and fitting fold 1 of 3...\n",
      "Training and fitting fold 2 of 3...\n",
      "Training and fitting fold 3 of 3...\n",
      "origin 0.995964125561\n",
      "error shape, 18\n",
      "[1 0 0 1 0 1 1 0 1 0 1 1 1 0 0 0 1 1]\n",
      "error pos shape 10\n",
      "error index\n",
      "[  18   35  354  466  468  795  917 1513 2122 2964 3025 3027 3254 3628 3860\n",
      " 3923 4027 4149]\n",
      "s_515010\n",
      "\n",
      "62\n",
      "[59, 67, 72, 78, 80, 136, 154, 155, 205, 276, 291, 312, 319, 323, 354, 379, 397, 400, 498, 513]\n",
      "Fitting 1 classifiers...\n",
      "Fitting classifier1: randomforestclassifier (1/1)\n",
      "Training and fitting fold 1 of 3...\n",
      "Training and fitting fold 2 of 3...\n",
      "Training and fitting fold 3 of 3...\n",
      "origin 1.0\n",
      "error shape, 0\n",
      "[]\n",
      "error pos shape 0\n",
      "error index\n",
      "[]\n",
      "s_515753\n",
      "\n",
      "63\n",
      "[67, 72, 78, 80, 129, 143, 154, 155, 176, 235, 241, 253, 263, 270, 277, 281, 283, 336, 358, 429, 458]\n",
      "Fitting 1 classifiers...\n",
      "Fitting classifier1: randomforestclassifier (1/1)\n",
      "Training and fitting fold 1 of 3...\n",
      "Training and fitting fold 2 of 3...\n",
      "Training and fitting fold 3 of 3...\n",
      "origin 0.999103139013\n",
      "error shape, 4\n",
      "[0 0 0 1]\n",
      "error pos shape 1\n",
      "error index\n",
      "[ 588  589 2103 2701]\n",
      "s_517510\n",
      "\n",
      "64\n",
      "[11, 16, 18, 22, 25, 30, 35, 38, 39, 43, 46, 69, 73, 81, 85, 87, 105, 172, 181, 194]\n",
      "Fitting 1 classifiers...\n",
      "Fitting classifier1: randomforestclassifier (1/1)\n",
      "Training and fitting fold 1 of 3...\n",
      "Training and fitting fold 2 of 3...\n",
      "Training and fitting fold 3 of 3...\n",
      "origin 0.997757847534\n",
      "error shape, 10\n",
      "[1 1 1 1 1 1 1 0 0 1]\n",
      "error pos shape 8\n",
      "error index\n",
      "[ 692  693 2633 3158 3926 3971 3972 4128 4129 4397]\n",
      "s_517764\n",
      "\n",
      "65\n",
      "[59, 67, 72, 78, 80, 136, 143, 154, 155, 239, 241, 249, 253, 270, 281, 283, 336, 354, 429]\n",
      "Fitting 1 classifiers...\n",
      "Fitting classifier1: randomforestclassifier (1/1)\n",
      "Training and fitting fold 1 of 3...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training and fitting fold 2 of 3...\n",
      "Training and fitting fold 3 of 3...\n",
      "origin 0.999551569507\n",
      "error shape, 2\n",
      "[1 1]\n",
      "error pos shape 2\n",
      "error index\n",
      "[627 629]\n",
      "s_517795\n",
      "\n",
      "66\n",
      "[93, 97, 100, 124, 127, 141, 142, 144, 149, 162, 170, 190, 210, 313, 370, 389, 428, 443]\n",
      "Fitting 1 classifiers...\n",
      "Fitting classifier1: randomforestclassifier (1/1)\n",
      "Training and fitting fold 1 of 3...\n",
      "Training and fitting fold 2 of 3...\n",
      "Training and fitting fold 3 of 3...\n",
      "origin 0.999103139013\n",
      "error shape, 4\n",
      "[0 1 1 1]\n",
      "error pos shape 3\n",
      "error index\n",
      "[ 860 2616 3778 3779]\n",
      "s_522897\n",
      "\n",
      "67\n",
      "[15, 24, 32, 41, 45, 47, 50, 52, 64, 66, 70, 131, 146, 151, 153, 164, 186, 187, 209, 216, 243, 247, 307, 348]\n",
      "Fitting 1 classifiers...\n",
      "Fitting classifier1: randomforestclassifier (1/1)\n",
      "Training and fitting fold 1 of 3...\n",
      "Training and fitting fold 2 of 3...\n",
      "Training and fitting fold 3 of 3...\n",
      "origin 0.99865470852\n",
      "error shape, 6\n",
      "[1 1 1 1 1 1]\n",
      "error pos shape 6\n",
      "error index\n",
      "[ 723 1841 1842 2846 3011 3262]\n",
      "s_525418\n",
      "\n",
      "68\n",
      "[30, 39, 44, 51, 68, 75, 76, 84, 88, 89, 92, 98, 103, 111, 122, 145, 198, 226, 233, 254, 256, 292, 295, 332, 350, 360]\n",
      "Fitting 1 classifiers...\n",
      "Fitting classifier1: randomforestclassifier (1/1)\n",
      "Training and fitting fold 1 of 3...\n",
      "Training and fitting fold 2 of 3...\n",
      "Training and fitting fold 3 of 3...\n",
      "origin 1.0\n",
      "error shape, 0\n",
      "[]\n",
      "error pos shape 0\n",
      "error index\n",
      "[]\n",
      "s_525779\n",
      "\n",
      "69\n",
      "[15, 45, 47, 70, 136, 186, 218, 247, 251, 284, 319, 328, 338, 342, 344, 391, 573, 595, 608, 912, 999, 2282]\n",
      "Fitting 1 classifiers...\n",
      "Fitting classifier1: randomforestclassifier (1/1)\n",
      "Training and fitting fold 1 of 3...\n",
      "Training and fitting fold 2 of 3...\n",
      "Training and fitting fold 3 of 3...\n",
      "origin 1.0\n",
      "error shape, 0\n",
      "[]\n",
      "error pos shape 0\n",
      "error index\n",
      "[]\n",
      "s_528553\n",
      "\n",
      "70\n",
      "[16, 30, 39, 68, 73, 103, 107, 126, 166, 173, 177, 198, 222, 268, 297, 315, 353, 378]\n",
      "Fitting 1 classifiers...\n",
      "Fitting classifier1: randomforestclassifier (1/1)\n",
      "Training and fitting fold 1 of 3...\n",
      "Training and fitting fold 2 of 3...\n",
      "Training and fitting fold 3 of 3...\n",
      "origin 1.0\n",
      "error shape, 0\n",
      "[]\n",
      "error pos shape 0\n",
      "error index\n",
      "[]\n",
      "s_540782\n",
      "\n",
      "71\n",
      "[39, 44, 51, 81, 88, 93, 100, 103, 142, 162, 198, 254, 256, 313, 361, 417, 443, 454, 469]\n",
      "Fitting 1 classifiers...\n",
      "Fitting classifier1: randomforestclassifier (1/1)\n",
      "Training and fitting fold 1 of 3...\n",
      "Training and fitting fold 2 of 3...\n",
      "Training and fitting fold 3 of 3...\n",
      "origin 1.0\n",
      "error shape, 0\n",
      "[]\n",
      "error pos shape 0\n",
      "error index\n",
      "[]\n",
      "s_542833\n",
      "\n",
      "72\n",
      "[228, 271, 286, 330, 335, 366, 381, 386, 402, 409, 416, 437, 459, 460, 504, 518, 522, 530, 555, 624, 647, 672]\n",
      "Fitting 1 classifiers...\n",
      "Fitting classifier1: randomforestclassifier (1/1)\n",
      "Training and fitting fold 1 of 3...\n",
      "Training and fitting fold 2 of 3...\n",
      "Training and fitting fold 3 of 3...\n",
      "origin 0.999103139013\n",
      "error shape, 4\n",
      "[0 1 1 0]\n",
      "error pos shape 2\n",
      "error index\n",
      "[ 367  398 1053 3209]\n",
      "s_570869\n",
      "\n",
      "73\n",
      "[5, 11, 16, 18, 22, 25, 30, 35, 38, 43, 46, 69, 73, 82, 87, 515]\n",
      "Fitting 1 classifiers...\n",
      "Fitting classifier1: randomforestclassifier (1/1)\n",
      "Training and fitting fold 1 of 3...\n",
      "Training and fitting fold 2 of 3...\n",
      "Training and fitting fold 3 of 3...\n",
      "origin 0.999103139013\n",
      "error shape, 4\n",
      "[1 1 1 1]\n",
      "error pos shape 4\n",
      "error index\n",
      "[2287 2289 2490 4434]\n",
      "s_572085\n",
      "\n",
      "74\n",
      "[158, 178, 179, 196, 201, 214, 225, 227, 250, 261, 262, 288, 299, 380, 382, 388, 395, 398, 399, 413, 446, 475, 483, 494]\n",
      "Fitting 1 classifiers...\n",
      "Fitting classifier1: randomforestclassifier (1/1)\n",
      "Training and fitting fold 1 of 3...\n",
      "Training and fitting fold 2 of 3...\n",
      "Training and fitting fold 3 of 3...\n",
      "origin 0.999775784753\n",
      "error shape, 1\n",
      "[1]\n",
      "error pos shape 1\n",
      "error index\n",
      "[902]\n",
      "s_621251\n",
      "\n",
      "75\n",
      "[16, 30, 39, 46, 51, 68, 73, 84, 111, 122, 126, 145, 166, 177, 199, 203, 314, 353, 371]\n",
      "Fitting 1 classifiers...\n",
      "Fitting classifier1: randomforestclassifier (1/1)\n",
      "Training and fitting fold 1 of 3...\n",
      "Training and fitting fold 2 of 3...\n",
      "Training and fitting fold 3 of 3...\n",
      "origin 1.0\n",
      "error shape, 0\n",
      "[]\n",
      "error pos shape 0\n",
      "error index\n",
      "[]\n",
      "s_627691\n",
      "\n",
      "76\n",
      "[15, 24, 32, 41, 50, 53, 64, 66, 88, 92, 98, 101, 217, 223, 236, 280, 294, 298, 340, 346, 375]\n",
      "Fitting 1 classifiers...\n",
      "Fitting classifier1: randomforestclassifier (1/1)\n",
      "Training and fitting fold 1 of 3...\n",
      "Training and fitting fold 2 of 3...\n",
      "Training and fitting fold 3 of 3...\n",
      "origin 0.998878923767\n",
      "error shape, 5\n",
      "[0 0 0 0 1]\n",
      "error pos shape 1\n",
      "error index\n",
      "[ 504  993 1142 1983 3261]\n",
      "s_643862\n",
      "\n",
      "77\n",
      "[15, 24, 32, 45, 47, 52, 70, 114, 131, 151, 163, 186, 187, 218, 234, 243, 296, 304, 328, 392]\n",
      "Fitting 1 classifiers...\n",
      "Fitting classifier1: randomforestclassifier (1/1)\n",
      "Training and fitting fold 1 of 3...\n",
      "Training and fitting fold 2 of 3...\n",
      "Training and fitting fold 3 of 3...\n",
      "origin 0.999551569507\n",
      "error shape, 2\n",
      "[1 1]\n",
      "error pos shape 2\n",
      "error index\n",
      "[  72 2064]\n",
      "s_792077\n",
      "\n",
      "78\n",
      "[20, 23, 31, 34, 55, 57, 62, 63, 90, 104, 113, 134, 137, 240, 260, 287, 423]\n",
      "Fitting 1 classifiers...\n",
      "Fitting classifier1: randomforestclassifier (1/1)\n",
      "Training and fitting fold 1 of 3...\n",
      "Training and fitting fold 2 of 3...\n",
      "Training and fitting fold 3 of 3...\n",
      "origin 0.99932735426\n",
      "error shape, 3\n",
      "[1 0 0]\n",
      "error pos shape 1\n",
      "error index\n",
      "[1978 3627 4453]\n",
      "s_819792\n",
      "\n",
      "79\n",
      "[110, 129, 157, 158, 159, 176, 179, 183, 185, 191, 200, 201, 205, 215, 227, 259, 263, 281, 288, 316, 320, 372, 461]\n",
      "Fitting 1 classifiers...\n",
      "Fitting classifier1: randomforestclassifier (1/1)\n",
      "Training and fitting fold 1 of 3...\n",
      "Training and fitting fold 2 of 3...\n",
      "Training and fitting fold 3 of 3...\n",
      "origin 0.99798206278\n",
      "error shape, 9\n",
      "[1 1 1 1 1 1 1 1 1]\n",
      "error pos shape 9\n",
      "error index\n",
      "[  70 1036 1037 2102 2246 2632 3250 3300 4351]\n",
      "s_856830\n",
      "\n",
      "{0: 0.9995515695067264, 1: 0.9995515695067264, 2: 1.0, 3: 1.0, 4: 1.0, 5: 0.9984304932735426, 6: 0.9961883408071749, 7: 0.9991031390134529, 8: 0.9988789237668162, 9: 1.0, 10: 1.0, 11: 0.9997757847533633, 12: 1.0, 13: 1.0, 14: 0.9966367713004485, 15: 1.0, 16: 1.0, 17: 0.9997757847533633, 18: 1.0, 19: 1.0, 20: 1.0, 21: 1.0, 22: 0.9995515695067264, 23: 1.0, 24: 0.9997757847533633, 25: 1.0, 26: 0.9926008968609865, 27: 0.9995515695067264, 28: 0.9993273542600897, 29: 0.9995515695067264, 30: 1.0, 31: 1.0, 32: 0.9988789237668162, 33: 0.9952914798206278, 34: 0.9997757847533633, 35: 0.9993273542600897, 36: 0, 37: 1.0, 38: 1.0, 39: 1.0, 40: 0.9997757847533633, 41: 0.9865470852017937, 42: 0.9982062780269059, 43: 0.9993273542600897, 44: 0.9997757847533633, 45: 0.9973094170403587, 46: 1.0, 47: 1.0, 48: 0.9993273542600897, 49: 0.9975336322869955, 50: 0.9995515695067264, 51: 0.9977578475336323, 52: 0.9914798206278027, 53: 0.9966367713004485, 54: 0.9997757847533633, 55: 0.9867713004484305, 56: 0.9993273542600897, 57: 0.9988789237668162, 58: 0.9984304932735426, 59: 0.9993273542600897, 60: 0.9993273542600897, 61: 0.9959641255605381, 62: 1.0, 63: 0.9991031390134529, 64: 0.9977578475336323, 65: 0.9995515695067264, 66: 0.9991031390134529, 67: 0.9986547085201793, 68: 1.0, 69: 1.0, 70: 1.0, 71: 1.0, 72: 0.9991031390134529, 73: 0.9991031390134529, 74: 0.9997757847533633, 75: 1.0, 76: 0.9988789237668162, 77: 0.9995515695067264, 78: 0.9993273542600897, 79: 0.997982062780269}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9482062780269058"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "r = {}\n",
    "probas = []\n",
    "for i in range(_train_b_y.shape[1]):\n",
    "#     i=26\n",
    "    print i\n",
    "    gt = train.iloc[train_y==lb.classes_[i]]\n",
    "    ngt = train.iloc[train_y!=lb.classes_[i]]\n",
    "    if len(gt) != 0:\n",
    "        \n",
    "        sorted_wifi = get_sorted_wifi([gt])\n",
    "        _indexs = get_indexs(df,sorted_wifi,0.1)\n",
    "       \n",
    "        cv = 3\n",
    "        n_sorted_wifi = get_sorted_wifi([ngt])\n",
    "        _nindexs = get_indexs2(df,n_sorted_wifi,50)\n",
    "#         _indexs = list(set(_indexs).union(set(_nindexs)))\n",
    "        prf = get_model(cv = cv)\n",
    "        otxs= []\n",
    "        ovxs = []\n",
    "#         _indexs = _indexs[:1]\n",
    "        print _indexs\n",
    "        modify_size =  (np.asarray(_indexs) < 0).sum()\n",
    "        _tx = train_wifi_all_x[:,_indexs]\n",
    "        _vx = valid_wifi_all_x[:,_indexs]\n",
    "        \n",
    "        __tx,__vx = modify_wifi(_tx,_vx,train,valid,modify_size)\n",
    "        _tx = np.concatenate([_tx,__tx],axis=1)\n",
    "        _vx = np.concatenate([_vx,__vx],axis=1)\n",
    "        \n",
    "        \n",
    "        _tx = np.concatenate([_tx,train_lonlats,train_wh],axis=1)\n",
    "        _vx = np.concatenate([_vx,valid_lonlats,valid_wh],axis=1)\n",
    "\n",
    "        if cv is not None and cv != 0:\n",
    "            _tx,_ty = expansion(_tx,_train_b_y[:,i],cv)\n",
    "        else:\n",
    "            _ty = _train_b_y[:,i]\n",
    "            \n",
    "        prf.fit(_tx,_ty)\n",
    "        p = prf.predict(_vx)\n",
    "        proba = prf.predict_proba(_vx)\n",
    "        probas.append(proba[:,1])\n",
    "        _acc = acc(p,_valid_b_y[:,i])\n",
    "        print \"origin\", _acc\n",
    "        r[i] = _acc\n",
    "        print \"error shape,\", (p != _valid_b_y[:,i]).sum()\n",
    "        print _valid_b_y[:,i][(p != _valid_b_y[:,i])]\n",
    "        print \"error pos shape\", (_valid_b_y[:,i][(p != _valid_b_y[:,i])]==1).sum()\n",
    "        print \"error index\"\n",
    "        print valid_index[(p != _valid_b_y[:,i])]\n",
    "#         print \"correct index\"\n",
    "#         print valid_index[(p == _valid_b_y[:,i]) & (_valid_b_y[:,i]==1)]\n",
    "        print le.classes_[i]\n",
    "        print\n",
    "    else:\n",
    "        probas.append(np.zeros((valid_y.shape[0],)))\n",
    "        r[i] = 0\n",
    "#     break\n",
    "print r\n",
    "acc(lb.classes_.take(np.argmax(np.vstack(probas).T,axis=1)), valid_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "[2, 3, 4, 5, 6, 7, 9, 13, 21, 27, 29, 37, 48, 49, 60, 61, 71, 79, 94]\n",
      "Fitting 1 classifiers...\n",
      "Fitting classifier1: randomforestclassifier (1/1)\n",
      "Training and fitting fold 1 of 3...\n",
      "Training and fitting fold 2 of 3...\n",
      "Training and fitting fold 3 of 3...\n",
      "origin 0.99932735426\n",
      "error shape, 3\n",
      "[1 1 1]\n",
      "error pos shape 3\n",
      "error index\n",
      "[1930 1931 1932]\n",
      "s_1346456\n",
      "\n",
      "1\n",
      "[24, 32, 41, 50, 53, 64, 66, 88, 92, 98, 101, 223, 236, 280, 292, 300, 334, 337, 387, 484, 487, 488, 523]\n",
      "Fitting 1 classifiers...\n",
      "Fitting classifier1: randomforestclassifier (1/1)\n",
      "Training and fitting fold 1 of 3...\n",
      "Training and fitting fold 2 of 3...\n",
      "Training and fitting fold 3 of 3...\n",
      "origin 0.999551569507\n",
      "error shape, 2\n",
      "[1 1]\n",
      "error pos shape 2\n",
      "error index\n",
      "[1193 1551]\n",
      "s_1392063\n",
      "\n",
      "2\n",
      "[23, 26, 34, 65, 123, 133, 169, 171, 230, 255, 302, 373, 405, 408, 540, 774, 776, 783, 800, 801, 810, 837, 924, 932, 974, 1304, 1584, 1969]\n",
      "Fitting 1 classifiers...\n",
      "Fitting classifier1: randomforestclassifier (1/1)\n",
      "Training and fitting fold 1 of 3...\n",
      "Training and fitting fold 2 of 3...\n",
      "Training and fitting fold 3 of 3...\n",
      "origin 1.0\n",
      "error shape, 0\n",
      "[]\n",
      "error pos shape 0\n",
      "error index\n",
      "[]\n",
      "s_1452940\n",
      "\n",
      "3\n",
      "[56, 267, 369, 438, 521, 531, 611, 619, 681, 687, 705, 718, 848, 1004, 1744, 4479]\n",
      "Fitting 1 classifiers...\n",
      "Fitting classifier1: randomforestclassifier (1/1)\n",
      "Training and fitting fold 1 of 3...\n",
      "Training and fitting fold 2 of 3...\n",
      "Training and fitting fold 3 of 3...\n",
      "origin 1.0\n",
      "error shape, 0\n",
      "[]\n",
      "error pos shape 0\n",
      "error index\n",
      "[]\n",
      "s_1671756\n",
      "\n",
      "4\n",
      "[258, 376, 390, 424, 427, 921]\n",
      "Fitting 1 classifiers...\n",
      "Fitting classifier1: randomforestclassifier (1/1)\n",
      "Training and fitting fold 1 of 3...\n",
      "Training and fitting fold 2 of 3...\n",
      "Training and fitting fold 3 of 3...\n",
      "origin 1.0\n",
      "error shape, 0\n",
      "[]\n",
      "error pos shape 0\n",
      "error index\n",
      "[]\n",
      "s_1882105\n",
      "\n",
      "5\n",
      "[20, 31, 34, 55, 57, 58, 62, 63, 117, 119, 139, 152, 244, 260, 282, 412, 423]\n",
      "Fitting 1 classifiers...\n",
      "Fitting classifier1: randomforestclassifier (1/1)\n",
      "Training and fitting fold 1 of 3...\n",
      "Training and fitting fold 2 of 3...\n",
      "Training and fitting fold 3 of 3...\n",
      "origin 0.998206278027\n",
      "error shape, 8\n",
      "[0 0 0 0 0 0 0 0]\n",
      "error pos shape 0\n",
      "error index\n",
      "[ 668 1468 1784 1810 2522 3047 3234 3741]\n",
      "s_1944816\n",
      "\n",
      "6\n",
      "[58, 96, 106, 128, 130, 150, 184, 248, 258, 310, 322, 355, 384, 410, 426]\n",
      "Fitting 1 classifiers...\n",
      "Fitting classifier1: randomforestclassifier (1/1)\n",
      "Training and fitting fold 1 of 3...\n",
      "Training and fitting fold 2 of 3...\n",
      "Training and fitting fold 3 of 3...\n",
      "origin 0.996188340807\n",
      "error shape, 17\n",
      "[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "error pos shape 17\n",
      "error index\n",
      "[ 750  939 1287 1303 1444 1667 2171 2316 2548 2792 3934 4406 4425 4426 4427\n",
      " 4428 4429]\n",
      "s_1997293\n",
      "\n",
      "7\n",
      "[58, 83, 160, 192, 197, 206, 207, 211, 219, 224, 237, 331, 385, 526, 570, 628]\n",
      "Fitting 1 classifiers...\n",
      "Fitting classifier1: randomforestclassifier (1/1)\n",
      "Training and fitting fold 1 of 3...\n",
      "Training and fitting fold 2 of 3...\n",
      "Training and fitting fold 3 of 3...\n",
      "origin 0.999103139013\n",
      "error shape, 4\n",
      "[0 1 1 1]\n",
      "error pos shape 3\n",
      "error index\n",
      "[ 408 1143 2503 3862]\n",
      "s_2257337\n",
      "\n",
      "8\n",
      "[2, 3, 4, 6, 7, 13, 21, 27, 29, 37, 49, 60, 61, 71, 79, 91, 94]\n",
      "Fitting 1 classifiers...\n",
      "Fitting classifier1: randomforestclassifier (1/1)\n",
      "Training and fitting fold 1 of 3...\n",
      "Training and fitting fold 2 of 3...\n",
      "Training and fitting fold 3 of 3...\n",
      "origin 0.998878923767\n",
      "error shape, 5\n",
      "[1 1 1 1 1]\n",
      "error pos shape 5\n",
      "error index\n",
      "[1572 1945 2050 4151 4152]\n",
      "s_2423948\n",
      "\n",
      "9\n",
      "[269, 424, 427, 3756]\n",
      "Fitting 1 classifiers...\n",
      "Fitting classifier1: randomforestclassifier (1/1)\n",
      "Training and fitting fold 1 of 3...\n",
      "Training and fitting fold 2 of 3...\n",
      "Training and fitting fold 3 of 3...\n",
      "origin 1.0\n",
      "error shape, 0\n",
      "[]\n",
      "error pos shape 0\n",
      "error index\n",
      "[]\n",
      "s_2631847\n",
      "\n",
      "10\n",
      "[10, 112, 168, 175]\n",
      "Fitting 1 classifiers...\n",
      "Fitting classifier1: randomforestclassifier (1/1)\n",
      "Training and fitting fold 1 of 3...\n",
      "Training and fitting fold 2 of 3...\n",
      "Training and fitting fold 3 of 3...\n",
      "origin 1.0\n",
      "error shape, 0\n",
      "[]\n",
      "error pos shape 0\n",
      "error index\n",
      "[]\n",
      "s_2651838\n",
      "\n",
      "11\n",
      "[20, 31, 34, 55, 57, 62, 63, 90, 104, 113, 137, 311]\n",
      "Fitting 1 classifiers...\n",
      "Fitting classifier1: randomforestclassifier (1/1)\n",
      "Training and fitting fold 1 of 3...\n",
      "Training and fitting fold 2 of 3...\n",
      "Training and fitting fold 3 of 3...\n",
      "origin 0.999775784753\n",
      "error shape, 1\n",
      "[1]\n",
      "error pos shape 1\n",
      "error index\n",
      "[2936]\n",
      "s_2691361\n",
      "\n",
      "12\n",
      "[58, 128, 130, 150, 248, 258, 269, 322, 355, 368, 376, 403, 410, 532, 767]\n",
      "Fitting 1 classifiers...\n",
      "Fitting classifier1: randomforestclassifier (1/1)\n",
      "Training and fitting fold 1 of 3...\n",
      "Training and fitting fold 2 of 3...\n",
      "Training and fitting fold 3 of 3...\n",
      "origin 1.0\n",
      "error shape, 0\n",
      "[]\n",
      "error pos shape 0\n",
      "error index\n",
      "[]\n",
      "s_2741830\n",
      "\n",
      "13\n",
      "[58, 96, 106, 128, 134, 150, 184, 212, 248, 258, 267, 269, 322, 369, 376, 390, 407, 424, 427, 521, 531, 616, 718, 1392, 1630, 4588]\n",
      "Fitting 1 classifiers...\n",
      "Fitting classifier1: randomforestclassifier (1/1)\n",
      "Training and fitting fold 1 of 3...\n",
      "Training and fitting fold 2 of 3...\n",
      "Training and fitting fold 3 of 3...\n",
      "origin 1.0\n",
      "error shape, 0\n",
      "[]\n",
      "error pos shape 0\n",
      "error index\n",
      "[]\n",
      "s_2795244\n",
      "\n",
      "14\n",
      "[0, 1, 12, 17, 19, 28, 36, 56]\n",
      "Fitting 1 classifiers...\n",
      "Fitting classifier1: randomforestclassifier (1/1)\n",
      "Training and fitting fold 1 of 3...\n",
      "Training and fitting fold 2 of 3...\n",
      "Training and fitting fold 3 of 3...\n",
      "origin 0.995964125561\n",
      "error shape, 18\n",
      "[0 1 0 0 1 0 0 0 0 0 1 1 0 1 1 1 1 0]\n",
      "error pos shape 8\n",
      "error index\n",
      "[ 429  442  626  627  639 1320 2076 2248 2249 2251 2565 2566 3080 3097 3172\n",
      " 3461 3462 3934]\n",
      "s_2939231\n",
      "\n",
      "15\n",
      "[258, 267, 269, 322, 376, 390, 521, 531, 818, 916, 921, 2621, 3650]\n",
      "Fitting 1 classifiers...\n",
      "Fitting classifier1: randomforestclassifier (1/1)\n",
      "Training and fitting fold 1 of 3...\n",
      "Training and fitting fold 2 of 3...\n",
      "Training and fitting fold 3 of 3...\n",
      "origin 1.0\n",
      "error shape, 0\n",
      "[]\n",
      "error pos shape 0\n",
      "error index\n",
      "[]\n",
      "s_2956845\n",
      "\n",
      "16\n",
      "[852]\n",
      "Fitting 1 classifiers...\n",
      "Fitting classifier1: randomforestclassifier (1/1)\n",
      "Training and fitting fold 1 of 3...\n",
      "Training and fitting fold 2 of 3...\n",
      "Training and fitting fold 3 of 3...\n",
      "origin 1.0\n",
      "error shape, 0\n",
      "[]\n",
      "error pos shape 0\n",
      "error index\n",
      "[]\n",
      "s_3031981\n",
      "\n",
      "17\n",
      "[41, 53, 88, 92, 98, 101, 236, 292, 300, 352, 383, 387, 404, 414, 447, 467, 500, 686]\n",
      "Fitting 1 classifiers...\n",
      "Fitting classifier1: randomforestclassifier (1/1)\n",
      "Training and fitting fold 1 of 3...\n",
      "Training and fitting fold 2 of 3...\n",
      "Training and fitting fold 3 of 3...\n",
      "origin 0.999551569507\n",
      "error shape, 2\n",
      "[1 1]\n",
      "error pos shape 2\n",
      "error index\n",
      "[3683 3974]\n",
      "s_3036726\n",
      "\n",
      "18\n",
      "[3183, 3194, 3274, 3339, 3364, 3563, 3648, 3708, 3818, 3985, 3986, 4023, 4057, 4350, 4441, 4521, 4617]\n",
      "Fitting 1 classifiers...\n",
      "Fitting classifier1: randomforestclassifier (1/1)\n",
      "Training and fitting fold 1 of 3...\n",
      "Training and fitting fold 2 of 3...\n",
      "Training and fitting fold 3 of 3...\n",
      "origin 1.0\n",
      "error shape, 0\n",
      "[]\n",
      "error pos shape 0\n",
      "error index\n",
      "[]\n",
      "s_3081549\n",
      "\n",
      "19\n",
      "[20, 104, 240, 287, 478]\n",
      "Fitting 1 classifiers...\n",
      "Fitting classifier1: randomforestclassifier (1/1)\n",
      "Training and fitting fold 1 of 3...\n",
      "Training and fitting fold 2 of 3...\n",
      "Training and fitting fold 3 of 3...\n",
      "origin 1.0\n",
      "error shape, 0\n",
      "[]\n",
      "error pos shape 0\n",
      "error index\n",
      "[]\n",
      "s_3493162\n",
      "\n",
      "20\n",
      "[0, 1, 12, 28, 56, 148, 174, 267, 309, 333, 369, 438, 468, 499, 506, 511]\n",
      "Fitting 1 classifiers...\n",
      "Fitting classifier1: randomforestclassifier (1/1)\n",
      "Training and fitting fold 1 of 3...\n",
      "Training and fitting fold 2 of 3...\n",
      "Training and fitting fold 3 of 3...\n",
      "origin 1.0\n",
      "error shape, 0\n",
      "[]\n",
      "error pos shape 0\n",
      "error index\n",
      "[]\n",
      "s_3534278\n",
      "\n",
      "21\n",
      "[267, 438, 531, 611, 619, 681, 687, 705, 746, 852, 915]\n",
      "Fitting 1 classifiers...\n",
      "Fitting classifier1: randomforestclassifier (1/1)\n",
      "Training and fitting fold 1 of 3...\n",
      "Training and fitting fold 2 of 3...\n",
      "Training and fitting fold 3 of 3...\n",
      "origin 1.0\n",
      "error shape, 0\n",
      "[]\n",
      "error pos shape 0\n",
      "error index\n",
      "[]\n",
      "s_3542509\n",
      "\n",
      "22\n",
      "[15, 24, 32, 45, 47, 50, 52, 66, 70, 114, 146, 151, 153, 163, 186, 187, 234, 278]\n",
      "Fitting 1 classifiers...\n",
      "Fitting classifier1: randomforestclassifier (1/1)\n",
      "Training and fitting fold 1 of 3...\n",
      "Training and fitting fold 2 of 3...\n",
      "Training and fitting fold 3 of 3...\n",
      "origin 0.999551569507\n",
      "error shape, 2\n",
      "[1 1]\n",
      "error pos shape 2\n",
      "error index\n",
      "[2107 4227]\n",
      "s_3622929\n",
      "\n",
      "23\n",
      "[31, 55, 57, 62, 63, 83, 139, 160, 197, 207, 224, 289, 331, 412, 423, 1067, 1299, 1367, 2133]\n",
      "Fitting 1 classifiers...\n",
      "Fitting classifier1: randomforestclassifier (1/1)\n",
      "Training and fitting fold 1 of 3...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training and fitting fold 2 of 3...\n",
      "Training and fitting fold 3 of 3...\n",
      "origin 1.0\n",
      "error shape, 0\n",
      "[]\n",
      "error pos shape 0\n",
      "error index\n",
      "[]\n",
      "s_3643138\n",
      "\n",
      "24\n",
      "[59, 67, 72, 78, 80, 136, 154, 155, 275, 319, 356, 357, 374, 397, 406, 415, 451, 505, 507, 513, 547, 564, 720, 882]\n",
      "Fitting 1 classifiers...\n",
      "Fitting classifier1: randomforestclassifier (1/1)\n",
      "Training and fitting fold 1 of 3...\n",
      "Training and fitting fold 2 of 3...\n",
      "Training and fitting fold 3 of 3...\n",
      "origin 0.999775784753\n",
      "error shape, 1\n",
      "[1]\n",
      "error pos shape 1\n",
      "error index\n",
      "[906]\n",
      "s_3644057\n",
      "\n",
      "25\n",
      "[58, 96, 106, 128, 130, 150, 167, 184, 192, 206, 212, 310, 419, 426, 476]\n",
      "Fitting 1 classifiers...\n",
      "Fitting classifier1: randomforestclassifier (1/1)\n",
      "Training and fitting fold 1 of 3...\n",
      "Training and fitting fold 2 of 3...\n",
      "Training and fitting fold 3 of 3...\n",
      "origin 1.0\n",
      "error shape, 0\n",
      "[]\n",
      "error pos shape 0\n",
      "error index\n",
      "[]\n",
      "s_3658709\n",
      "\n",
      "26\n",
      "[8, 10, 20, 23, 26, 40, 65, 112, 123, 132, 133, 147, 168, 169, 171, 175, 180, 195, 202, 230, 302, 436]\n",
      "Fitting 1 classifiers...\n",
      "Fitting classifier1: randomforestclassifier (1/1)\n",
      "Training and fitting fold 1 of 3...\n",
      "Training and fitting fold 2 of 3...\n",
      "Training and fitting fold 3 of 3...\n",
      "origin 0.992600896861\n",
      "error shape, 33\n",
      "[0 0 1 1 0 1 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 1 0 0 0 0 1 1 0 0 0 0]\n",
      "error pos shape 8\n",
      "error index\n",
      "[ 241  493  728  799  927 1068 1158 1470 1493 1516 1546 1554 1590 1703 2094\n",
      " 2133 2601 2637 2679 2734 2789 2887 2915 2926 3144 3184 3652 3711 4057 4062\n",
      " 4117 4367 4435]\n",
      "s_3726779\n",
      "\n",
      "27\n",
      "[58, 128, 130, 150, 248, 258, 267, 269, 322, 368, 376, 390, 424, 427, 521, 532]\n",
      "Fitting 1 classifiers...\n",
      "Fitting classifier1: randomforestclassifier (1/1)\n",
      "Training and fitting fold 1 of 3...\n",
      "Training and fitting fold 2 of 3...\n",
      "Training and fitting fold 3 of 3...\n",
      "origin 0.999103139013\n",
      "error shape, 4\n",
      "[1 1 1 1]\n",
      "error pos shape 4\n",
      "error index\n",
      "[ 650 1448 2672 3246]\n",
      "s_3727061\n",
      "\n",
      "28\n",
      "[15, 24, 32, 41, 45, 50, 52, 53, 64, 66, 88, 92, 146, 153, 217, 221, 223, 229, 245, 278, 290, 317]\n",
      "Fitting 1 classifiers...\n",
      "Fitting classifier1: randomforestclassifier (1/1)\n",
      "Training and fitting fold 1 of 3...\n",
      "Training and fitting fold 2 of 3...\n",
      "Training and fitting fold 3 of 3...\n",
      "origin 0.999551569507\n",
      "error shape, 2\n",
      "[1 0]\n",
      "error pos shape 1\n",
      "error index\n",
      "[ 868 3620]\n",
      "s_3738103\n",
      "\n",
      "29\n",
      "[26, 65, 132, 133, 147, 171, 180, 230, 255, 257, 405, 408, 435, 452, 456, 492, 503]\n",
      "Fitting 1 classifiers...\n",
      "Fitting classifier1: randomforestclassifier (1/1)\n",
      "Training and fitting fold 1 of 3...\n",
      "Training and fitting fold 2 of 3...\n",
      "Training and fitting fold 3 of 3...\n",
      "origin 0.999775784753\n",
      "error shape, 1\n",
      "[1]\n",
      "error pos shape 1\n",
      "error index\n",
      "[3287]\n",
      "s_3743937\n",
      "\n",
      "30\n",
      "[130, 150, 248, 368, 403, 419, 470]\n",
      "Fitting 1 classifiers...\n",
      "Fitting classifier1: randomforestclassifier (1/1)\n",
      "Training and fitting fold 1 of 3...\n",
      "Training and fitting fold 2 of 3...\n",
      "Training and fitting fold 3 of 3...\n",
      "origin 1.0\n",
      "error shape, 0\n",
      "[]\n",
      "error pos shape 0\n",
      "error index\n",
      "[]\n",
      "s_3759545\n",
      "\n",
      "31\n",
      "[58, 96, 106, 167, 184, 192, 206, 211, 219, 237, 384, 517, 582]\n",
      "Fitting 1 classifiers...\n",
      "Fitting classifier1: randomforestclassifier (1/1)\n",
      "Training and fitting fold 1 of 3...\n",
      "Training and fitting fold 2 of 3...\n",
      "Training and fitting fold 3 of 3...\n",
      "origin 1.0\n",
      "error shape, 0\n",
      "[]\n",
      "error pos shape 0\n",
      "error index\n",
      "[]\n",
      "s_3779904\n",
      "\n",
      "32\n",
      "[16, 30, 35, 39, 46, 68, 73, 84, 126, 166, 199, 203, 274, 306, 353, 377, 378, 489, 591, 642, 684]\n",
      "Fitting 1 classifiers...\n",
      "Fitting classifier1: randomforestclassifier (1/1)\n",
      "Training and fitting fold 1 of 3...\n",
      "Training and fitting fold 2 of 3...\n",
      "Training and fitting fold 3 of 3...\n",
      "origin 0.99932735426\n",
      "error shape, 3\n",
      "[1 1 0]\n",
      "error pos shape 2\n",
      "error index\n",
      "[ 809 3379 4108]\n",
      "s_3818635\n",
      "\n",
      "33\n",
      "[2, 3, 4, 6, 7, 9, 13, 21, 27, 29, 37, 42, 49, 54, 60, 61, 71, 79, 91, 94, 125, 165]\n",
      "Fitting 1 classifiers...\n",
      "Fitting classifier1: randomforestclassifier (1/1)\n",
      "Training and fitting fold 1 of 3...\n",
      "Training and fitting fold 2 of 3...\n",
      "Training and fitting fold 3 of 3...\n",
      "origin 0.995515695067\n",
      "error shape, 20\n",
      "[0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 1]\n",
      "error pos shape 17\n",
      "error index\n",
      "[  94  104  468  606  720  863  989 1397 1484 1513 1524 2182 2374 2634 2964\n",
      " 3251 3860 4017 4219 4301]\n",
      "s_3827392\n",
      "\n",
      "34\n",
      "[10, 26, 40, 65, 112, 123, 133, 147, 168, 171, 175, 255, 257, 302, 393, 408, 436, 452, 466, 516, 566, 612, 614, 657]\n",
      "Fitting 1 classifiers...\n",
      "Fitting classifier1: randomforestclassifier (1/1)\n",
      "Training and fitting fold 1 of 3...\n",
      "Training and fitting fold 2 of 3...\n",
      "Training and fitting fold 3 of 3...\n",
      "origin 1.0\n",
      "error shape, 0\n",
      "[]\n",
      "error pos shape 0\n",
      "error index\n",
      "[]\n",
      "s_3833950\n",
      "\n",
      "35\n",
      "[31, 55, 57, 58, 62, 63, 83, 96, 139, 160, 224, 244, 266, 423, 540]\n",
      "Fitting 1 classifiers...\n",
      "Fitting classifier1: randomforestclassifier (1/1)\n",
      "Training and fitting fold 1 of 3...\n",
      "Training and fitting fold 2 of 3...\n",
      "Training and fitting fold 3 of 3...\n",
      "origin 0.99932735426\n",
      "error shape, 3\n",
      "[1 0 0]\n",
      "error pos shape 1\n",
      "error index\n",
      "[1469 2503 4025]\n",
      "s_3902873\n",
      "\n",
      "36\n",
      "37\n",
      "[130, 150, 248, 258, 267, 269, 322, 368, 369, 376, 390, 403, 410, 424, 427, 532, 1023]\n",
      "Fitting 1 classifiers...\n",
      "Fitting classifier1: randomforestclassifier (1/1)\n",
      "Training and fitting fold 1 of 3...\n",
      "Training and fitting fold 2 of 3...\n",
      "Training and fitting fold 3 of 3...\n",
      "origin 1.0\n",
      "error shape, 0\n",
      "[]\n",
      "error pos shape 0\n",
      "error index\n",
      "[]\n",
      "s_3907298\n",
      "\n",
      "38\n",
      "[31, 55, 62, 83, 244, 266, 289, 412, 423]\n",
      "Fitting 1 classifiers...\n",
      "Fitting classifier1: randomforestclassifier (1/1)\n",
      "Training and fitting fold 1 of 3...\n",
      "Training and fitting fold 2 of 3...\n",
      "Training and fitting fold 3 of 3...\n",
      "origin 1.0\n",
      "error shape, 0\n",
      "[]\n",
      "error pos shape 0\n",
      "error index\n",
      "[]\n",
      "s_3931209\n",
      "\n",
      "39\n",
      "[10, 20, 31, 34, 55, 57, 58, 63, 83, 96, 134, 139, 160, 197, 207, 211, 219, 224, 266, 289, 321, 331, 384, 526]\n",
      "Fitting 1 classifiers...\n",
      "Fitting classifier1: randomforestclassifier (1/1)\n",
      "Training and fitting fold 1 of 3...\n",
      "Training and fitting fold 2 of 3...\n",
      "Training and fitting fold 3 of 3...\n",
      "origin 1.0\n",
      "error shape, 0\n",
      "[]\n",
      "error pos shape 0\n",
      "error index\n",
      "[]\n",
      "s_3939053\n",
      "\n",
      "40\n",
      "[10, 20, 23, 34, 55, 57, 62, 63, 74, 90, 104, 134, 240, 287, 407, 478, 1747]\n",
      "Fitting 1 classifiers...\n",
      "Fitting classifier1: randomforestclassifier (1/1)\n",
      "Training and fitting fold 1 of 3...\n",
      "Training and fitting fold 2 of 3...\n",
      "Training and fitting fold 3 of 3...\n",
      "origin 0.999551569507\n",
      "error shape, 2\n",
      "[1 1]\n",
      "error pos shape 2\n",
      "error index\n",
      "[  97 1810]\n",
      "s_4005274\n",
      "\n",
      "41\n",
      "[8, 10, 20, 23, 26, 31, 34, 40, 57, 65, 74, 112, 123, 132, 133, 134, 147, 168, 175, 180, 195, 255]\n",
      "Fitting 1 classifiers...\n",
      "Fitting classifier1: randomforestclassifier (1/1)\n",
      "Training and fitting fold 1 of 3...\n",
      "Training and fitting fold 2 of 3...\n",
      "Training and fitting fold 3 of 3...\n",
      "origin 0.988789237668\n",
      "error shape, 50\n",
      "[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 0 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 0 1 1 1 1 1 1 1 1 1 1]\n",
      "error pos shape 47\n",
      "error index\n",
      "[  38  124  408  493  709  898  920  927 1114 1158 1394 1468 1493 1516 1554\n",
      " 1565 1590 1703 1733 1783 1784 1785 1898 2014 2080 2094 2121 2193 2463 2677\n",
      " 2734 3080 3131 3144 3349 3385 3400 3516 3652 3711 3755 3757 4094 4103 4128\n",
      " 4129 4177 4355 4412 4435]\n",
      "s_4009954\n",
      "\n",
      "42\n",
      "[107, 161, 173, 182, 208, 228, 231, 238, 242, 265, 268, 271, 285, 286, 293, 343, 349, 365, 401, 434, 437, 442, 450, 481]\n",
      "Fitting 1 classifiers...\n",
      "Fitting classifier1: randomforestclassifier (1/1)\n",
      "Training and fitting fold 1 of 3...\n",
      "Training and fitting fold 2 of 3...\n",
      "Training and fitting fold 3 of 3...\n",
      "origin 0.99798206278\n",
      "error shape, 9\n",
      "[1 1 1 1 1 1 1 1 1]\n",
      "error pos shape 9\n",
      "error index\n",
      "[ 290  415  471 1511 1589 1940 1941 2012 4155]\n",
      "s_433337\n",
      "\n",
      "43\n",
      "[2, 3, 4, 5, 6, 7, 9, 13, 14, 33, 42, 48, 95, 213, 326, 433]\n",
      "Fitting 1 classifiers...\n",
      "Fitting classifier1: randomforestclassifier (1/1)\n",
      "Training and fitting fold 1 of 3...\n",
      "Training and fitting fold 2 of 3...\n",
      "Training and fitting fold 3 of 3...\n",
      "origin 0.99932735426\n",
      "error shape, 3\n",
      "[1 1 1]\n",
      "error pos shape 3\n",
      "error index\n",
      "[2355 3052 3247]\n",
      "s_490562\n",
      "\n",
      "44\n",
      "[59, 67, 72, 78, 80, 109, 136, 143, 154, 155, 239, 249, 253, 275, 301, 319, 336, 374, 400, 458, 569]\n",
      "Fitting 1 classifiers...\n",
      "Fitting classifier1: randomforestclassifier (1/1)\n",
      "Training and fitting fold 1 of 3...\n",
      "Training and fitting fold 2 of 3...\n",
      "Training and fitting fold 3 of 3...\n",
      "origin 0.999775784753\n",
      "error shape, 1\n",
      "[1]\n",
      "error pos shape 1\n",
      "error index\n",
      "[4021]\n",
      "s_490980\n",
      "\n",
      "45\n",
      "[59, 67, 72, 78, 80, 136, 143, 154, 155, 205, 239, 249, 275, 319, 356, 357, 367, 374, 397, 406, 415, 431, 445, 451, 465, 501]\n",
      "Fitting 1 classifiers...\n",
      "Fitting classifier1: randomforestclassifier (1/1)\n",
      "Training and fitting fold 1 of 3...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training and fitting fold 2 of 3...\n",
      "Training and fitting fold 3 of 3...\n",
      "origin 0.996860986547\n",
      "error shape, 14\n",
      "[0 1 1 1 0 0 0 1 0 0 0 1 1 0]\n",
      "error pos shape 6\n",
      "error index\n",
      "[ 311 1173 1174 1175 1279 2131 2442 2933 3034 3200 3600 4054 4061 4299]\n",
      "s_491277\n",
      "\n",
      "46\n",
      "[88, 92, 98, 101, 103, 116, 122, 233, 236, 254, 256, 295, 300, 330, 345, 349, 352, 383, 404, 422, 500, 556, 643, 696, 738]\n",
      "Fitting 1 classifiers...\n",
      "Fitting classifier1: randomforestclassifier (1/1)\n",
      "Training and fitting fold 1 of 3...\n",
      "Training and fitting fold 2 of 3...\n",
      "Training and fitting fold 3 of 3...\n",
      "origin 1.0\n",
      "error shape, 0\n",
      "[]\n",
      "error pos shape 0\n",
      "error index\n",
      "[]\n",
      "s_493201\n",
      "\n",
      "47\n",
      "[59, 67, 72, 80, 143, 155, 179, 235, 239, 253, 277, 283, 301, 336, 358, 429, 458, 874, 2263, 2547]\n",
      "Fitting 1 classifiers...\n",
      "Fitting classifier1: randomforestclassifier (1/1)\n",
      "Training and fitting fold 1 of 3...\n",
      "Training and fitting fold 2 of 3...\n",
      "Training and fitting fold 3 of 3...\n",
      "origin 1.0\n",
      "error shape, 0\n",
      "[]\n",
      "error pos shape 0\n",
      "error index\n",
      "[]\n",
      "s_496413\n",
      "\n",
      "48\n",
      "[2, 3, 4, 5, 7, 11, 13, 14, 16, 33, 38, 42, 82, 95, 99, 102, 118, 204]\n",
      "Fitting 1 classifiers...\n",
      "Fitting classifier1: randomforestclassifier (1/1)\n",
      "Training and fitting fold 1 of 3...\n",
      "Training and fitting fold 2 of 3...\n",
      "Training and fitting fold 3 of 3...\n",
      "origin 0.99932735426\n",
      "error shape, 3\n",
      "[1 1 1]\n",
      "error pos shape 3\n",
      "error index\n",
      "[ 343 3267 3712]\n",
      "s_497803\n",
      "\n",
      "49\n",
      "[2, 3, 4, 6, 7, 13, 21, 27, 29, 37, 49, 54, 60, 61, 71, 91, 94, 125, 165, 232, 394, 432]\n",
      "Fitting 1 classifiers...\n",
      "Fitting classifier1: randomforestclassifier (1/1)\n",
      "Training and fitting fold 1 of 3...\n",
      "Training and fitting fold 2 of 3...\n",
      "Training and fitting fold 3 of 3...\n",
      "origin 0.996412556054\n",
      "error shape, 16\n",
      "[0 1 0 0 0 0 1 1 1 1 0 0 1 1 0 1]\n",
      "error pos shape 8\n",
      "error index\n",
      "[  18   79  184  466  899 1138 1396 1399 1671 1672 2122 2352 3028 3029 3251\n",
      " 3253]\n",
      "s_501724\n",
      "\n",
      "50\n",
      "[15, 45, 47, 136, 186, 218, 251, 284, 318, 328, 338, 342, 344, 363, 391, 444]\n",
      "Fitting 1 classifiers...\n",
      "Fitting classifier1: randomforestclassifier (1/1)\n",
      "Training and fitting fold 1 of 3...\n",
      "Training and fitting fold 2 of 3...\n",
      "Training and fitting fold 3 of 3...\n",
      "origin 0.999775784753\n",
      "error shape, 1\n",
      "[1]\n",
      "error pos shape 1\n",
      "error index\n",
      "[867]\n",
      "s_504009\n",
      "\n",
      "51\n",
      "[4, 5, 11, 13, 16, 18, 22, 25, 33, 35, 38, 43, 82, 99, 102, 105, 118, 194, 204]\n",
      "Fitting 1 classifiers...\n",
      "Fitting classifier1: randomforestclassifier (1/1)\n",
      "Training and fitting fold 1 of 3...\n",
      "Training and fitting fold 2 of 3...\n",
      "Training and fitting fold 3 of 3...\n",
      "origin 0.997757847534\n",
      "error shape, 10\n",
      "[1 1 1 1 1 1 1 1 1 1]\n",
      "error pos shape 10\n",
      "error index\n",
      "[ 145  146  151  165 1758 2159 2259 3960 4067 4068]\n",
      "s_506583\n",
      "\n",
      "52\n",
      "[2, 3, 4, 5, 6, 7, 9, 13, 14, 21, 27, 29, 37, 42, 48, 49, 60, 61, 71, 91, 232, 273]\n",
      "Fitting 1 classifiers...\n",
      "Fitting classifier1: randomforestclassifier (1/1)\n",
      "Training and fitting fold 1 of 3...\n",
      "Training and fitting fold 2 of 3...\n",
      "Training and fitting fold 3 of 3...\n",
      "origin 0.992152466368\n",
      "error shape, 35\n",
      "[1 1 0 1 1 1 0 1 1 1 1 1 0 1 0 0 0 1 1 1 1 1 0 1 1 1 0 1 1 0 1 0 1 0 1]\n",
      "error pos shape 24\n",
      "error index\n",
      "[ 122  279  412  544  846 1099 1272 1566 1630 1752 1770 1826 1846 1885 1979\n",
      " 2048 2174 2403 2472 2474 2484 2909 2965 2973 3026 3157 3328 3413 3480 3506\n",
      " 3689 3753 3828 4078 4305]\n",
      "s_506597\n",
      "\n",
      "53\n",
      "[15, 24, 32, 41, 45, 50, 52, 53, 64, 66, 88, 92, 101, 217, 223, 272, 280, 294, 303, 347, 425]\n",
      "Fitting 1 classifiers...\n",
      "Fitting classifier1: randomforestclassifier (1/1)\n",
      "Training and fitting fold 1 of 3...\n",
      "Training and fitting fold 2 of 3...\n",
      "Training and fitting fold 3 of 3...\n",
      "origin 0.99730941704\n",
      "error shape, 12\n",
      "[1 1 0 1 1 0 1 0 1 1 1 0]\n",
      "error pos shape 8\n",
      "error index\n",
      "[ 504  636  868  993 1145 1918 1983 2853 3620 3809 4093 4319]\n",
      "s_506646\n",
      "\n",
      "54\n",
      "[15, 24, 32, 41, 45, 50, 52, 53, 64, 66, 88, 92, 146, 153, 217, 221, 229, 245, 290, 308]\n",
      "Fitting 1 classifiers...\n",
      "Fitting classifier1: randomforestclassifier (1/1)\n",
      "Training and fitting fold 1 of 3...\n",
      "Training and fitting fold 2 of 3...\n",
      "Training and fitting fold 3 of 3...\n",
      "origin 0.999775784753\n",
      "error shape, 1\n",
      "[1]\n",
      "error pos shape 1\n",
      "error index\n",
      "[3927]\n",
      "s_507000\n",
      "\n",
      "55\n",
      "[2, 3, 4, 5, 6, 7, 9, 13, 14, 21, 27, 29, 33, 37, 42, 48, 49, 60, 71, 77, 86, 95, 193, 220]\n",
      "Fitting 1 classifiers...\n",
      "Fitting classifier1: randomforestclassifier (1/1)\n",
      "Training and fitting fold 1 of 3...\n",
      "Training and fitting fold 2 of 3...\n",
      "Training and fitting fold 3 of 3...\n",
      "origin 0.986547085202\n",
      "error shape, 60\n",
      "[1 1 0 1 1 0 1 1 1 1 1 0 0 0 1 1 0 0 1 1 0 1 0 0 0 0 1 1 1 1 1 1 1 0 0 1 1\n",
      " 0 1 1 0 0 0 0 1 1 1 1 1 0 0 1 1 1 1 1 1 1 1 0]\n",
      "error pos shape 38\n",
      "error index\n",
      "[  78  250  279  431  432  474  587  588  590  591  630  753  781  801  958\n",
      " 1033 1097 1099 1230 1276 1566 1695 1752 1756 1770 1885 1958 1979 2248 2267\n",
      " 2297 2354 2393 2403 2474 2575 2623 2632 2712 2824 2909 2973 3026 3027 3112\n",
      " 3115 3373 3378 3431 3480 3689 3849 4079 4108 4111 4114 4115 4153 4192 4301]\n",
      "s_508287\n",
      "\n",
      "56\n",
      "[6, 54, 59, 67, 72, 78, 80, 109, 120, 121, 138, 140, 156, 239, 264, 301]\n",
      "Fitting 1 classifiers...\n",
      "Fitting classifier1: randomforestclassifier (1/1)\n",
      "Training and fitting fold 1 of 3...\n",
      "Training and fitting fold 2 of 3...\n",
      "Training and fitting fold 3 of 3...\n",
      "origin 0.999551569507\n",
      "error shape, 2\n",
      "[1 1]\n",
      "error pos shape 2\n",
      "error index\n",
      "[1909 3805]\n",
      "s_509630\n",
      "\n",
      "57\n",
      "[15, 24, 45, 47, 52, 70, 131, 151, 164, 186, 187, 216, 243, 247, 472, 491, 549, 629, 631, 649]\n",
      "Fitting 1 classifiers...\n",
      "Fitting classifier1: randomforestclassifier (1/1)\n",
      "Training and fitting fold 1 of 3...\n",
      "Training and fitting fold 2 of 3...\n",
      "Training and fitting fold 3 of 3...\n",
      "origin 0.998878923767\n",
      "error shape, 5\n",
      "[1 0 1 1 1]\n",
      "error pos shape 4\n",
      "error index\n",
      "[ 458 1287 2620 3405 4019]\n",
      "s_510225\n",
      "\n",
      "58\n",
      "[44, 51, 75, 76, 81, 85, 89, 98, 103, 115, 116, 122, 145, 198, 226, 254, 332, 360]\n",
      "Fitting 1 classifiers...\n",
      "Fitting classifier1: randomforestclassifier (1/1)\n",
      "Training and fitting fold 1 of 3...\n",
      "Training and fitting fold 2 of 3...\n",
      "Training and fitting fold 3 of 3...\n",
      "origin 0.998430493274\n",
      "error shape, 7\n",
      "[1 1 1 1 1 1 1]\n",
      "error pos shape 7\n",
      "error index\n",
      "[ 306  309 1684 1696 1897 3318 3319]\n",
      "s_510334\n",
      "\n",
      "59\n",
      "[2, 6, 27, 29, 37, 54, 59, 61, 67, 72, 78, 91, 109, 120, 121, 125, 138, 140, 156, 165, 264, 457]\n",
      "Fitting 1 classifiers...\n",
      "Fitting classifier1: randomforestclassifier (1/1)\n",
      "Training and fitting fold 1 of 3...\n",
      "Training and fitting fold 2 of 3...\n",
      "Training and fitting fold 3 of 3...\n",
      "origin 0.99932735426\n",
      "error shape, 3\n",
      "[1 1 0]\n",
      "error pos shape 2\n",
      "error index\n",
      "[ 386  388 1033]\n",
      "s_512700\n",
      "\n",
      "60\n",
      "[2, 3, 4, 6, 7, 13, 21, 27, 29, 37, 49, 54, 61, 91, 120, 121, 125, 140, 165, 605]\n",
      "Fitting 1 classifiers...\n",
      "Fitting classifier1: randomforestclassifier (1/1)\n",
      "Training and fitting fold 1 of 3...\n",
      "Training and fitting fold 2 of 3...\n",
      "Training and fitting fold 3 of 3...\n",
      "origin 0.99932735426\n",
      "error shape, 3\n",
      "[1 1 1]\n",
      "error pos shape 3\n",
      "error index\n",
      "[ 994 3821 4235]\n",
      "s_514913\n",
      "\n",
      "61\n",
      "[2, 3, 4, 6, 7, 9, 13, 21, 27, 29, 37, 49, 54, 60, 61, 71, 79, 91, 94, 120, 125, 165, 232]\n",
      "Fitting 1 classifiers...\n",
      "Fitting classifier1: randomforestclassifier (1/1)\n",
      "Training and fitting fold 1 of 3...\n",
      "Training and fitting fold 2 of 3...\n",
      "Training and fitting fold 3 of 3...\n",
      "origin 0.995515695067\n",
      "error shape, 20\n",
      "[1 0 0 1 0 1 1 0 0 1 0 0 1 1 1 0 0 0 1 1]\n",
      "error pos shape 10\n",
      "error index\n",
      "[  18   35  354  466  468  795  917 1112 1513 2122 2182 2964 3025 3027 3254\n",
      " 3628 3860 3923 4027 4149]\n",
      "s_515010\n",
      "\n",
      "62\n",
      "[59, 67, 72, 78, 80, 136, 154, 155, 205, 276, 291, 312, 319, 323, 354, 379, 397, 400, 498, 513]\n",
      "Fitting 1 classifiers...\n",
      "Fitting classifier1: randomforestclassifier (1/1)\n",
      "Training and fitting fold 1 of 3...\n",
      "Training and fitting fold 2 of 3...\n",
      "Training and fitting fold 3 of 3...\n",
      "origin 1.0\n",
      "error shape, 0\n",
      "[]\n",
      "error pos shape 0\n",
      "error index\n",
      "[]\n",
      "s_515753\n",
      "\n",
      "63\n",
      "[67, 72, 78, 80, 129, 143, 154, 155, 176, 235, 241, 253, 263, 270, 277, 281, 283, 336, 358, 429, 458]\n",
      "Fitting 1 classifiers...\n",
      "Fitting classifier1: randomforestclassifier (1/1)\n",
      "Training and fitting fold 1 of 3...\n",
      "Training and fitting fold 2 of 3...\n",
      "Training and fitting fold 3 of 3...\n",
      "origin 0.999103139013\n",
      "error shape, 4\n",
      "[0 0 0 1]\n",
      "error pos shape 1\n",
      "error index\n",
      "[ 588  589 2103 2701]\n",
      "s_517510\n",
      "\n",
      "64\n",
      "[11, 16, 18, 22, 25, 30, 35, 38, 39, 43, 46, 69, 73, 81, 85, 87, 105, 172, 181, 194]\n",
      "Fitting 1 classifiers...\n",
      "Fitting classifier1: randomforestclassifier (1/1)\n",
      "Training and fitting fold 1 of 3...\n",
      "Training and fitting fold 2 of 3...\n",
      "Training and fitting fold 3 of 3...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "origin 0.99798206278\n",
      "error shape, 9\n",
      "[1 1 1 1 1 1 0 0 1]\n",
      "error pos shape 7\n",
      "error index\n",
      "[ 692  693 2633 3158 3971 3972 4128 4129 4397]\n",
      "s_517764\n",
      "\n",
      "65\n",
      "[59, 67, 72, 78, 80, 136, 143, 154, 155, 239, 241, 249, 253, 270, 281, 283, 336, 354, 429]\n",
      "Fitting 1 classifiers...\n",
      "Fitting classifier1: randomforestclassifier (1/1)\n",
      "Training and fitting fold 1 of 3...\n",
      "Training and fitting fold 2 of 3...\n",
      "Training and fitting fold 3 of 3...\n",
      "origin 0.999551569507\n",
      "error shape, 2\n",
      "[1 1]\n",
      "error pos shape 2\n",
      "error index\n",
      "[627 629]\n",
      "s_517795\n",
      "\n",
      "66\n",
      "[93, 97, 100, 124, 127, 141, 142, 144, 149, 162, 170, 190, 210, 313, 370, 389, 428, 443]\n",
      "Fitting 1 classifiers...\n",
      "Fitting classifier1: randomforestclassifier (1/1)\n",
      "Training and fitting fold 1 of 3...\n",
      "Training and fitting fold 2 of 3...\n",
      "Training and fitting fold 3 of 3...\n",
      "origin 0.999103139013\n",
      "error shape, 4\n",
      "[0 1 1 1]\n",
      "error pos shape 3\n",
      "error index\n",
      "[ 860 2616 3778 3779]\n",
      "s_522897\n",
      "\n",
      "67\n",
      "[15, 24, 32, 41, 45, 47, 50, 52, 64, 66, 70, 131, 146, 151, 153, 164, 186, 187, 209, 216, 243, 247, 307, 348]\n",
      "Fitting 1 classifiers...\n",
      "Fitting classifier1: randomforestclassifier (1/1)\n",
      "Training and fitting fold 1 of 3...\n",
      "Training and fitting fold 2 of 3...\n",
      "Training and fitting fold 3 of 3...\n",
      "origin 0.998878923767\n",
      "error shape, 5\n",
      "[1 1 1 1 1]\n",
      "error pos shape 5\n",
      "error index\n",
      "[ 723 1841 1842 2846 3011]\n",
      "s_525418\n",
      "\n",
      "68\n",
      "[30, 39, 44, 51, 68, 75, 76, 84, 88, 89, 92, 98, 103, 111, 122, 145, 198, 226, 233, 254, 256, 292, 295, 332, 350, 360]\n",
      "Fitting 1 classifiers...\n",
      "Fitting classifier1: randomforestclassifier (1/1)\n",
      "Training and fitting fold 1 of 3...\n",
      "Training and fitting fold 2 of 3...\n",
      "Training and fitting fold 3 of 3...\n",
      "origin 1.0\n",
      "error shape, 0\n",
      "[]\n",
      "error pos shape 0\n",
      "error index\n",
      "[]\n",
      "s_525779\n",
      "\n",
      "69\n",
      "[15, 45, 47, 70, 136, 186, 218, 247, 251, 284, 319, 328, 338, 342, 344, 391, 573, 595, 608, 912, 999, 2282]\n",
      "Fitting 1 classifiers...\n",
      "Fitting classifier1: randomforestclassifier (1/1)\n",
      "Training and fitting fold 1 of 3...\n",
      "Training and fitting fold 2 of 3...\n",
      "Training and fitting fold 3 of 3...\n",
      "origin 0.999775784753\n",
      "error shape, 1\n",
      "[0]\n",
      "error pos shape 0\n",
      "error index\n",
      "[291]\n",
      "s_528553\n",
      "\n",
      "70\n",
      "[16, 30, 39, 68, 73, 103, 107, 126, 166, 173, 177, 198, 222, 268, 297, 315, 353, 378]\n",
      "Fitting 1 classifiers...\n",
      "Fitting classifier1: randomforestclassifier (1/1)\n",
      "Training and fitting fold 1 of 3...\n",
      "Training and fitting fold 2 of 3...\n",
      "Training and fitting fold 3 of 3...\n",
      "origin 0.999775784753\n",
      "error shape, 1\n",
      "[1]\n",
      "error pos shape 1\n",
      "error index\n",
      "[3290]\n",
      "s_540782\n",
      "\n",
      "71\n",
      "[39, 44, 51, 81, 88, 93, 100, 103, 142, 162, 198, 254, 256, 313, 361, 417, 443, 454, 469]\n",
      "Fitting 1 classifiers...\n",
      "Fitting classifier1: randomforestclassifier (1/1)\n",
      "Training and fitting fold 1 of 3...\n",
      "Training and fitting fold 2 of 3...\n",
      "Training and fitting fold 3 of 3...\n",
      "origin 1.0\n",
      "error shape, 0\n",
      "[]\n",
      "error pos shape 0\n",
      "error index\n",
      "[]\n",
      "s_542833\n",
      "\n",
      "72\n",
      "[228, 271, 286, 330, 335, 366, 381, 386, 402, 409, 416, 437, 459, 460, 504, 518, 522, 530, 555, 624, 647, 672]\n",
      "Fitting 1 classifiers...\n",
      "Fitting classifier1: randomforestclassifier (1/1)\n",
      "Training and fitting fold 1 of 3...\n",
      "Training and fitting fold 2 of 3...\n",
      "Training and fitting fold 3 of 3...\n",
      "origin 0.99932735426\n",
      "error shape, 3\n",
      "[1 1 0]\n",
      "error pos shape 2\n",
      "error index\n",
      "[ 398 1053 3209]\n",
      "s_570869\n",
      "\n",
      "73\n",
      "[5, 11, 16, 18, 22, 25, 30, 35, 38, 43, 46, 69, 73, 82, 87, 515]\n",
      "Fitting 1 classifiers...\n",
      "Fitting classifier1: randomforestclassifier (1/1)\n",
      "Training and fitting fold 1 of 3...\n",
      "Training and fitting fold 2 of 3...\n",
      "Training and fitting fold 3 of 3...\n",
      "origin 0.999103139013\n",
      "error shape, 4\n",
      "[1 1 1 1]\n",
      "error pos shape 4\n",
      "error index\n",
      "[2287 2289 2490 4434]\n",
      "s_572085\n",
      "\n",
      "74\n",
      "[158, 178, 179, 196, 201, 214, 225, 227, 250, 261, 262, 288, 299, 380, 382, 388, 395, 398, 399, 413, 446, 475, 483, 494]\n",
      "Fitting 1 classifiers...\n",
      "Fitting classifier1: randomforestclassifier (1/1)\n",
      "Training and fitting fold 1 of 3...\n",
      "Training and fitting fold 2 of 3...\n",
      "Training and fitting fold 3 of 3...\n",
      "origin 0.999775784753\n",
      "error shape, 1\n",
      "[1]\n",
      "error pos shape 1\n",
      "error index\n",
      "[902]\n",
      "s_621251\n",
      "\n",
      "75\n",
      "[16, 30, 39, 46, 51, 68, 73, 84, 111, 122, 126, 145, 166, 177, 199, 203, 314, 353, 371]\n",
      "Fitting 1 classifiers...\n",
      "Fitting classifier1: randomforestclassifier (1/1)\n",
      "Training and fitting fold 1 of 3...\n",
      "Training and fitting fold 2 of 3...\n",
      "Training and fitting fold 3 of 3...\n",
      "origin 1.0\n",
      "error shape, 0\n",
      "[]\n",
      "error pos shape 0\n",
      "error index\n",
      "[]\n",
      "s_627691\n",
      "\n",
      "76\n",
      "[15, 24, 32, 41, 50, 53, 64, 66, 88, 92, 98, 101, 217, 223, 236, 280, 294, 298, 340, 346, 375]\n",
      "Fitting 1 classifiers...\n",
      "Fitting classifier1: randomforestclassifier (1/1)\n",
      "Training and fitting fold 1 of 3...\n",
      "Training and fitting fold 2 of 3...\n",
      "Training and fitting fold 3 of 3...\n",
      "origin 0.998878923767\n",
      "error shape, 5\n",
      "[0 0 0 0 1]\n",
      "error pos shape 1\n",
      "error index\n",
      "[ 504  868  993 1142 3261]\n",
      "s_643862\n",
      "\n",
      "77\n",
      "[15, 24, 32, 45, 47, 52, 70, 114, 131, 151, 163, 186, 187, 218, 234, 243, 296, 304, 328, 392]\n",
      "Fitting 1 classifiers...\n",
      "Fitting classifier1: randomforestclassifier (1/1)\n",
      "Training and fitting fold 1 of 3...\n",
      "Training and fitting fold 2 of 3...\n",
      "Training and fitting fold 3 of 3...\n",
      "origin 0.999551569507\n",
      "error shape, 2\n",
      "[1 1]\n",
      "error pos shape 2\n",
      "error index\n",
      "[  72 2064]\n",
      "s_792077\n",
      "\n",
      "78\n",
      "[20, 23, 31, 34, 55, 57, 62, 63, 90, 104, 113, 134, 137, 240, 260, 287, 423]\n",
      "Fitting 1 classifiers...\n",
      "Fitting classifier1: randomforestclassifier (1/1)\n",
      "Training and fitting fold 1 of 3...\n",
      "Training and fitting fold 2 of 3...\n",
      "Training and fitting fold 3 of 3...\n",
      "origin 0.999775784753\n",
      "error shape, 1\n",
      "[1]\n",
      "error pos shape 1\n",
      "error index\n",
      "[1978]\n",
      "s_819792\n",
      "\n",
      "79\n",
      "[110, 129, 157, 158, 159, 176, 179, 183, 185, 191, 200, 201, 205, 215, 227, 259, 263, 281, 288, 316, 320, 372, 461]\n",
      "Fitting 1 classifiers...\n",
      "Fitting classifier1: randomforestclassifier (1/1)\n",
      "Training and fitting fold 1 of 3...\n",
      "Training and fitting fold 2 of 3...\n",
      "Training and fitting fold 3 of 3...\n",
      "origin 0.998206278027\n",
      "error shape, 8\n",
      "[1 1 1 1 1 1 1 1]\n",
      "error pos shape 8\n",
      "error index\n",
      "[  70 1036 1037 2246 2632 3250 3300 4351]\n",
      "s_856830\n",
      "\n",
      "{0: 0.9993273542600897, 1: 0.9995515695067264, 2: 1.0, 3: 1.0, 4: 1.0, 5: 0.9982062780269059, 6: 0.9961883408071749, 7: 0.9991031390134529, 8: 0.9988789237668162, 9: 1.0, 10: 1.0, 11: 0.9997757847533633, 12: 1.0, 13: 1.0, 14: 0.9959641255605381, 15: 1.0, 16: 1.0, 17: 0.9995515695067264, 18: 1.0, 19: 1.0, 20: 1.0, 21: 1.0, 22: 0.9995515695067264, 23: 1.0, 24: 0.9997757847533633, 25: 1.0, 26: 0.9926008968609865, 27: 0.9991031390134529, 28: 0.9995515695067264, 29: 0.9997757847533633, 30: 1.0, 31: 1.0, 32: 0.9993273542600897, 33: 0.9955156950672646, 34: 1.0, 35: 0.9993273542600897, 36: 0, 37: 1.0, 38: 1.0, 39: 1.0, 40: 0.9995515695067264, 41: 0.9887892376681614, 42: 0.997982062780269, 43: 0.9993273542600897, 44: 0.9997757847533633, 45: 0.9968609865470852, 46: 1.0, 47: 1.0, 48: 0.9993273542600897, 49: 0.9964125560538116, 50: 0.9997757847533633, 51: 0.9977578475336323, 52: 0.992152466367713, 53: 0.9973094170403587, 54: 0.9997757847533633, 55: 0.9865470852017937, 56: 0.9995515695067264, 57: 0.9988789237668162, 58: 0.9984304932735426, 59: 0.9993273542600897, 60: 0.9993273542600897, 61: 0.9955156950672646, 62: 1.0, 63: 0.9991031390134529, 64: 0.997982062780269, 65: 0.9995515695067264, 66: 0.9991031390134529, 67: 0.9988789237668162, 68: 1.0, 69: 0.9997757847533633, 70: 0.9997757847533633, 71: 1.0, 72: 0.9993273542600897, 73: 0.9991031390134529, 74: 0.9997757847533633, 75: 1.0, 76: 0.9988789237668162, 77: 0.9995515695067264, 78: 0.9997757847533633, 79: 0.9982062780269059}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9468609865470852"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = {}\n",
    "probas = []\n",
    "for i in range(_train_b_y.shape[1]):\n",
    "#     i=26\n",
    "    print i\n",
    "    gt = train.iloc[train_y==lb.classes_[i]]\n",
    "    ngt = train.iloc[train_y!=lb.classes_[i]]\n",
    "    if len(gt) != 0:\n",
    "        \n",
    "        sorted_wifi = get_sorted_wifi([gt])\n",
    "        _indexs = get_indexs(df,sorted_wifi,0.1)\n",
    "       \n",
    "        cv = 3\n",
    "        n_sorted_wifi = get_sorted_wifi([ngt])\n",
    "        _nindexs = get_indexs2(df,n_sorted_wifi,50)\n",
    "#         _indexs = list(set(_indexs).union(set(_nindexs)))\n",
    "        prf = get_model(cv = cv)\n",
    "        otxs= []\n",
    "        ovxs = []\n",
    "#         _indexs = _indexs[:1]\n",
    "        print _indexs\n",
    "        modify_size =  (np.asarray(_indexs) < 6).sum()\n",
    "        _tx = train_wifi_all_x[:,_indexs]\n",
    "        _vx = valid_wifi_all_x[:,_indexs]\n",
    "        __tx,__vx = modify_wifi(_tx,_vx,train,valid,modify_size)\n",
    "        _tx = np.concatenate([_tx,__tx],axis=1)\n",
    "        _vx = np.concatenate([_vx,__vx],axis=1)\n",
    "        \n",
    "        _tx = np.concatenate([_tx,train_lonlats,train_wh],axis=1)\n",
    "        _vx = np.concatenate([_vx,valid_lonlats,valid_wh],axis=1)\n",
    "\n",
    "        if cv is not None and cv != 0:\n",
    "            _tx,_ty = expansion(_tx,_train_b_y[:,i],cv)\n",
    "        else:\n",
    "            _ty = _train_b_y[:,i]\n",
    "            \n",
    "        prf.fit(_tx,_ty)\n",
    "        p = prf.predict(_vx)\n",
    "        proba = prf.predict_proba(_vx)\n",
    "        probas.append(proba[:,1])\n",
    "        _acc = acc(p,_valid_b_y[:,i])\n",
    "        print \"origin\", _acc\n",
    "        r[i] = _acc\n",
    "        print \"error shape,\", (p != _valid_b_y[:,i]).sum()\n",
    "        print _valid_b_y[:,i][(p != _valid_b_y[:,i])]\n",
    "        print \"error pos shape\", (_valid_b_y[:,i][(p != _valid_b_y[:,i])]==1).sum()\n",
    "        print \"error index\"\n",
    "        print valid_index[(p != _valid_b_y[:,i])]\n",
    "#         print \"correct index\"\n",
    "#         print valid_index[(p == _valid_b_y[:,i]) & (_valid_b_y[:,i]==1)]\n",
    "        print le.classes_[i]\n",
    "        print\n",
    "    else:\n",
    "        probas.append(np.zeros((valid_y.shape[0],)))\n",
    "        r[i] = 0\n",
    "#     break\n",
    "print r\n",
    "acc(lb.classes_.take(np.argmax(np.vstack(probas).T,axis=1)), valid_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index([u'user_id', u'shop_id', u'time_stamp', u'longitude', u'latitude',\n",
       "       u'wifi_infos', u'category_id', u'shop_longitude', u'shop_latitude',\n",
       "       u'price', u'mall_id', u'dt', u'weekday', u'hour', u'is_weekend',\n",
       "       u'basic_wifi_info', u'wifi_size', u'use_wifi_size', u'no_use_wifi_size',\n",
       "       u'use_wifi_freq', u'no_use_wifi_freq', u'i_loc', u'day', u'dayofyear'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8947368421052632"
      ]
     },
     "execution_count": 343,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf1 = RandomForestClassifier(n_estimators=388,n_jobs=-1,class_weight=\"balanced\",min_weight_fraction_leaf=0.00002)\n",
    "rf1.fit(_train_x3,train_y)\n",
    "acc(rf1.predict(_valid_x3),valid_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 665,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.69780977178741"
      ]
     },
     "execution_count": 665,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf2 = RandomForestClassifier(n_estimators=388,n_jobs=-1,class_weight=\"balanced\")\n",
    "rf2.fit(_train_x3,train_y)\n",
    "acc(rf2.predict(_valid_x3),valid_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 679,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9017543859649123"
      ]
     },
     "execution_count": 679,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ovr = OneVsRestClassifier(estimator=RandomForestClassifier(n_estimators=188,n_jobs=-1,class_weight=\"balanced\"),n_jobs=-1)\n",
    "ovr.fit(_train_x3, train_y)\n",
    "acc(ovr.predict(_valid_x3),valid_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 1 classifiers...\n",
      "Fitting classifier1: onevsrestclassifier (1/1)\n",
      "Training and fitting fold 1 of 3...\n",
      "Training and fitting fold 2 of 3...\n",
      "Training and fitting fold 3 of 3...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9413919413919414"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#stack\n",
    "cv = 3\n",
    "_x,_y = expansion(_train_x33,train_y,cv)\n",
    "stack = StackingCVClassifier([\n",
    "#                                 RandomForestClassifier(n_jobs=-1,n_estimators=388,class_weight=\"balanced\"),\n",
    "#                               RandomForestClassifier(n_jobs=-1,n_estimators=388,class_weight=\"balanced\",min_weight_fraction_leaf=0.00002),\n",
    "                              OneVsRestClassifier(estimator=RandomForestClassifier(n_estimators=188,n_jobs=-1,class_weight=\"balanced\"))\n",
    "                              ],\n",
    "               \n",
    "                             OneVsRestClassifier(estimator=RandomForestClassifier(n_estimators=188,n_jobs=-1,class_weight=\"balanced\")),\n",
    "                             use_probas=True,\n",
    "                             verbose=1,\n",
    "                             use_features_in_secondary = True,\n",
    "                             cv = cv) \n",
    "stack.fit(_x,_y)\n",
    "p = stack.predict(_valid_x33)\n",
    "acc(p, valid_y)\n",
    "# OneVsRestClassifier(estimator=RandomForestClassifier(n_estimators=188,n_jobs=-1,class_weight=\"balanced\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 classifiers...\n",
      "Fitting classifier1: randomforestclassifier (1/2)\n",
      "Training and fitting fold 1 of 3...\n",
      "Training and fitting fold 2 of 3...\n",
      "Training and fitting fold 3 of 3...\n",
      "Fitting classifier2: onevsrestclassifier (2/2)\n",
      "Training and fitting fold 1 of 3...\n",
      "Training and fitting fold 2 of 3...\n",
      "Training and fitting fold 3 of 3...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.713126052994333"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#stack\n",
    "cv = 3\n",
    "_x,_y = expansion(_train_x3,train_y,cv)\n",
    "stack = StackingCVClassifier([RandomForestClassifier(n_jobs=-1,\n",
    "                                                     n_estimators=388,\n",
    "                                                     class_weight=\"balanced\"),\n",
    "                              OneVsRestClassifier(estimator=RandomForestClassifier(n_estimators=188,\n",
    "                                                                                   n_jobs=-1,\n",
    "                                                                                   class_weight=\"balanced\")),\n",
    "                              ],\n",
    "               \n",
    "                             RandomForestClassifier(n_estimators=666,n_jobs=-1,class_weight=\"balanced\"),\n",
    "                             use_probas=True,\n",
    "                             verbose=1,\n",
    "                             use_features_in_secondary = True,\n",
    "                             cv = cv) \n",
    "stack.fit(_x,_y)\n",
    "p = stack.predict(_valid_x3)\n",
    "acc(p, valid_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stack\n",
    "cv = 3\n",
    "_x,_y = expansion(_train_x3,train_y,cv * 2)\n",
    "stack = StackingCVClassifier([StackingCVClassifier([RandomForestClassifier(n_estimators=500,n_jobs=-1,class_weight=\"balanced\")],\n",
    "                                                     RandomForestClassifier(n_estimators=800,n_jobs=-1,class_weight=\"balanced\"),\n",
    "                                                     use_probas=True,\n",
    "                                                     verbose=0,\n",
    "                                                     use_features_in_secondary = False,\n",
    "                                                     cv = cv),\n",
    "                              OneVsRestClassifier(estimator=StackingCVClassifier([RandomForestClassifier(n_estimators=188,n_jobs=-1,class_weight=\"balanced\")],\n",
    "                                                         RandomForestClassifier(n_estimators=288,n_jobs=-1,class_weight=\"balanced\"),\n",
    "                                                         cv=cv,\n",
    "                                                         verbose=0,\n",
    "                                                         use_probas=True,\n",
    "                                                         use_features_in_secondary=True\n",
    "                                                        )),\n",
    "                              ],\n",
    "                             RandomForestClassifier(n_estimators=800,n_jobs=-1,class_weight=\"balanced\"),\n",
    "                             use_probas=True,\n",
    "                             use_features_in_secondary = True,\n",
    "                             verbose=1,\n",
    "                             cv = cv) \n",
    "stack.fit(_x,_y)\n",
    "p = stack.predict(_valid_x3)\n",
    "acc(p, valid_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfs = []\n",
    "for _i in range(_train_b_y.shape[1]):\n",
    "    rf = RandomForestClassifier(n_jobs=-1,n_estimators=188,class_weight=\"balanced\")\n",
    "    rf.fit(_train_x3, _train_b_y[:,_i])\n",
    "    rfs.append(rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7097564711288099"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# predict\n",
    "ps = []\n",
    "for _rf in rfs:\n",
    "    p = _rf.predict_proba(_valid_x3)[:,1].reshape(-1,1)\n",
    "    ps.append(p)\n",
    "p = np.hstack(ps)\n",
    "p = np.argmax(p,axis=1)\n",
    "acc(lb.classes_.take(p),valid_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "negetive_num = 3\n",
    "rfs2 = []\n",
    "kf = KFold(n_splits=negetive_num)\n",
    "for _i in range(_train_b_y.shape[1]):\n",
    "    _y = _train_b_y[:,_i]\n",
    "    _y_pos = _y[_y==1]\n",
    "    _x_pos = _train_x3[_y==1]\n",
    "    _y_neg = _y[_y!=1]\n",
    "    _x_neg = _train_x3[_y!=1]\n",
    "    _rfs = []\n",
    "    for _, _test_index in kf.split(_x_neg):\n",
    "        rf = RandomForestClassifier(n_jobs=-1,n_estimators=188,class_weight=\"balanced\")\n",
    "        rf.fit(np.concatenate([_x_pos,_x_neg[_test_index]],axis=0),\n",
    "              np.concatenate([_y_pos,_y_neg[_test_index]],axis=0))\n",
    "        _rfs.append(rf)\n",
    "    rfs2.append(_rfs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.69980088834431"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# predict\n",
    "ps = []\n",
    "for _rfs in rfs2:\n",
    "    _ps = []\n",
    "    for _rf in _rfs:\n",
    "        p = _rf.predict_proba(_valid_x3)[:,1].reshape(-1,1)\n",
    "        _ps.append(p)\n",
    "    p = np.mean(_ps,axis=0)\n",
    "    ps.append(p)\n",
    "p = np.hstack(ps)\n",
    "p = np.argmax(p,axis=1)\n",
    "acc(lb.classes_.take(p),valid_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfs3 = []\n",
    "validxs = []\n",
    "for _i in range(_train_b_y.shape[1]):\n",
    "    print _i\n",
    "    _v =[]\n",
    "    _rfs = []\n",
    "    for _sp in [2,5,10]:\n",
    "        indexs = choose_strong_wifi_index(-115,_sp,train_wifi_all_x)\n",
    "        _train_x_sp = np.concatenate([train_wifi_all_x[:,indexs],train_lonlats,train_wh],axis=1)\n",
    "        _v.append(np.concatenate([valid_wifi_all_x[:,indexs],valid_lonlats,valid_wh],axis=1))\n",
    "        rf = RandomForestClassifier(n_jobs=-1,n_estimators=188,class_weight=\"balanced\")\n",
    "        rf.fit(_train_x_sp,_train_b_y[:, _i])\n",
    "        _rfs.append(rf)\n",
    "    validxs.append(_v)\n",
    "    rfs3.append(_rfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.710522285189156"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#predict \n",
    "ps = []\n",
    "for _rfs,_vx in zip(rfs3,validxs):\n",
    "    _ps = []\n",
    "    for _rf,_x in zip(_rfs,_vx):\n",
    "        p = _rf.predict_proba(_x)[:,1].reshape(-1,1)\n",
    "        _ps.append(p)\n",
    "    p = np.mean(_ps,axis=0)\n",
    "    ps.append(p)\n",
    "p = np.hstack(ps)\n",
    "p = np.argmax(p,axis=1)\n",
    "acc(lb.classes_.take(p),valid_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfs3 = []\n",
    "validxs = []\n",
    "for _i in range(_train_b_y.shape[1]):\n",
    "    print _i\n",
    "    _v =[]\n",
    "    _rfs = []\n",
    "    for _sp in [2,5,10]:\n",
    "        indexs = choose_strong_wifi_index(-115,_sp,train_wifi_all_x)\n",
    "        _train_x_sp = np.concatenate([train_wifi_all_x[:,indexs],train_lonlats,train_wh],axis=1)\n",
    "        _v.append(np.concatenate([valid_wifi_all_x[:,indexs],valid_lonlats,valid_wh],axis=1))\n",
    "        rf = RandomForestClassifier(n_jobs=-1,n_estimators=188,class_weight=\"balanced\")\n",
    "        rf.fit(_train_x_sp,_train_b_y[:, _i])\n",
    "        _rfs.append(rf)\n",
    "    validxs.append(_v)\n",
    "    rfs3.append(_rfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8731856378915203"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rfs4 = []\n",
    "validxs =[]\n",
    "for _sp in [2,6,10]:\n",
    "    indexs = choose_strong_wifi_index(-90,_sp,train_wifi_all_x)\n",
    "    _train_x_sp = np.concatenate([train_wifi_all_x[:,indexs],train_lonlats,train_wh],axis=1)\n",
    "    validxs.append(np.concatenate([valid_wifi_all_x[:,indexs],valid_lonlats,valid_wh],axis=1))\n",
    "    rf = RandomForestClassifier(n_jobs=-1,n_estimators=500,class_weight=\"balanced\")\n",
    "    rf.fit(_train_x_sp,train_y)\n",
    "    rfs4.append(rf)\n",
    "ps = []\n",
    "for _rfs,_vx in zip(rfs4, validxs):\n",
    "    p = _rfs.predict_proba(_vx)\n",
    "    ps.append(p)\n",
    "p = np.mean(ps,axis=0)\n",
    "acc(rfs4[0].classes_.take(np.argmax(p,axis=1)),valid_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CVEstimator(object):\n",
    "    def __expansion(trainx, trainy, cv):\n",
    "        # 对样本少的进行复制扩充\n",
    "        bin = np.bincount(trainy)\n",
    "        labels = np.unique(trainy)\n",
    "        l = np.asarray(range(np.max(labels) + 1))[bin < cv]\n",
    "        l = np.intersect1d(l, labels)\n",
    "        for _l in l:\n",
    "            n = (trainy == _l).sum()\n",
    "            n = int(np.ceil(float(cv) / n - 1))\n",
    "            trainx = np.concatenate([trainx, np.tile(trainx[trainy == _l], (n, 1))], axis=0)\n",
    "            trainy = np.concatenate([trainy, np.tile(trainy[trainy == _l], (n,))], axis=0)\n",
    "        return trainx, trainy\n",
    "    \n",
    "    def __init__(self, estimator, cv = 3, use_proba = True):\n",
    "        self.estimator = estimator\n",
    "        self.cv = cv\n",
    "        self.kf = StratifiedKFold(cv,shuffle=True)\n",
    "        from sklearn.base import clone\n",
    "        self.clfs_ = [clone(self.estimator) for _ in range(self.cv)]\n",
    "        self.use_proba = use_proba\n",
    "        self.classes_ = None\n",
    "        \n",
    "        \n",
    "    def fit(self, X,y):\n",
    "#         if np.unique(y) > 2:\n",
    "#             self.multiclass = True\n",
    "#         else:\n",
    "#             self.multiclass = False\n",
    "        origin_size = y.shape[0]\n",
    "        _x,_y = expansion(X,y,self.cv)\n",
    "        self.indexs = []\n",
    "        self.ys = []\n",
    "        self.train_predicts=[]\n",
    "        for _i,(_train_index,_test_index) in enumerate(self.kf.split(_x,_y)):\n",
    "            _train_x = _x[_train_index]\n",
    "            _train_y = _y[_train_index]\n",
    "            _test_x = _x[_test_index]\n",
    "            _test_y = _y[_test_index]\n",
    "            self.clfs_[_i].fit(_train_x, _train_y)\n",
    "            self.indexs.append(_test_index)\n",
    "            self.ys.append(_test_y)\n",
    "            if self.use_proba:\n",
    "                self.train_predicts.append(self.clfs_[_i].predict_proba(_test_x))\n",
    "            else:\n",
    "                self.train_predicts.append(self.clfs_[_i].predict(_test_x))\n",
    "            if self.classes_  is None:\n",
    "                self.classes_ = self.clfs_[_i].classes_\n",
    "            else:\n",
    "                assert((self.classes_ != self.clfs_[_i].classes_).sum()==0)\n",
    "        self.indexs = np.concatenate(self.indexs, axis=0)\n",
    "        self.train_predicts = np.concatenate(self.train_predicts, axis=0)\n",
    "        self.ys = np.concatenate(self.ys,axis=0)\n",
    "        all_train_predicts = zip(self.indexs,self.ys,self.train_predicts)\n",
    "        all_train_predicts = sorted(all_train_predicts, key=lambda x: x[0])\n",
    "        all_train_predicts = all_train_predicts[:origin_size]\n",
    "        self.train_ys = np.asarray([_l[1] for _l in all_train_predicts])\n",
    "        self.train_predicts = np.asarray([_l[2] for _l in all_train_predicts])\n",
    "        \n",
    "        \n",
    "    def get_all_train_predicts(self):\n",
    "        return self.train_predicts, self.train_ys\n",
    "        \n",
    "    def predict(self,X):\n",
    "        px = self.predict_proba(X)\n",
    "        p = np.argmax(px,axis=1)\n",
    "        return self.classes_.take(p)\n",
    "        \n",
    "        \n",
    "    def predict_proba(self,X):\n",
    "        _y = []\n",
    "        for _cls in self.clfs_:\n",
    "            _y.append(_cls.predict_proba(X))\n",
    "        p = np.max(_y,axis=0)\n",
    "        return p\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8708938120702827"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cvrf = CVEstimator(RandomForestClassifier(n_jobs=-1,n_estimators=500,class_weight=\"balanced\"), cv=3, use_proba=True)\n",
    "cvrf.fit(_train_x3, train_y)\n",
    "cvrf.predict(_valid_x3)\n",
    "acc(cvrf.predict(_valid_x3),valid_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8624904507257448"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StackingCVEstimator(object):\n",
    "    def __init__(self, base_cv_estimators, meta_estimator,use_proba = True,use_feature_in_secondary=False):\n",
    "        self.base_cv_estimators = base_cv_estimators\n",
    "        for _cls in self.base_cv_estimators:\n",
    "            _cls.use_proba = use_proba\n",
    "        self.meta_estimator = meta_estimator\n",
    "        self.use_proba = use_proba\n",
    "        self.use_feature_in_secondary = use_feature_in_secondary\n",
    "        self.x_num = 1\n",
    "    \n",
    "    \n",
    "    def __check(self,xs):\n",
    "        if isinstance(xs, np.ndarray):\n",
    "            xs = [xs]\n",
    "        if isinstance(xs,list):\n",
    "            if(len(xs) == 1):\n",
    "                _xs =xs[0]\n",
    "                xs = [_xs for _ in range(len(self.base_cv_estimators))]\n",
    "                self.x_num = 1\n",
    "            else:\n",
    "                self.x_num = len(xs)\n",
    "        return xs\n",
    "        \n",
    "    def fit(self,xs,y):\n",
    "        xs = self.__check(xs)\n",
    "        if not self.use_feature_in_secondary:\n",
    "            self.x_num = 1\n",
    "        if self.x_num == 1:\n",
    "            self.meta_estimetors = None\n",
    "        else:\n",
    "            from sklearn.base import clone\n",
    "            self.meta_estimetors = [clone(self.meta_estimator) for _ in range(len(xs))]\n",
    "                \n",
    "        \n",
    "        \n",
    "        for _xs,_cve in zip(xs, self.base_cv_estimators):\n",
    "            _cve.fit(_xs,y)\n",
    "        \n",
    "        # meta predict\n",
    "        newxs = []\n",
    "        for _cve in self.base_cv_estimators:\n",
    "            _newx = _cve.get_all_train_predicts()[0]\n",
    "            if self.use_proba and _newx.shape[1] == 2:\n",
    "                _newx = _newx[:,1].reshape(-1,1)\n",
    "            newxs.append(_newx)\n",
    "        _newx = np.concatenate(newxs,axis=1)\n",
    "        if self.meta_estimetors is None:\n",
    "            if self.use_feature_in_secondary:\n",
    "                newx = np.concatenate([_newx,xs[0]],axis=1)\n",
    "            else:\n",
    "                newx = _newx\n",
    "            self.meta_estimator.fit(newx,y)\n",
    "            self.classes_ = self.meta_estimator.classes_\n",
    "        else:\n",
    "            for _xs,_meta_cls in zip(xs,self.meta_estimetors):\n",
    "                newx = np.concatenate([_newx,_xs],axis=1)\n",
    "                _meta_cls.fit(newx,y)\n",
    "            self.classes_ = self.meta_estimators[0].classes_\n",
    "    \n",
    "        \n",
    "    def predict(self,Xs):\n",
    "        return self.classes_.take(np.argmax(self.predict_proba(Xs),axis=1))\n",
    "    \n",
    "    def predict_proba(self, Xs):\n",
    "        ps = []\n",
    "        Xs = self.__check(Xs)\n",
    "        for _cls,_xs in zip(self.base_cv_estimators,Xs):\n",
    "            p = _cls.predict_proba(_xs)\n",
    "            if self.use_proba and p.shape[1] == 2:\n",
    "                p = p[:,1].reshape(-1,1)\n",
    "            ps.append(p)\n",
    "        ps = np.concatenate(ps, axis=1)\n",
    "        if self.meta_estimetors is None:\n",
    "            if self.use_feature_in_secondary:\n",
    "                newx = np.concatenate([ps,Xs[0]],axis=1)\n",
    "            else:\n",
    "                newx = ps\n",
    "            p = self.meta_estimator.predict_proba(newx)\n",
    "        else:\n",
    "            pass\n",
    "        \n",
    "        return p\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9955582784499923"
      ]
     },
     "execution_count": 382,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i = 11\n",
    "rf = RandomForestClassifier(n_jobs=-1,n_estimators=188,class_weight=\"balanced\")\n",
    "rf.fit(_train_x3, _train_b_y[:,i])\n",
    "acc(rf.predict(_valid_x3),_valid_b_y[:,i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainxs = []\n",
    "validxs = []\n",
    "for _sp in [2,6,10]:\n",
    "    indexs = choose_strong_wifi_index(-90,_sp,train_wifi_all_x)\n",
    "    _train_x_sp = np.concatenate([train_wifi_all_x[:,indexs],train_lonlats,train_wh],axis=1)\n",
    "    trainxs.append(_train_x_sp)\n",
    "    validxs.append(np.concatenate([valid_wifi_all_x[:,indexs],valid_lonlats,valid_wh],axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9834584162965232"
      ]
     },
     "execution_count": 383,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scve = StackingCVEstimator([CVEstimator(RandomForestClassifier(n_jobs=-1,n_estimators=188,class_weight=\"balanced\")),\n",
    "                            CVEstimator(ExtraTreesClassifier(n_jobs=-1,n_estimators=188,class_weight=\"balanced\")),\n",
    "                           ],\n",
    "                           LogisticRegression(C=10,class_weight=\"balanced\"),\n",
    "                           use_feature_in_secondary=False)\n",
    "scve.fit(trainxs,_train_b_y[:,i])\n",
    "acc(scve.predict(validxs),_valid_b_y[:,i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8724216959511077"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rfs4 = []\n",
    "validxs =[]\n",
    "for _sp in [2,6,10]:\n",
    "    indexs = choose_strong_wifi_index(-90,_sp,train_wifi_all_x)\n",
    "    _train_x_sp = np.concatenate([train_wifi_all_x[:,indexs],train_lonlats,train_wh],axis=1)\n",
    "    validxs.append(np.concatenate([valid_wifi_all_x[:,indexs],valid_lonlats,valid_wh],axis=1))\n",
    "    rf = CVEstimator(RandomForestClassifier(n_jobs=-1,n_estimators=500,class_weight=\"balanced\"))\n",
    "    rf.fit(_train_x_sp,train_y)\n",
    "    rfs4.append(rf)\n",
    "ps = []\n",
    "for _rfs,_vx in zip(rfs4, validxs):\n",
    "    p = _rfs.predict_proba(_vx)\n",
    "    ps.append(p)\n",
    "p = np.max(ps,axis=0)\n",
    "acc(rfs4[0].classes_.take(np.argmax(p,axis=1)),valid_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 classifiers...\n",
      "Fitting classifier1: randomforestclassifier (1/4)\n",
      "Training and fitting fold 1 of 3...\n",
      "Training and fitting fold 2 of 3...\n",
      "Training and fitting fold 3 of 3...\n",
      "Fitting classifier2: randomforestclassifier (2/4)\n",
      "Training and fitting fold 1 of 3...\n",
      "Training and fitting fold 2 of 3...\n",
      "Training and fitting fold 3 of 3...\n",
      "Fitting classifier3: randomforestclassifier (3/4)\n",
      "Training and fitting fold 1 of 3...\n",
      "Training and fitting fold 2 of 3...\n",
      "Training and fitting fold 3 of 3...\n",
      "Fitting classifier4: randomforestclassifier (4/4)\n",
      "Training and fitting fold 1 of 3...\n",
      "Training and fitting fold 2 of 3...\n",
      "Training and fitting fold 3 of 3...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7106754480012253"
      ]
     },
     "execution_count": 342,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#stack\n",
    "cv = 3\n",
    "_x,_y = expansion(_train_x3,train_y,cv)\n",
    "stack = StackingCVClassifier([RandomForestClassifier(n_jobs=-1,n_estimators=188,class_weight=\"balanced\"),\n",
    "                              RandomForestClassifier(n_jobs=-1,n_estimators=188,class_weight=\"balanced\",min_weight_fraction_leaf=0.00002),\n",
    "                              ],\n",
    "                               RandomForestClassifier(n_jobs=-1,n_estimators=888,class_weight=\"balanced\"),\n",
    "                             use_probas=True,\n",
    "                             verbose=1,\n",
    "                             use_features_in_secondary = True,\n",
    "                             cv = cv) \n",
    "stack.fit(_x,_y)\n",
    "p = stack.predict(_valid_x3)\n",
    "acc(p, valid_y)\n",
    "# OneVsRestClassifier(estimator=RandomForestClassifier(n_estimators=188,n_jobs=-1,class_weight=\"balanced\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7030173073977638"
      ]
     },
     "execution_count": 330,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf1 = RandomForestClassifier(n_jobs=-1,n_estimators=188,class_weight=\"balanced\",min_weight_fraction_leaf=0.00002)\n",
    "rf1.fit(_train_x3,train_y)\n",
    "p = rf1.predict(_valid_x3)\n",
    "acc(p, valid_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7057742380150099"
      ]
     },
     "execution_count": 331,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv1 = CVEstimator(RandomForestClassifier(n_jobs=-1,n_estimators=188,class_weight=\"balanced\",min_weight_fraction_leaf=0.00002))\n",
    "cv1.fit(_train_x3,train_y)\n",
    "p = cv1.predict(_valid_x3)\n",
    "acc(p, valid_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7025578189615561"
      ]
     },
     "execution_count": 335,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv2 = CVEstimator(RandomForestClassifier(n_jobs=-1,n_estimators=188,class_weight=\"balanced\",min_weight_fraction_leaf=0.00002))\n",
    "cv2.fit(_train_x3,train_y)\n",
    "p = cv2.predict(_valid_x3)\n",
    "acc(p, valid_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6639607903201102"
      ]
     },
     "execution_count": 336,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv3 = CVEstimator(ExtraTreesClassifier(n_jobs=-1,n_estimators=188,class_weight=\"balanced\",min_weight_fraction_leaf=0.00002))\n",
    "cv3.fit(_train_x3,train_y)\n",
    "p = cv3.predict(_valid_x3)\n",
    "acc(p, valid_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [],
   "source": [
    "newx = np.concatenate([cv1.get_all_train_predicts()[0],\n",
    "                       cv2.get_all_train_predicts()[0]],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7073058661357022"
      ]
     },
     "execution_count": 338,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l = LogisticRegression(class_weight=\"balanced\", n_jobs=-1,C=30)\n",
    "l.fit(newx,train_y)\n",
    "p1 = cv1.predict_proba(_valid_x3)\n",
    "p2 = cv2.predict_proba(_valid_x3)\n",
    "newvalidx = np.concatenate([p1,p2],axis=1)\n",
    "p = l.predict(newvalidx)\n",
    "acc(p,valid_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = RandomForestClassifier(class_weight=\"balanced\", n_jobs=-1,n_estimators=500)\n",
    "l.fit(newx,train_y)\n",
    "p1 = cv1.predict_proba(_valid_x3)\n",
    "p2 = cv2.predict_proba(_valid_x3)\n",
    "newvalidx = np.concatenate([p1,p2],axis=1)\n",
    "p = l.predict(newvalidx)\n",
    "acc(p,valid_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "52,55': 30class Stacking(object):\n",
    "\n",
    "    def __init__(self, base_clses,\n",
    "                 meta_cls,\n",
    "                 use_prob=False,\n",
    "                 kfold=-1,\n",
    "                 stratify=True,\n",
    "                 Kfold_shuffle=False,\n",
    "                 num_class = None,\n",
    "                 use_features_in_secondary = False):\n",
    "        \"\"\"\n",
    "\n",
    "        :param base_clses: sklearn model 或者lightgbm lgb的话存入tuple(\"lgb\", params)\n",
    "        :param meta_cls:\n",
    "        :param objective:\n",
    "        \"\"\"\n",
    "        self.base_clses = base_clses\n",
    "        self.meta_cls = meta_cls\n",
    "        self.spetial_base_clses = {}\n",
    "        self.use_prob = use_prob\n",
    "        self.kfold = kfold\n",
    "        self.Kfold_shuffle = Kfold_shuffle\n",
    "        self.stratify = stratify\n",
    "        self.num_class = num_class\n",
    "        self.use_feature_in_secondary = use_features_in_secondary\n",
    "        if self.kfold > 0:\n",
    "            self.cv = True\n",
    "            self.sklearn_models = {}\n",
    "            self.spetial_base_clses_params = {}\n",
    "            for _cls in base_clses:\n",
    "                if _cls[0].startswith(\"lgb\"):\n",
    "                    for _i in range(self.kfold):\n",
    "                        lgb_name = _cls[0] + \"_\" + str(_i)\n",
    "                        self.spetial_base_clses_params[lgb_name] = (lgb_name, _cls[1].copy())\n",
    "                elif _cls[0].startswith(\"xgb\"):\n",
    "                    for _i in range(self.kfold):\n",
    "                        xgb_name = _cls[0] + \"_\" + str(_i)\n",
    "                        self.spetial_base_clses_params[xgb_name] = (xgb_name, _cls[1].copy())\n",
    "                else:\n",
    "                    base_name = _cls[0]\n",
    "                    for _i in range(self.kfold):\n",
    "                        self.sklearn_models[base_name + \"_{}\".format(_i)] = clone(_cls[1])\n",
    "\n",
    "\n",
    "        else:\n",
    "            self.cv = False\n",
    "\n",
    "\n",
    "    def _lgb_train(self, _cls, train_x, train_y, valid_x=None, valid_y=None):\n",
    "        _train = lgb.Dataset(train_x, label=train_y)\n",
    "        _valid = None\n",
    "        if valid_x is not None:\n",
    "            _valid = lgb.Dataset(valid_x, label=valid_y, reference=_train)\n",
    "        if _valid is None:\n",
    "            bst = lgb.train(_cls[1],\n",
    "                            _train)\n",
    "        else:\n",
    "            bst = lgb.train(_cls[1],\n",
    "                            _train,\n",
    "                            valid_sets=_valid)\n",
    "        assert _cls[0] not in self.spetial_base_clses.keys()\n",
    "        self.spetial_base_clses[_cls[0]] = bst\n",
    "        return self._lgb_predict(_cls[0], train_x, self.use_prob)\n",
    "\n",
    "    def _lgb_predict(self, _cls_name, x, use_proba=False):\n",
    "        bst = self.spetial_base_clses[_cls_name]\n",
    "        if use_proba:\n",
    "            p = bst.predict(x, bst.best_iteration, raw_score = True)\n",
    "        else:\n",
    "            p = np.argmax(bst.predict(x, bst.best_iteration), axis=1).astype(int).reshape((-1, 1))\n",
    "        return p\n",
    "\n",
    "    def _sklearn_predict(self, model, x, use_proba=False):\n",
    "        if use_proba:\n",
    "            p = model.predict_proba(x)\n",
    "        else:\n",
    "            p = model.predict(x).reshape((-1, 1))\n",
    "\n",
    "        if use_proba and self.num_class is not None:#针对没有出现的类别导致的feature不一样\n",
    "            sample_size = p.shape[0]\n",
    "            clss = model.classes_\n",
    "            p = dict(zip(clss, p.transpose()))\n",
    "            for _i in range(self.num_class):\n",
    "                if _i not in p.keys():\n",
    "                    p[_i] = np.zeros((sample_size,))\n",
    "            p = np.vstack(p.values()).transpose()\n",
    "        return p\n",
    "\n",
    "    def fit(self, train_x, train_y):\n",
    "        self.split = check_cv(self.kfold, train_y, self.stratify)\n",
    "        self.split.shuffle = self.Kfold_shuffle\n",
    "        p_rs = []\n",
    "        v_rs = []\n",
    "        if not self.cv:\n",
    "            for _cls in self.base_clses:\n",
    "                if _cls[0].startswith(\"lgb\"):\n",
    "                    p = self._lgb_train(_cls, train_x, train_y)\n",
    "                    p_rs.append(p)\n",
    "                else:\n",
    "                    _cls[1].fit(train_x, train_y)\n",
    "                    p = self._sklearn_predict(_cls[1], train_x, self.use_prob)\n",
    "                    p_rs.append(p)\n",
    "\n",
    "        else:\n",
    "            for _cls in self.base_clses:\n",
    "                p_part = []\n",
    "                y_index = []\n",
    "                v_part = []\n",
    "                if _cls[0].startswith(\"lgb\"):  # lgb or xgb\n",
    "                    for _i, (_train_index, _test_index) in enumerate(self.split.split(train_x, train_y)):\n",
    "                        _train_x = train_x[_train_index]\n",
    "                        _train_y = train_y[_train_index]\n",
    "                        _test_x = train_x[_test_index]\n",
    "                        _test_y = train_y[_test_index]\n",
    "                        lgb_name = _cls[0] + \"_\" + str(_i)\n",
    "                        self._lgb_train(self.spetial_base_clses_params[lgb_name],\n",
    "                                        _train_x,\n",
    "                                        _train_y,\n",
    "                                        _test_x,\n",
    "                                        _test_y)\n",
    "                        p_part.append(self._lgb_predict(lgb_name, _test_x, self.use_prob))\n",
    "                        y_index.append(_test_index.reshape((-1, 1)))\n",
    "\n",
    "                else:  # sklearn\n",
    "                    for _i, (_train_index, _test_index) in enumerate(self.split.split(train_x, train_y)):\n",
    "                        _train_x = train_x[_train_index]\n",
    "                        _train_y = train_y[_train_index]\n",
    "                        _test_x = train_x[_test_index]\n",
    "                        name = _cls[0]\n",
    "                        model = self.sklearn_models[name + \"_\" + str(_i)]\n",
    "                        model.fit(_train_x, _train_y)\n",
    "                        p = self._sklearn_predict(model, _test_x, self.use_prob)\n",
    "                        p_part.append(p)\n",
    "                        y_index.append(_test_index.reshape((-1, 1)))\n",
    "                p_part = np.vstack(p_part)\n",
    "                y_index = np.vstack(y_index)\n",
    "                p = dict(zip(list(y_index.reshape((-1,))), list(p_part)))\n",
    "                p_rs.append(np.vstack(p.values()))\n",
    "\n",
    "        new_train = np.hstack(p_rs)\n",
    "\n",
    "        if self.use_feature_in_secondary:\n",
    "            new_train = np.concatenate([new_train, train_x],axis=1)\n",
    "\n",
    "\n",
    "        assert new_train.shape[0] == train_y.shape[0]\n",
    "\n",
    "        # 次学习其\n",
    "        # if self.meta_cls[0].startswith(\"lgb\"):\n",
    "        #     self._lgb_train(self.meta_cls, new_train, train_y)\n",
    "        # else:\n",
    "        self.meta_cls[1].fit(new_train, train_y)\n",
    "\n",
    "    def predict(self, test_x):\n",
    "        p_rs = []\n",
    "        if not self.cv:\n",
    "            for _cls in self.base_clses:\n",
    "                if _cls[0].startswith(\"lgb\"):\n",
    "                    p = self._lgb_predict(_cls[0], test_x, self.use_prob)\n",
    "                    p_rs.append(p)\n",
    "                else:\n",
    "                    p = self._sklearn_predict(_cls[1], test_x, self.use_prob)\n",
    "                    p_rs.append(p)\n",
    "\n",
    "        else:\n",
    "            for _cls in self.base_clses:\n",
    "                p_part = []\n",
    "                if _cls[0].startswith(\"lgb\"):  # lgb or xgb\n",
    "                    for _i in range(self.kfold):\n",
    "                        lgb_name = _cls[0] + \"_\" + str(_i)\n",
    "                        p_part.append(self._lgb_predict(lgb_name, test_x, self.use_prob))\n",
    "\n",
    "                else:  # sklearn\n",
    "                    for _i in range(self.kfold):\n",
    "                        name = _cls[0]\n",
    "                        model = self.sklearn_models[name + \"_\" + str(_i)]\n",
    "                        p = self._sklearn_predict(model, test_x, self.use_prob)\n",
    "                        p_part.append(p)\n",
    "                p_rs.append(np.mean(np.stack(p_part, axis=2), axis=2))\n",
    "\n",
    "        new_train = np.hstack(p_rs)\n",
    "        if self.use_feature_in_secondary:\n",
    "            new_train = np.concatenate([new_train, test_x],axis=1)\n",
    "        # 次学习器\n",
    "        if self.meta_cls[0].startswith(\"lgb\"):\n",
    "            p = self._lgb_predict(self.meta_cls[0], new_train)\n",
    "        else:\n",
    "            p = self._sklearn_predict(self.meta_cls[1], new_train)\n",
    "        return p.reshape((-1,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\tvalid_0's multi_error: 0.369015\n",
      "Training until validation scores don't improve for 15 rounds.\n",
      "[2]\tvalid_0's multi_error: 0.34598\n",
      "[3]\tvalid_0's multi_error: 0.331828\n",
      "[4]\tvalid_0's multi_error: 0.325053\n",
      "[5]\tvalid_0's multi_error: 0.321138\n",
      "[6]\tvalid_0's multi_error: 0.317977\n",
      "[7]\tvalid_0's multi_error: 0.317675\n",
      "[8]\tvalid_0's multi_error: 0.314965\n",
      "[9]\tvalid_0's multi_error: 0.315266\n",
      "[10]\tvalid_0's multi_error: 0.312858\n",
      "[11]\tvalid_0's multi_error: 0.310599\n",
      "[12]\tvalid_0's multi_error: 0.309244\n",
      "[13]\tvalid_0's multi_error: 0.309395\n",
      "[14]\tvalid_0's multi_error: 0.308491\n",
      "[15]\tvalid_0's multi_error: 0.306534\n",
      "[16]\tvalid_0's multi_error: 0.30804\n",
      "[17]\tvalid_0's multi_error: 0.307136\n",
      "[18]\tvalid_0's multi_error: 0.30533\n",
      "[19]\tvalid_0's multi_error: 0.30533\n",
      "[20]\tvalid_0's multi_error: 0.305781\n",
      "[21]\tvalid_0's multi_error: 0.304878\n",
      "[22]\tvalid_0's multi_error: 0.304276\n",
      "[23]\tvalid_0's multi_error: 0.304426\n",
      "[24]\tvalid_0's multi_error: 0.303674\n",
      "[25]\tvalid_0's multi_error: 0.304426\n",
      "[26]\tvalid_0's multi_error: 0.304276\n",
      "[27]\tvalid_0's multi_error: 0.30533\n",
      "[28]\tvalid_0's multi_error: 0.303674\n",
      "[29]\tvalid_0's multi_error: 0.304426\n",
      "[30]\tvalid_0's multi_error: 0.303071\n",
      "[31]\tvalid_0's multi_error: 0.302469\n",
      "[32]\tvalid_0's multi_error: 0.302168\n",
      "[33]\tvalid_0's multi_error: 0.302319\n",
      "[34]\tvalid_0's multi_error: 0.301265\n",
      "[35]\tvalid_0's multi_error: 0.301265\n",
      "[36]\tvalid_0's multi_error: 0.301415\n",
      "[37]\tvalid_0's multi_error: 0.300813\n",
      "[38]\tvalid_0's multi_error: 0.300361\n",
      "[39]\tvalid_0's multi_error: 0.300211\n",
      "[40]\tvalid_0's multi_error: 0.299006\n",
      "[41]\tvalid_0's multi_error: 0.298705\n",
      "[42]\tvalid_0's multi_error: 0.299006\n",
      "[43]\tvalid_0's multi_error: 0.29991\n",
      "[44]\tvalid_0's multi_error: 0.30006\n",
      "[45]\tvalid_0's multi_error: 0.29991\n",
      "[46]\tvalid_0's multi_error: 0.298555\n",
      "[47]\tvalid_0's multi_error: 0.298103\n",
      "[48]\tvalid_0's multi_error: 0.298254\n",
      "[49]\tvalid_0's multi_error: 0.2972\n",
      "[50]\tvalid_0's multi_error: 0.297651\n",
      "[51]\tvalid_0's multi_error: 0.296899\n",
      "[52]\tvalid_0's multi_error: 0.296748\n",
      "[53]\tvalid_0's multi_error: 0.297501\n",
      "[54]\tvalid_0's multi_error: 0.297049\n",
      "[55]\tvalid_0's multi_error: 0.296899\n",
      "[56]\tvalid_0's multi_error: 0.296447\n",
      "[57]\tvalid_0's multi_error: 0.295995\n",
      "[58]\tvalid_0's multi_error: 0.2972\n",
      "[59]\tvalid_0's multi_error: 0.296748\n",
      "[60]\tvalid_0's multi_error: 0.297049\n",
      "[61]\tvalid_0's multi_error: 0.295995\n",
      "[62]\tvalid_0's multi_error: 0.295694\n",
      "[63]\tvalid_0's multi_error: 0.295242\n",
      "[64]\tvalid_0's multi_error: 0.295544\n",
      "[65]\tvalid_0's multi_error: 0.295694\n",
      "[66]\tvalid_0's multi_error: 0.294791\n",
      "[67]\tvalid_0's multi_error: 0.294941\n",
      "[68]\tvalid_0's multi_error: 0.294339\n",
      "[69]\tvalid_0's multi_error: 0.293436\n",
      "[70]\tvalid_0's multi_error: 0.293586\n",
      "[71]\tvalid_0's multi_error: 0.293436\n",
      "[72]\tvalid_0's multi_error: 0.293135\n",
      "[73]\tvalid_0's multi_error: 0.293586\n",
      "[74]\tvalid_0's multi_error: 0.293887\n",
      "[75]\tvalid_0's multi_error: 0.293887\n",
      "[76]\tvalid_0's multi_error: 0.293586\n",
      "[77]\tvalid_0's multi_error: 0.293436\n",
      "[78]\tvalid_0's multi_error: 0.293586\n",
      "[79]\tvalid_0's multi_error: 0.293436\n",
      "[80]\tvalid_0's multi_error: 0.294038\n",
      "[81]\tvalid_0's multi_error: 0.293586\n",
      "[82]\tvalid_0's multi_error: 0.293887\n",
      "[83]\tvalid_0's multi_error: 0.294188\n",
      "[84]\tvalid_0's multi_error: 0.293737\n",
      "[85]\tvalid_0's multi_error: 0.293285\n",
      "[86]\tvalid_0's multi_error: 0.293436\n",
      "[87]\tvalid_0's multi_error: 0.292984\n",
      "[88]\tvalid_0's multi_error: 0.292382\n",
      "[89]\tvalid_0's multi_error: 0.292081\n",
      "[90]\tvalid_0's multi_error: 0.292683\n",
      "[91]\tvalid_0's multi_error: 0.292081\n",
      "[92]\tvalid_0's multi_error: 0.29193\n",
      "[93]\tvalid_0's multi_error: 0.292833\n",
      "[94]\tvalid_0's multi_error: 0.292081\n",
      "[95]\tvalid_0's multi_error: 0.291478\n",
      "[96]\tvalid_0's multi_error: 0.29193\n",
      "[97]\tvalid_0's multi_error: 0.291629\n",
      "[98]\tvalid_0's multi_error: 0.291478\n",
      "[99]\tvalid_0's multi_error: 0.291027\n",
      "[100]\tvalid_0's multi_error: 0.291027\n",
      "[101]\tvalid_0's multi_error: 0.291328\n",
      "[102]\tvalid_0's multi_error: 0.291328\n",
      "[103]\tvalid_0's multi_error: 0.290876\n",
      "[104]\tvalid_0's multi_error: 0.290123\n",
      "[105]\tvalid_0's multi_error: 0.290425\n",
      "[106]\tvalid_0's multi_error: 0.290876\n",
      "[107]\tvalid_0's multi_error: 0.291027\n",
      "[108]\tvalid_0's multi_error: 0.290425\n",
      "[109]\tvalid_0's multi_error: 0.290274\n",
      "[110]\tvalid_0's multi_error: 0.289521\n",
      "[111]\tvalid_0's multi_error: 0.288768\n",
      "[112]\tvalid_0's multi_error: 0.28907\n",
      "[113]\tvalid_0's multi_error: 0.289521\n",
      "[114]\tvalid_0's multi_error: 0.289521\n",
      "[115]\tvalid_0's multi_error: 0.28907\n",
      "[116]\tvalid_0's multi_error: 0.28922\n",
      "[117]\tvalid_0's multi_error: 0.28907\n",
      "[118]\tvalid_0's multi_error: 0.288768\n",
      "[119]\tvalid_0's multi_error: 0.288618\n",
      "[120]\tvalid_0's multi_error: 0.28922\n",
      "[121]\tvalid_0's multi_error: 0.288919\n",
      "[122]\tvalid_0's multi_error: 0.288768\n",
      "[123]\tvalid_0's multi_error: 0.28907\n",
      "[124]\tvalid_0's multi_error: 0.288467\n",
      "[125]\tvalid_0's multi_error: 0.288768\n",
      "[126]\tvalid_0's multi_error: 0.287865\n",
      "[127]\tvalid_0's multi_error: 0.288317\n",
      "[128]\tvalid_0's multi_error: 0.288166\n",
      "[129]\tvalid_0's multi_error: 0.288467\n",
      "[130]\tvalid_0's multi_error: 0.287715\n",
      "[131]\tvalid_0's multi_error: 0.287564\n",
      "[132]\tvalid_0's multi_error: 0.287715\n",
      "[133]\tvalid_0's multi_error: 0.288768\n",
      "[134]\tvalid_0's multi_error: 0.288016\n",
      "[135]\tvalid_0's multi_error: 0.288016\n",
      "[136]\tvalid_0's multi_error: 0.287865\n",
      "[137]\tvalid_0's multi_error: 0.287564\n",
      "[138]\tvalid_0's multi_error: 0.287715\n",
      "[139]\tvalid_0's multi_error: 0.287112\n",
      "[140]\tvalid_0's multi_error: 0.287413\n",
      "[141]\tvalid_0's multi_error: 0.28636\n",
      "[142]\tvalid_0's multi_error: 0.28651\n",
      "[143]\tvalid_0's multi_error: 0.286811\n",
      "[144]\tvalid_0's multi_error: 0.28651\n",
      "[145]\tvalid_0's multi_error: 0.28651\n",
      "[146]\tvalid_0's multi_error: 0.286058\n",
      "[147]\tvalid_0's multi_error: 0.285757\n",
      "[148]\tvalid_0's multi_error: 0.285908\n",
      "[149]\tvalid_0's multi_error: 0.285908\n",
      "[150]\tvalid_0's multi_error: 0.285607\n",
      "[151]\tvalid_0's multi_error: 0.285908\n",
      "[152]\tvalid_0's multi_error: 0.285456\n",
      "[153]\tvalid_0's multi_error: 0.285306\n",
      "[154]\tvalid_0's multi_error: 0.285607\n",
      "[155]\tvalid_0's multi_error: 0.284854\n",
      "[156]\tvalid_0's multi_error: 0.285306\n",
      "[157]\tvalid_0's multi_error: 0.285908\n",
      "[158]\tvalid_0's multi_error: 0.285607\n",
      "[159]\tvalid_0's multi_error: 0.285155\n",
      "[160]\tvalid_0's multi_error: 0.285005\n",
      "[161]\tvalid_0's multi_error: 0.285005\n",
      "[162]\tvalid_0's multi_error: 0.285005\n",
      "[163]\tvalid_0's multi_error: 0.285456\n",
      "[164]\tvalid_0's multi_error: 0.285456\n",
      "[165]\tvalid_0's multi_error: 0.285757\n",
      "[166]\tvalid_0's multi_error: 0.285908\n",
      "[167]\tvalid_0's multi_error: 0.285607\n",
      "[168]\tvalid_0's multi_error: 0.286058\n",
      "[169]\tvalid_0's multi_error: 0.286058\n",
      "[170]\tvalid_0's multi_error: 0.285005\n",
      "Early stopping, best iteration is:\n",
      "[155]\tvalid_0's multi_error: 0.284854\n",
      "[1]\tvalid_0's multi_error: 0.370482\n",
      "Training until validation scores don't improve for 15 rounds.\n",
      "[2]\tvalid_0's multi_error: 0.346892\n",
      "[3]\tvalid_0's multi_error: 0.337215\n",
      "[4]\tvalid_0's multi_error: 0.330561\n",
      "[5]\tvalid_0's multi_error: 0.326781\n",
      "[6]\tvalid_0's multi_error: 0.320278\n",
      "[7]\tvalid_0's multi_error: 0.319069\n",
      "[8]\tvalid_0's multi_error: 0.3168\n",
      "[9]\tvalid_0's multi_error: 0.311961\n",
      "[10]\tvalid_0's multi_error: 0.31302\n",
      "[11]\tvalid_0's multi_error: 0.310147\n",
      "[12]\tvalid_0's multi_error: 0.31181\n",
      "[13]\tvalid_0's multi_error: 0.309693\n",
      "[14]\tvalid_0's multi_error: 0.309391\n",
      "[15]\tvalid_0's multi_error: 0.309391\n",
      "[16]\tvalid_0's multi_error: 0.307727\n",
      "[17]\tvalid_0's multi_error: 0.306064\n",
      "[18]\tvalid_0's multi_error: 0.305308\n",
      "[19]\tvalid_0's multi_error: 0.304854\n",
      "[20]\tvalid_0's multi_error: 0.303342\n",
      "[21]\tvalid_0's multi_error: 0.302132\n",
      "[22]\tvalid_0's multi_error: 0.30183\n",
      "[23]\tvalid_0's multi_error: 0.299108\n",
      "[24]\tvalid_0's multi_error: 0.298352\n",
      "[25]\tvalid_0's multi_error: 0.299259\n",
      "[26]\tvalid_0's multi_error: 0.298957\n",
      "[27]\tvalid_0's multi_error: 0.298957\n",
      "[28]\tvalid_0's multi_error: 0.298805\n",
      "[29]\tvalid_0's multi_error: 0.298352\n",
      "[30]\tvalid_0's multi_error: 0.299108\n",
      "[31]\tvalid_0's multi_error: 0.297898\n",
      "[32]\tvalid_0's multi_error: 0.297444\n",
      "[33]\tvalid_0's multi_error: 0.297747\n",
      "[34]\tvalid_0's multi_error: 0.29684\n",
      "[35]\tvalid_0's multi_error: 0.296537\n",
      "[36]\tvalid_0's multi_error: 0.296083\n",
      "[37]\tvalid_0's multi_error: 0.295176\n",
      "[38]\tvalid_0's multi_error: 0.295327\n",
      "[39]\tvalid_0's multi_error: 0.29442\n",
      "[40]\tvalid_0's multi_error: 0.29563\n",
      "[41]\tvalid_0's multi_error: 0.294571\n",
      "[42]\tvalid_0's multi_error: 0.294118\n",
      "[43]\tvalid_0's multi_error: 0.292908\n",
      "[44]\tvalid_0's multi_error: 0.292908\n",
      "[45]\tvalid_0's multi_error: 0.292908\n",
      "[46]\tvalid_0's multi_error: 0.292303\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[47]\tvalid_0's multi_error: 0.293059\n",
      "[48]\tvalid_0's multi_error: 0.292152\n",
      "[49]\tvalid_0's multi_error: 0.292454\n",
      "[50]\tvalid_0's multi_error: 0.293059\n",
      "[51]\tvalid_0's multi_error: 0.292757\n",
      "[52]\tvalid_0's multi_error: 0.292605\n",
      "[53]\tvalid_0's multi_error: 0.292605\n",
      "[54]\tvalid_0's multi_error: 0.292757\n",
      "[55]\tvalid_0's multi_error: 0.293059\n",
      "[56]\tvalid_0's multi_error: 0.292908\n",
      "[57]\tvalid_0's multi_error: 0.292605\n",
      "[58]\tvalid_0's multi_error: 0.291698\n",
      "[59]\tvalid_0's multi_error: 0.291093\n",
      "[60]\tvalid_0's multi_error: 0.291396\n",
      "[61]\tvalid_0's multi_error: 0.292303\n",
      "[62]\tvalid_0's multi_error: 0.292152\n",
      "[63]\tvalid_0's multi_error: 0.292303\n",
      "[64]\tvalid_0's multi_error: 0.292152\n",
      "[65]\tvalid_0's multi_error: 0.292454\n",
      "[66]\tvalid_0's multi_error: 0.291698\n",
      "[67]\tvalid_0's multi_error: 0.291396\n",
      "[68]\tvalid_0's multi_error: 0.290186\n",
      "[69]\tvalid_0's multi_error: 0.291093\n",
      "[70]\tvalid_0's multi_error: 0.289884\n",
      "[71]\tvalid_0's multi_error: 0.290337\n",
      "[72]\tvalid_0's multi_error: 0.288523\n",
      "[73]\tvalid_0's multi_error: 0.288825\n",
      "[74]\tvalid_0's multi_error: 0.288825\n",
      "[75]\tvalid_0's multi_error: 0.289279\n",
      "[76]\tvalid_0's multi_error: 0.288976\n",
      "[77]\tvalid_0's multi_error: 0.289884\n",
      "[78]\tvalid_0's multi_error: 0.289884\n",
      "[79]\tvalid_0's multi_error: 0.28943\n",
      "[80]\tvalid_0's multi_error: 0.289127\n",
      "[81]\tvalid_0's multi_error: 0.289732\n",
      "[82]\tvalid_0's multi_error: 0.28943\n",
      "[83]\tvalid_0's multi_error: 0.28943\n",
      "[84]\tvalid_0's multi_error: 0.288674\n",
      "[85]\tvalid_0's multi_error: 0.288371\n",
      "[86]\tvalid_0's multi_error: 0.288069\n",
      "[87]\tvalid_0's multi_error: 0.288523\n",
      "[88]\tvalid_0's multi_error: 0.287918\n",
      "[89]\tvalid_0's multi_error: 0.287918\n",
      "[90]\tvalid_0's multi_error: 0.287464\n",
      "[91]\tvalid_0's multi_error: 0.287464\n",
      "[92]\tvalid_0's multi_error: 0.287918\n",
      "[93]\tvalid_0's multi_error: 0.287313\n",
      "[94]\tvalid_0's multi_error: 0.287464\n",
      "[95]\tvalid_0's multi_error: 0.287918\n",
      "[96]\tvalid_0's multi_error: 0.28822\n",
      "[97]\tvalid_0's multi_error: 0.287767\n",
      "[98]\tvalid_0's multi_error: 0.287767\n",
      "[99]\tvalid_0's multi_error: 0.287162\n",
      "[100]\tvalid_0's multi_error: 0.287918\n",
      "[101]\tvalid_0's multi_error: 0.287918\n",
      "[102]\tvalid_0's multi_error: 0.288069\n",
      "[103]\tvalid_0's multi_error: 0.287918\n",
      "[104]\tvalid_0's multi_error: 0.287918\n",
      "[105]\tvalid_0's multi_error: 0.28822\n",
      "[106]\tvalid_0's multi_error: 0.287918\n",
      "[107]\tvalid_0's multi_error: 0.287767\n",
      "[108]\tvalid_0's multi_error: 0.287767\n",
      "[109]\tvalid_0's multi_error: 0.28822\n",
      "[110]\tvalid_0's multi_error: 0.287464\n",
      "[111]\tvalid_0's multi_error: 0.28701\n",
      "[112]\tvalid_0's multi_error: 0.286254\n",
      "[113]\tvalid_0's multi_error: 0.287162\n",
      "[114]\tvalid_0's multi_error: 0.286254\n",
      "[115]\tvalid_0's multi_error: 0.285952\n",
      "[116]\tvalid_0's multi_error: 0.286103\n",
      "[117]\tvalid_0's multi_error: 0.286406\n",
      "[118]\tvalid_0's multi_error: 0.285952\n",
      "[119]\tvalid_0's multi_error: 0.285649\n",
      "[120]\tvalid_0's multi_error: 0.285196\n",
      "[121]\tvalid_0's multi_error: 0.285196\n",
      "[122]\tvalid_0's multi_error: 0.285045\n",
      "[123]\tvalid_0's multi_error: 0.285196\n",
      "[124]\tvalid_0's multi_error: 0.285498\n",
      "[125]\tvalid_0's multi_error: 0.285045\n",
      "[126]\tvalid_0's multi_error: 0.28444\n",
      "[127]\tvalid_0's multi_error: 0.283835\n",
      "[128]\tvalid_0's multi_error: 0.284893\n",
      "[129]\tvalid_0's multi_error: 0.283684\n",
      "[130]\tvalid_0's multi_error: 0.283684\n",
      "[131]\tvalid_0's multi_error: 0.283381\n",
      "[132]\tvalid_0's multi_error: 0.283381\n",
      "[133]\tvalid_0's multi_error: 0.283986\n",
      "[134]\tvalid_0's multi_error: 0.283532\n",
      "[135]\tvalid_0's multi_error: 0.283532\n",
      "[136]\tvalid_0's multi_error: 0.283381\n",
      "[137]\tvalid_0's multi_error: 0.283079\n",
      "[138]\tvalid_0's multi_error: 0.282928\n",
      "[139]\tvalid_0's multi_error: 0.282928\n",
      "[140]\tvalid_0's multi_error: 0.283079\n",
      "[141]\tvalid_0's multi_error: 0.283079\n",
      "[142]\tvalid_0's multi_error: 0.283532\n",
      "[143]\tvalid_0's multi_error: 0.282474\n",
      "[144]\tvalid_0's multi_error: 0.282474\n",
      "[145]\tvalid_0's multi_error: 0.282776\n",
      "[146]\tvalid_0's multi_error: 0.282323\n",
      "[147]\tvalid_0's multi_error: 0.282323\n",
      "[148]\tvalid_0's multi_error: 0.282474\n",
      "[149]\tvalid_0's multi_error: 0.28202\n",
      "[150]\tvalid_0's multi_error: 0.282776\n",
      "[151]\tvalid_0's multi_error: 0.282171\n",
      "[152]\tvalid_0's multi_error: 0.28202\n",
      "[153]\tvalid_0's multi_error: 0.282625\n",
      "[154]\tvalid_0's multi_error: 0.28202\n",
      "[155]\tvalid_0's multi_error: 0.281415\n",
      "[156]\tvalid_0's multi_error: 0.28202\n",
      "[157]\tvalid_0's multi_error: 0.282625\n",
      "[158]\tvalid_0's multi_error: 0.28202\n",
      "[159]\tvalid_0's multi_error: 0.281264\n",
      "[160]\tvalid_0's multi_error: 0.280811\n",
      "[161]\tvalid_0's multi_error: 0.280811\n",
      "[162]\tvalid_0's multi_error: 0.280659\n",
      "[163]\tvalid_0's multi_error: 0.280508\n",
      "[164]\tvalid_0's multi_error: 0.281113\n",
      "[165]\tvalid_0's multi_error: 0.280962\n",
      "[166]\tvalid_0's multi_error: 0.280357\n",
      "[167]\tvalid_0's multi_error: 0.279601\n",
      "[168]\tvalid_0's multi_error: 0.280508\n",
      "[169]\tvalid_0's multi_error: 0.280508\n",
      "[170]\tvalid_0's multi_error: 0.279601\n",
      "[171]\tvalid_0's multi_error: 0.27945\n",
      "[172]\tvalid_0's multi_error: 0.279752\n",
      "[173]\tvalid_0's multi_error: 0.27945\n",
      "[174]\tvalid_0's multi_error: 0.278542\n",
      "[175]\tvalid_0's multi_error: 0.278845\n",
      "[176]\tvalid_0's multi_error: 0.278996\n",
      "[177]\tvalid_0's multi_error: 0.278996\n",
      "[178]\tvalid_0's multi_error: 0.278996\n",
      "[179]\tvalid_0's multi_error: 0.279147\n",
      "[180]\tvalid_0's multi_error: 0.279601\n",
      "[181]\tvalid_0's multi_error: 0.279752\n",
      "[182]\tvalid_0's multi_error: 0.280206\n",
      "[183]\tvalid_0's multi_error: 0.280659\n",
      "[184]\tvalid_0's multi_error: 0.279903\n",
      "[185]\tvalid_0's multi_error: 0.279752\n",
      "[186]\tvalid_0's multi_error: 0.279752\n",
      "[187]\tvalid_0's multi_error: 0.27945\n",
      "[188]\tvalid_0's multi_error: 0.279903\n",
      "[189]\tvalid_0's multi_error: 0.280357\n",
      "Early stopping, best iteration is:\n",
      "[174]\tvalid_0's multi_error: 0.278542\n",
      "[1]\tvalid_0's multi_error: 0.368485\n",
      "Training until validation scores don't improve for 15 rounds.\n",
      "[2]\tvalid_0's multi_error: 0.343261\n",
      "[3]\tvalid_0's multi_error: 0.330345\n",
      "[4]\tvalid_0's multi_error: 0.326546\n",
      "[5]\tvalid_0's multi_error: 0.32138\n",
      "[6]\tvalid_0's multi_error: 0.315302\n",
      "[7]\tvalid_0's multi_error: 0.314542\n",
      "[8]\tvalid_0's multi_error: 0.315757\n",
      "[9]\tvalid_0's multi_error: 0.31287\n",
      "[10]\tvalid_0's multi_error: 0.313022\n",
      "[11]\tvalid_0's multi_error: 0.309527\n",
      "[12]\tvalid_0's multi_error: 0.30664\n",
      "[13]\tvalid_0's multi_error: 0.306184\n",
      "[14]\tvalid_0's multi_error: 0.304513\n",
      "[15]\tvalid_0's multi_error: 0.301474\n",
      "[16]\tvalid_0's multi_error: 0.301018\n",
      "[17]\tvalid_0's multi_error: 0.302234\n",
      "[18]\tvalid_0's multi_error: 0.301626\n",
      "[19]\tvalid_0's multi_error: 0.302082\n",
      "[20]\tvalid_0's multi_error: 0.30193\n",
      "[21]\tvalid_0's multi_error: 0.30269\n",
      "[22]\tvalid_0's multi_error: 0.301778\n",
      "[23]\tvalid_0's multi_error: 0.302234\n",
      "[24]\tvalid_0's multi_error: 0.301018\n",
      "[25]\tvalid_0's multi_error: 0.299802\n",
      "[26]\tvalid_0's multi_error: 0.299802\n",
      "[27]\tvalid_0's multi_error: 0.300562\n",
      "[28]\tvalid_0's multi_error: 0.299802\n",
      "[29]\tvalid_0's multi_error: 0.299954\n",
      "[30]\tvalid_0's multi_error: 0.299347\n",
      "[31]\tvalid_0's multi_error: 0.299347\n",
      "[32]\tvalid_0's multi_error: 0.298891\n",
      "[33]\tvalid_0's multi_error: 0.297675\n",
      "[34]\tvalid_0's multi_error: 0.298283\n",
      "[35]\tvalid_0's multi_error: 0.296156\n",
      "[36]\tvalid_0's multi_error: 0.296763\n",
      "[37]\tvalid_0's multi_error: 0.297219\n",
      "[38]\tvalid_0's multi_error: 0.295852\n",
      "[39]\tvalid_0's multi_error: 0.295852\n",
      "[40]\tvalid_0's multi_error: 0.296004\n",
      "[41]\tvalid_0's multi_error: 0.295548\n",
      "[42]\tvalid_0's multi_error: 0.294788\n",
      "[43]\tvalid_0's multi_error: 0.294028\n",
      "[44]\tvalid_0's multi_error: 0.294484\n",
      "[45]\tvalid_0's multi_error: 0.294788\n",
      "[46]\tvalid_0's multi_error: 0.29494\n",
      "[47]\tvalid_0's multi_error: 0.295244\n",
      "[48]\tvalid_0's multi_error: 0.294636\n",
      "[49]\tvalid_0's multi_error: 0.293876\n",
      "[50]\tvalid_0's multi_error: 0.29342\n",
      "[51]\tvalid_0's multi_error: 0.293724\n",
      "[52]\tvalid_0's multi_error: 0.293269\n",
      "[53]\tvalid_0's multi_error: 0.292813\n",
      "[54]\tvalid_0's multi_error: 0.292813\n",
      "[55]\tvalid_0's multi_error: 0.292357\n",
      "[56]\tvalid_0's multi_error: 0.292053\n",
      "[57]\tvalid_0's multi_error: 0.291293\n",
      "[58]\tvalid_0's multi_error: 0.291445\n",
      "[59]\tvalid_0's multi_error: 0.290077\n",
      "[60]\tvalid_0's multi_error: 0.290229\n",
      "[61]\tvalid_0's multi_error: 0.290533\n",
      "[62]\tvalid_0's multi_error: 0.289166\n",
      "[63]\tvalid_0's multi_error: 0.289318\n",
      "[64]\tvalid_0's multi_error: 0.290077\n",
      "[65]\tvalid_0's multi_error: 0.28947\n",
      "[66]\tvalid_0's multi_error: 0.28947\n",
      "[67]\tvalid_0's multi_error: 0.289622\n",
      "[68]\tvalid_0's multi_error: 0.288406\n",
      "[69]\tvalid_0's multi_error: 0.28871\n",
      "[70]\tvalid_0's multi_error: 0.288102\n",
      "[71]\tvalid_0's multi_error: 0.287798\n",
      "[72]\tvalid_0's multi_error: 0.287646\n",
      "[73]\tvalid_0's multi_error: 0.287798\n",
      "[74]\tvalid_0's multi_error: 0.287494\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[75]\tvalid_0's multi_error: 0.287494\n",
      "[76]\tvalid_0's multi_error: 0.286886\n",
      "[77]\tvalid_0's multi_error: 0.286583\n",
      "[78]\tvalid_0's multi_error: 0.285367\n",
      "[79]\tvalid_0's multi_error: 0.286279\n",
      "[80]\tvalid_0's multi_error: 0.285671\n",
      "[81]\tvalid_0's multi_error: 0.284759\n",
      "[82]\tvalid_0's multi_error: 0.284911\n",
      "[83]\tvalid_0's multi_error: 0.284607\n",
      "[84]\tvalid_0's multi_error: 0.283544\n",
      "[85]\tvalid_0's multi_error: 0.283847\n",
      "[86]\tvalid_0's multi_error: 0.285215\n",
      "[87]\tvalid_0's multi_error: 0.284607\n",
      "[88]\tvalid_0's multi_error: 0.284607\n",
      "[89]\tvalid_0's multi_error: 0.284759\n",
      "[90]\tvalid_0's multi_error: 0.285519\n",
      "[91]\tvalid_0's multi_error: 0.284455\n",
      "[92]\tvalid_0's multi_error: 0.283847\n",
      "[93]\tvalid_0's multi_error: 0.284151\n",
      "[94]\tvalid_0's multi_error: 0.283544\n",
      "[95]\tvalid_0's multi_error: 0.283695\n",
      "[96]\tvalid_0's multi_error: 0.28324\n",
      "[97]\tvalid_0's multi_error: 0.283544\n",
      "[98]\tvalid_0's multi_error: 0.282936\n",
      "[99]\tvalid_0's multi_error: 0.282632\n",
      "[100]\tvalid_0's multi_error: 0.28248\n",
      "[101]\tvalid_0's multi_error: 0.282024\n",
      "[102]\tvalid_0's multi_error: 0.282328\n",
      "[103]\tvalid_0's multi_error: 0.28172\n",
      "[104]\tvalid_0's multi_error: 0.28172\n",
      "[105]\tvalid_0's multi_error: 0.281112\n",
      "[106]\tvalid_0's multi_error: 0.281112\n",
      "[107]\tvalid_0's multi_error: 0.28172\n",
      "[108]\tvalid_0's multi_error: 0.28172\n",
      "[109]\tvalid_0's multi_error: 0.281872\n",
      "[110]\tvalid_0's multi_error: 0.281112\n",
      "[111]\tvalid_0's multi_error: 0.28096\n",
      "[112]\tvalid_0's multi_error: 0.280504\n",
      "[113]\tvalid_0's multi_error: 0.280504\n",
      "[114]\tvalid_0's multi_error: 0.280504\n",
      "[115]\tvalid_0's multi_error: 0.280656\n",
      "[116]\tvalid_0's multi_error: 0.280201\n",
      "[117]\tvalid_0's multi_error: 0.280504\n",
      "[118]\tvalid_0's multi_error: 0.280049\n",
      "[119]\tvalid_0's multi_error: 0.280504\n",
      "[120]\tvalid_0's multi_error: 0.28096\n",
      "[121]\tvalid_0's multi_error: 0.280656\n",
      "[122]\tvalid_0's multi_error: 0.28172\n",
      "[123]\tvalid_0's multi_error: 0.282024\n",
      "[124]\tvalid_0's multi_error: 0.281568\n",
      "[125]\tvalid_0's multi_error: 0.281416\n",
      "[126]\tvalid_0's multi_error: 0.281416\n",
      "[127]\tvalid_0's multi_error: 0.281112\n",
      "[128]\tvalid_0's multi_error: 0.281264\n",
      "[129]\tvalid_0's multi_error: 0.28172\n",
      "[130]\tvalid_0's multi_error: 0.281264\n",
      "[131]\tvalid_0's multi_error: 0.281872\n",
      "[132]\tvalid_0's multi_error: 0.28096\n",
      "[133]\tvalid_0's multi_error: 0.280656\n",
      "Early stopping, best iteration is:\n",
      "[118]\tvalid_0's multi_error: 0.280049\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7157298207995099"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#stack\n",
    "import lightgbm as lgb\n",
    "num_class = np.unique(y).shape[0]\n",
    "params = {\n",
    "            'task': 'train',\n",
    "            'boosting_type': 'gbdt',\n",
    "            'objective': 'multiclass',\n",
    "            'metric': ['multi_error'],\n",
    "            'num_leaves': 31,\n",
    "            'learning_rate': 0.02,\n",
    "            'feature_fraction': 0.8,\n",
    "            'bagging_fraction': 0.8,\n",
    "            'bagging_freq': 5,\n",
    "            'verbose': 0,\n",
    "            'num_class': num_class,\n",
    "            'num_iterations':400,\n",
    "            'early_stopping_round':15\n",
    "        }\n",
    "from sklearn.base import clone\n",
    "from sklearn.model_selection import check_cv\n",
    "cv = 3\n",
    "_x,_y = expansion(_train_x3,train_y,cv)\n",
    "stack = Stacking([(\"rf1\",RandomForestClassifier(n_jobs=-1,\n",
    "                                                     n_estimators=388,\n",
    "                                                     class_weight=\"balanced\")),\n",
    "                  (\"lgb\",params),\n",
    "                  (\"ovr\",OneVsRestClassifier(estimator=RandomForestClassifier(n_estimators=188,n_jobs=-1,class_weight=\"balanced\")))\n",
    "                              ],\n",
    "                             (\"rf3\",RandomForestClassifier(n_estimators=888,n_jobs=-1,class_weight=\"balanced\")),\n",
    "                             use_prob=True,\n",
    "                             use_features_in_secondary = True,\n",
    "                             kfold = cv,\n",
    "                num_class =num_class) \n",
    "stack.fit(_x,_y)\n",
    "p = stack.predict(_valid_x3)\n",
    "acc(p, valid_y)\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_index = np.arange(_valid_x3.shape[0])\n",
    "train_x1 = np.concatenate([train_wifi_all_x[:,indexs],train_lonlats],axis=1)\n",
    "valid_x1= np.concatenate([valid_wifi_all_x[:,indexs],valid_lonlats],axis=1)\n",
    "train_x0 = np.concatenate([train_wifi_all_x[:,indexs]],axis=1)\n",
    "valid_x0= np.concatenate([valid_wifi_all_x[:,indexs]],axis=1)\n",
    "train_x2 = np.concatenate([train_wifi_all_x[:,indexs],train_wh],axis=1)\n",
    "valid_x2= np.concatenate([valid_wifi_all_x[:,indexs],valid_wh],axis=1)\n",
    "train_x3 = np.concatenate([train_wifi_all_x[:,indexs],train_lonlats,train_wh],axis=1)\n",
    "valid_x3= np.concatenate([valid_wifi_all_x[:,indexs],valid_lonlats,valid_wh],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "other_train_wifi_features=[]\n",
    "other_test_wifi_features= []\n",
    "sorted_wifi_all = get_sorted_wifi_just_train(train, valid)\n",
    "for _index in range(len(sorted_wifi_all), 0, -1):\n",
    "    if sorted_wifi_all[_index - 1][1] >= 10:\n",
    "        break\n",
    "sorted_wifi = sorted_wifi_all[:_index]\n",
    "d = rank_sorted_wifi(sorted_wifi)\n",
    "\n",
    "# use\n",
    "test_use_wifi_in_wifi_rank, train_use_wifi_in_wifi_rank = use_wifi_in_wifi_rank2(valid, train, d)\n",
    "other_train_wifi_features.append(train_use_wifi_in_wifi_rank.values.reshape((-1, 1)))\n",
    "other_test_wifi_features.append(test_use_wifi_in_wifi_rank.values.reshape((-1, 1)))\n",
    "other_train_wifi_feature = np.concatenate(other_train_wifi_features, axis=1)\n",
    "other_test_wifi_feature = np.concatenate(other_test_wifi_features, axis=1)\n",
    "\n",
    "train_x4 = np.concatenate([train_wifi_all_x[:,indexs],train_lonlats,train_wh,other_train_wifi_feature],axis=1)\n",
    "valid_x4= np.concatenate([valid_wifi_all_x[:,indexs],valid_lonlats,valid_wh, other_test_wifi_feature],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_wifis = {}\n",
    "for i in range(len(le.classes_)):\n",
    "    t_ = train[ (train.shop_id == le.classes_[i])]\n",
    "    s_ = t_.shape[0]\n",
    "    sorted_wifi = get_sorted_wifi([t_])\n",
    "    sp = 0.5\n",
    "    for _index in range(len(sorted_wifi), 0, -1):\n",
    "        if sorted_wifi[_index - 1][1] / float(s_) >= sp:\n",
    "            break\n",
    "    sorted_wifi = sorted_wifi[:_index]\n",
    "    d = rank_sorted_wifi(sorted_wifi)\n",
    "    sorted_wifis[i] = d.keys()\n",
    "def intersect_size(x,sorted_wifis):\n",
    "    l=[]\n",
    "    m = []\n",
    "    for _x in x[1]:\n",
    "        m.append(_x[0])\n",
    "    for _x in x[2]:\n",
    "        m.append(_x[0])\n",
    "    for _i in range(len(sorted_wifis)):\n",
    "        s = np.intersect1d(sorted_wifis[_i],m).shape[0]\n",
    "        l.append(s)\n",
    "    return np.array(l)\n",
    "train_f = np.vstack(train.basic_wifi_info.map(lambda x: intersect_size(x,sorted_wifis)).values)\n",
    "valid_f = np.vstack(valid.basic_wifi_info.map(lambda x: intersect_size(x,sorted_wifis)).values)\n",
    "\n",
    "# pca 降维\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(20).fit(train_f)\n",
    "train_f = pca.transform(train_f)\n",
    "valid_f = pca.transform(valid_f)\n",
    "\n",
    "train_x5 = np.concatenate([train_wifi_all_x[:,indexs],train_lonlats,train_wh,train_f],axis=1)\n",
    "valid_x5= np.concatenate([valid_wifi_all_x[:,indexs],valid_lonlats,valid_wh, valid_f],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_wifis = {}\n",
    "for i in range(len(le.classes_)):\n",
    "    t_ = train[ (train.shop_id == le.classes_[i])]\n",
    "    s_ = t_.shape[0]\n",
    "    sorted_wifi = get_sorted_wifi([t_])\n",
    "    sp = 0.9\n",
    "    for _index in range(len(sorted_wifi), 0, -1):\n",
    "        if sorted_wifi[_index - 1][1] / float(s_) >= sp:\n",
    "            break\n",
    "    sorted_wifi = sorted_wifi[:_index]\n",
    "    d = rank_sorted_wifi(sorted_wifi)\n",
    "    sorted_wifis[i] = d.keys()\n",
    "def intersect_size(x,sorted_wifis):\n",
    "    l=[]\n",
    "    m = []\n",
    "    for _x in x[1]:\n",
    "        m.append(_x[0])\n",
    "    for _x in x[2]:\n",
    "        m.append(_x[0])\n",
    "    for _i in range(len(sorted_wifis)):\n",
    "        s = np.intersect1d(sorted_wifis[_i],m).shape[0]\n",
    "        l.append(s)\n",
    "    return np.array(l)\n",
    "train_f = np.vstack(train.basic_wifi_info.map(lambda x: intersect_size(x,sorted_wifis)).values)\n",
    "valid_f = np.vstack(valid.basic_wifi_info.map(lambda x: intersect_size(x,sorted_wifis)).values)\n",
    "train_x5 = np.concatenate([train_wifi_all_x[:,indexs],train_lonlats,train_wh,train_f],axis=1)\n",
    "valid_x5= np.concatenate([valid_wifi_all_x[:,indexs],valid_lonlats,valid_wh, valid_f],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_median(d,t):\n",
    "    for _k in d:\n",
    "        d[_k] = []\n",
    "    def get_one_sample(x,d):\n",
    "        for _x in x[1]:\n",
    "            if _x[0] in d:\n",
    "                d[_x[0]].append(_x[1])\n",
    "        for _x in x[2]:\n",
    "            if _x[0] in d:\n",
    "                d[_x[0]].append(_x[1])   \n",
    "    \n",
    "    t.basic_wifi_info.map(lambda x: get_one_sample(x,d))\n",
    "    \n",
    "    for _k in d:\n",
    "        d[_k] = np.median(d[_k])\n",
    "    return d\n",
    "sorted_wifis = {}\n",
    "for i in range(len(le.classes_)):\n",
    "    t_ = train[ (train.shop_id == le.classes_[i])]\n",
    "    s_ = t_.shape[0]\n",
    "    sorted_wifi = get_sorted_wifi([t_])\n",
    "    sp = 0.3\n",
    "    for _index in range(len(sorted_wifi), 0, -1):\n",
    "        if sorted_wifi[_index - 1][1] / float(s_) >= sp:\n",
    "            break\n",
    "    sorted_wifi = sorted_wifi[:_index]\n",
    "    d = rank_sorted_wifi(sorted_wifi)\n",
    "    d = get_median(d, t_)\n",
    "    sorted_wifis[i] = d\n",
    "\n",
    "def dis(x1,x2):\n",
    "    return np.sqrt((x1-x2)*(x1-x2))\n",
    "def dis_to_wifi(x,sorted_wifis):\n",
    "    l=[]\n",
    "    for _i in range(len(sorted_wifis)):\n",
    "        s = 0\n",
    "        c = 0\n",
    "        wifis = sorted_wifis[_i]\n",
    "        for _x in x[1]:\n",
    "            if _x[0] in wifis:\n",
    "                c += 1\n",
    "                s+= dis(_x[1],wifis[_x[0]])\n",
    "        for _x in x[2]:\n",
    "            if _x[0] in wifis:\n",
    "                c += 1\n",
    "                s+= dis(_x[1],wifis[_x[0]])\n",
    "        if c!=0:\n",
    "            l.append(float(s) / float(c))\n",
    "        else:\n",
    "            l.append(10000)\n",
    "    return np.array(l)\n",
    "train_f = np.vstack(train.basic_wifi_info.map(lambda x: dis_to_wifi(x,sorted_wifis)).values)\n",
    "valid_f = np.vstack(valid.basic_wifi_info.map(lambda x: dis_to_wifi(x,sorted_wifis)).values)\n",
    "train_x6 = np.concatenate([train_wifi_all_x[:,indexs],train_lonlats,train_wh,train_f],axis=1)\n",
    "valid_x6= np.concatenate([valid_wifi_all_x[:,indexs],valid_lonlats,valid_wh, valid_f],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       ..., \n",
       "       [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "         0.00694444,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.01041667, ...,  0.        ,\n",
       "         0.        ,  0.        ]])"
      ]
     },
     "execution_count": 320,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv = CVEstimator(RandomForestClassifier(n_jobs=-1,n_estimators=288,class_weight=\"balanced\"))\n",
    "cv.fit(train_x3,train_y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexs = choose_strong_wifi_index(-90,6,train_wifi_all_x)\n",
    "train_x7 = np.concatenate([train_wifi_all_x[:,indexs],cv.predict_proba(train_x3)],axis=1)\n",
    "valid_x7= np.concatenate([valid_wifi_all_x[:,indexs],cv.predict_proba(valid_x3)],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "origin 0.999387348752\n",
      "error shape, 4\n",
      "[1 1 0 0]\n",
      "[1587 1588 3174 3819]\n",
      "s_136016\n"
     ]
    }
   ],
   "source": [
    "\n",
    "_train_b_y.shape[0]\n",
    "i = 3\n",
    "c=7\n",
    "\n",
    "if c == 3:\n",
    "    tx = train_x3\n",
    "    vx = valid_x3\n",
    "elif c == 2:\n",
    "    tx = train_x2\n",
    "    vx = valid_x2\n",
    "elif c == 4:\n",
    "    tx = train_x4\n",
    "    vx = valid_x4\n",
    "elif c == 5:\n",
    "    tx = train_x5\n",
    "    vx = valid_x5\n",
    "elif c == 6:\n",
    "    tx = train_x6\n",
    "    vx = valid_x6\n",
    "elif c == 7:\n",
    "    tx = train_x7\n",
    "    vx = valid_x7\n",
    "\n",
    "def get_model():\n",
    "    rf2 = RandomForestClassifier(n_estimators=188,n_jobs=-1,class_weight=\"balanced\",min_samples_leaf=1)\n",
    "    return rf2\n",
    "    \n",
    "prf = get_model()\n",
    "prf.fit(tx,_train_b_y[:,i])\n",
    "p = prf.predict(vx)\n",
    "proba = prf.predict_proba(vx)\n",
    "print \"origin\",acc(p,_valid_b_y[:,i])\n",
    "print \"error shape,\", (p != _valid_b_y[:,i]).sum()\n",
    "print _valid_b_y[:,i][(p != _valid_b_y[:,i])]\n",
    "print valid_index[(p != _valid_b_y[:,i])]\n",
    "print le.classes_[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "689680    b_55640889|-58|false;b_52574361|-73|false;b_62470|-73|false;b_55570664|-57|false;b_55640412|-48|false;b_26730374|-67|false;b_62516|-79|false;b_62704|-48|false;b_62705|-66|false;b_42231552|-72|false\n",
      "Name: wifi_infos, dtype: object\n",
      "\n",
      "803695    b_55570664|-60|false;b_32358751|-61|false;b_52574362|-66|false;b_62516|-78|false;b_55640889|-59|false;b_62550|-80|false;b_52574361|-81|false;b_62705|-71|false;b_55640412|-48|false;b_20616910|-77|false\n",
      "Name: wifi_infos, dtype: object\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print train[train.shop_id==\"s_193954\"][\"wifi_infos\"]\n",
    "print \n",
    "print valid[valid.shop_id==\"s_193954\"][\"wifi_infos\"]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
